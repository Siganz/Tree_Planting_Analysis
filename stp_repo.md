This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-07-11 22:02:55

# File Summary

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
   a. A header with the file path (## File: path/to/file)
   b. The full contents of the file in a code block

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Information:

For more information about Repomix, visit: https://github.com/andersonby/python-repomix


# Repository Structure

```
.codexignore
.continue
  docs
    new-doc.yaml
.gitignore
AGENTS.md
config
  defaults.yaml
  sources.json
  user.example.yaml
  user.yaml
  worfklow.yaml
data
  gpkg
    _gpkg.md
  table
    _table.md
docker.yaml
docs
  todo.md
License
pyproject.toml
README.md
requirements.txt
requirements_dev.txt
scrap
  01.md
  02.json
  03.md
  tokens.py
  zip.py
secrets
  secrets.yaml
src
  arcpy
    arcpy converted
      curb.py
      nostandingy.py
      rank.py
    arcpy depracated
      arcpy_First Step.py
      arcpy_nostanding.py
      arcpy_rank_dominant_working.py
      arcpy_Second_Step_Alternative.py
  stp
    cli
      stp_pipeline.py
    core
      config.py
      http.py
      settings.py
    fetch
      arcgis.py
      csv.py
      download.py
      gdb.py
      geojson.py
      gpkg.py
      lookup.py
      socrata.py
      _optional_deps.py
      __init__.py
    process
      clean
        address.py
        trees.py
        __init__.py
      custom_ops.py
      data_cleaning.py
      fields_inventory.py
      field_ops.py
      geometry_ops.py
      table.py
    record
      csv.py
      db.py
      export.py
      gpkg.py
      postgis.py
      __init__.py
    scaffold.md
    scrap.md
    storage
      db_storage.py
      file_storage.py
      __init__.py
    stp.md
    zip.py
    __init__.py
  temp
    download_data.py
    political_boundary.py
stp_repo.md
tests
  conftest.py
  test_address.py
  test_config_loader.py
  test_csv.py
  test_db_storage.py
  test_download.py
  test_file_storage.py
  test_geojson.py
  test_http_client.py
  test_trees.py
```

# Repository Files


## .codexignore

```text
# ignore VCS
.git/
# ignore virtual environments
.venv/
env/
# ignore data blobs
Data/
docs/
# ignore miscellaneous
*.tar
*.zip
AGENTS.md
hello.txt
scratch.py
```

## .continue/docs/new-doc.yaml

```yaml
name: New doc
version: 0.0.1
schema: v1
docs:
  - name: New docs
    startUrl: https://docs.continue.dev
```

## .gitignore

```text
# -------------------------------
# üêç Python build artifacts
# -------------------------------
__pycache__/
*.py[cod]
.venv/
.env/
.pytest_cache
*.log
/scrap
src/arcpy
data/
# -------------------------------
# üß™ Project-specific junk & output
# -------------------------------
*.tar
*.zip
project.zip
config/user.yaml
tokens.py
# -------------------------------
# üßæ Secret or local-only folders
# -------------------------------
secrets/

# -------------------------------
# üìä Data outputs (selectively ignored)
# -------------------------------
Data/table/*
Data/gpkg/*

# Keep placeholder files and readmes
!Data/table/
!Data/gpkg/
!Data/table/table.md
!Data/gpkg/gpkg.md

# -------------------------------
# üîß Developer tools/utilities
# -------------------------------
zip.py
project.zip
```

## AGENTS.md

````markdown
# AGENTS.md

## ‚úèÔ∏è Code Style Guide

This project enforces basic Python style conventions across `src/stp/`.

---

### üîπ Formatting Rules

- **Indentation**: 4 spaces  
- **Line length**: max 79 characters  
- **No trailing whitespace**  
- **Blank lines**:  
  - 2 between top-level functions and classes  
  - 1 between method definitions inside classes  

---

### üîπ Import Order

1. **Standard library**
2. **Third-party**
3. **Local modules** (`src/stp/...`)

Use `isort` or arrange manually to match.

---

### üîπ Docstrings

- Required on all **public functions** and **classes**
- Use triple double quotes (`"""docstring"""`)
- Be concise and descriptive

---

### üîπ Comments

- Use inline `#` comments sparingly ‚Äî only for **non-obvious logic**
- Avoid restating what the code already expresses clearly

---

### üîπ Enforcement

You can check for style issues using:

```bash
flake8 src/stp/ --max-line-length=79
pylint src/stp/
```

---

### ‚ùå Exclusions

These folders are not checked by linters:

- `.venv/`
- `Data/`
- `config/`
- `tests/gis_*`

---

This file focuses only on style. Testing, CI, and agents are optional layers you can add later.
````

## config/defaults.yaml

```yaml
epsg:
  default: 4326
  nysp: 2263

limits:
  socrata: 50000
  arcgis_default_max_records: 1000

validation:
  layer_name_max_length: 60
  min_dbh: 0.01
```

## config/sources.json

```json
[
    {
      "id": "trees",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/hn5i-inap.json",
      "schema": "political"
    },
    {
      "id": "work_orders",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/bdjm-n7q4.json",
      "schema": "political"
    },
    {
      "id": "planting_spaces",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/82zj-84is.json",
      "schema": "political"
    },
    {
      "id": "street_sign",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/qt6m-xctn.json",
      "schema": "political"
    },
    {
      "id": "hydrants",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/5bgh-vtsn.json",
      "schema": "political"
    },
    {
      "id": "green_infrastructure",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/df32-vzax.json",
      "schema": "political"
    },
    {
      "id": "subway_lines",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services.arcgis.com/ue9rwulIoeLEI9bj/arcgis/rest/services/NYC_SubwayLines/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "borough",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "community_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Community_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "council_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_City_Council_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "congressional_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/ArcGIS/rest/services/NYC_Congressional_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "senate_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_State_Senate_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "assembly_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_State_Assembly_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "community_tabulations",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Community_District_Tabulation_Areas_2020/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "neighborhood_tabulations",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Neighborhood_Tabulation_Areas_2020/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "census_tracts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Tracts_for_2020_US_Census/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "census_blocks",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Blocks_for_2020_US_Census/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "zoning_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyzd/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "commercial_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyco/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "special_purpose_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nysp/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "pluto",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/MAPPLUTO/FeatureServer/0",
      "schema": "land_use"
    },
    {
      "id": "street_center",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/DCM_Street_Center_Line/FeatureServer/0",
      "schema": "infrastructure"
    },
    {
      "id": "curb",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Curb_2022/FeatureServer/4",
      "schema": "infrastructure"
    },
    {
      "id": "curb_cut",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/ArcGIS/rest/services/Curb_Cut_2022/FeatureServer/5",
      "schema": "infrastructure"
    },
    {
      "id": "sidewalk",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Sidewalk_2022/FeatureServer/22",
      "schema": "infrastructure"
    }
  ]
```

## config/user.example.yaml

```yaml
epsg:
  default: 4326
  nysp: 2263

limits:
  socrata: 50000
  arcgis_default_max_records: 1000

validation:
  layer_name_max_length: 60
  min_dbh: 0.01
```

## config/user.yaml

```yaml
epsg:
  default: 4326
  nysp: 2263

limits:
  socrata: 50000
  arcgis_default_max_records: 1000

validation:
  layer_name_max_length: 60
  min_dbh: 0.01

paths:
  config:   "Config/config.yaml"
  constants: "Config/constants.yaml"
  output:
    shapefiles: "Data/shapefiles"
    tables:     "Data/tables"

filenames:
  data_inventory:  "data_inventory.csv"
  default_gpkg:    "project_data.gpkg"
```

## config/worfklow.yaml

```yaml
# Unified NYC Tree Planting Workflow Config (combined from your 01.md ops, 02.json sources, 03.md filters)
# Edit for toggles, other cities, or reordering. Pipeline reads this to run dynamically.

sources:  # From 02.json (URLs/types/schema) + 03.md (filters/simple types)
  trees:
    type: "json"  # From 03.md
    source_type: "socrata"  # From 02.json
    format: "json"
    url: "https://data.cityofnewyork.us/resource/hn5i-inap.json"
    schema: "political"
    filter: "SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')"  # From 03.md
    to_layer: "trees_raw"  # Initial save in GPKG
    enabled: true

  work_orders:
    type: "json"
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/bdjm-n7q4.json"
    schema: "political"
    filter: "STATUS <> 'Cancelled'"
    to_layer: "work_orders_raw"
    enabled: true

  planting_spaces:
    type: "json"  # Assumed from 03.md pattern (not listed, but in scaffold)
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/82zj-84is.json"
    schema: "political"
    filter: null  # None specified
    to_layer: "planting_spaces_raw"
    enabled: true

  street_sign:
    type: "json"
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/nfid-uabd.json"  # Updated from search (old qt6m-xctn invalid)
    schema: "political"
    filter: null  # Or "$where=sign_desc like '%NO STANDING%' OR sign_desc like '%NO PARKING%'" if pre-filtering; done in nostanding.py otherwise
    to_layer: "street_sign_raw"
    enabled: true

  hydrants:
    type: "json"
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/5bgh-vtsn.json"
    schema: "political"
    filter: null
    to_layer: "hydrants_raw"
    enabled: true

  green_infrastructure:
    type: "json"
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/df32-vzax.json"  # Confirmed current
    schema: "political"
    filter: null
    to_layer: "green_infrastructure_raw"
    enabled: true

  subway_lines:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services.arcgis.com/ue9rwulIoeLEI9bj/arcgis/rest/services/NYC_SubwayLines/FeatureServer/0"
    schema: "political"
    filter: null
    to_layer: "subway_lines_raw"
    enabled: true

  # ... (Adding the rest from 02.json/03.md similarly; I shortened for space, but include all in your file)
  borough:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary/FeatureServer/0"
    schema: "political"
    filter: null
    to_layer: "borough_raw"
    enabled: true

  # community_districts, council_districts, etc. - follow pattern above

  census_blocks:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Blocks_for_2020_US_Census/FeatureServer/0"
    schema: "political"
    filter: null
    to_layer: "census_blocks_raw"
    enabled: true

  zoning_districts:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyzd/FeatureServer/0"
    schema: "zoning"
    filter: null  # Filter in ops
    to_layer: "zoning_districts_raw"
    enabled: true

  # commercial_districts, special_purpose_districts, pluto, street_center, curb, curb_cut, sidewalk - similar

  sidewalk:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Sidewalk_2022/FeatureServer/22"
    schema: "infrastructure"
    filter: null
    to_layer: "sidewalk_raw"
    enabled: true

  grass_shrub:  # Added from search (2017 land cover raster for Grass_Shrub ops)
    type: "raster"  # Special handling (download GeoTIFF, convert to poly)
    source_type: "socrata"
    format: "geotiff"  # Export from Socrata
    url: "https://data.cityofnewyork.us/api/geospatial/he6d-2qns?method=export&format=GeoTIFF"  # Or Shapefile if preferred
    schema: "land_use"
    filter: null
    to_layer: "grass_shrub_raw"  # Save as raster or convert early
    enabled: true

prep_ops:  # From 01.md - ops per layer (description, filters if not in sources, steps)
  nyzd:  # Maps to zoning_districts source
    description: "Zoning districts where planting is prohibited"
    filter: "ZONE_DIST IN ('M1','M2','M3','IG','IH')"  # Applied in select step
    enabled: true
    steps:
      - op: copy
        input: "zoning_districts_raw"
        output: "nyzd_copy"
      - op: select
        query: "ZONE_DIST IN ('M1','M2','M3','IG','IH')"
        output: "nyzd_ready"

  dep_gi_assets:  # Maps to green_infrastructure
    description: "Green infrastructure assets to avoid"
    filter: null
    enabled: true
    steps:
      - op: copy
        input: "green_infrastructure_raw"
        output: "dep_gi_assets_copy"
      - op: buffer
        distance: 20  # Feet
        output: "dep_gi_assets_ready"

  sidewalk:
    description: "Raw sidewalk polygons, split for different logic"
    filter: null
    enabled: true
    steps:
      - op: copy
        input: "sidewalk_raw"
        output: "sidewalk_copy"
      - op: polygon_to_polyline
        input: "sidewalk_copy"
        output: "sidewalk_1"
      - op: split_immutable_mutable  # Custom? (Assume function for splitting)
        input: "sidewalk_1"
        outputs: ["sidewalk_immutable_ready", "sidewalk_mutable_ready"]

  curb_cuts:  # Maps to curb_cut (for intersections)
    description: "Sidewalk curb-cut intersections"
    filter: "SUB_FEATURE_CODE = 222700"
    enabled: true
    steps:
      - op: copy
        input: "curb_cut_raw"
        output: "curb_cuts_copy"
      - op: select
        query: "SUB_FEATURE_CODE = 222700"
        output: "curb_cuts_1"
      - op: buffer
        distance: 30
        output: "curb_cuts_ready"

  # curb_cuts_driveways: Separate group for driveway filter (same source as curb_cuts, but different filter/ops)
  curb_cuts_driveways:
    description: "Driveway curb cuts to exclude planting"
    filter: "SUB_FEATURE_CODE = 222600"
    enabled: true
    steps:
      - op: copy
        input: "curb_cut_raw"  # Reuse source
        output: "curb_cuts_driveways_copy"
      - op: select
        query: "SUB_FEATURE_CODE = 222600"
        output: "curb_cuts_driveways_1"
      - op: buffer
        distance: 15
        output: "curb_cuts_driveways_ready"

  subway_lines:
    description: "Subway buffers to exclude planting near tracks"
    filter: null
    enabled: true
    steps:
      - op: copy
        input: "subway_lines_raw"
        output: "subway_lines_copy"
      - op: buffer
        distance: 80
        output: "subway_lines_ready"

  workorders:
    description: "Active DOT work orders"
    filter: "STATUS <> 'Cancelled'"  # Already in sources, but repeated for clarity
    enabled: true
    steps:
      - op: copy  # Or export_table + select
        input: "work_orders_raw"
        output: "workorders_copy"
      - op: xy_to_point
        input: "workorders_copy"
        output: "workorders_1"
      - op: buffer
        distance: 25
        output: "workorders_ready"

  treeandsite:  # Maps to trees (or planting_spaces? Assumed trees based on filter)
    description: "Existing tree locations to avoid"
    filter: "SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')"
    enabled: true
    steps:
      - op: copy  # Export_table + select
        input: "trees_raw"
        output: "treeandsite_copy"
      - op: xy_to_point
        input: "treeandsite_copy"
        output: "treeandsite_1"
      - op: buffer
        distance: 25
        output: "treeandsite_ready"

  grass_shrub:
    description: "2017 land-use raster converted to shrub/grass polygons"
    filter: null
    enabled: true
    steps:
      - op: copy  # Copy raster
        input: "grass_shrub_raw"
        output: "grass_shrub_copy"
      - op: raster_to_polygon
        simplify: false
        output: "grass_shrub_1"
      - op: delete_field
        fields: ["gridcode", "Id"]
        output: "grass_shrub_ready"

  political_boundary:  # Composite (no direct source; unions others)
    description: "Union of all political boundaries for spatial join"
    filter: null
    enabled: true
    steps:
      - op: copy  # Or direct union
        input: null  # No single input
        output: "political_boundary_copy"
      - op: union
        inputs: ["borough_raw", "community_districts_raw", "council_districts_raw", "congressional_districts_raw", "senate_districts_raw", "assembly_districts_raw", "community_tabulations_raw", "neighborhood_tabulations_raw", "census_tracts_raw"]  # From scaffold
        output: "political_boundary_1"
      - op: delete_field
        fields: ["FID_*"]  # All FID_ fields
        output: "political_boundary_ready"

  street_centerline:  # Maps to street_center
    description: "Simplify street geometry and buffer vertices"
    filter: "L_LOW_HN IS NOT NULL"
    enabled: true
    steps:
      - op: copy
        input: "street_center_raw"
        output: "street_centerline_copy"
      - op: simplify_line
        method: "POINT_REMOVE"
        tolerance: 1  # Feet
        output: "street_centerline_1"
      - op: vertices_to_points
        input: "street_centerline_1"
        output: "street_centerline_2"
      - op: buffer
        distance: 40
        output: "street_centerline_ready"

  dep_hydrants:  # Maps to hydrants
    description: "Hydrant proximity adjustments"
    filter: null
    enabled: true
    steps:
      - op: copy
        input: "hydrants_raw"
        output: "dep_hydrants_copy"
      - op: generate_near_table
        distance: 10
        output: "dep_hydrants_1"
      - op: move_street_signs  # Custom script/tool
        input: "dep_hydrants_1"
        output: "dep_hydrants_2"
      - op: buffer
        distance: 3
        output: "dep_hydrants_ready"

final_clip:  # From 01.md end - separate section for pipeline end
  description: "Compute allowable planting points within clipped sidewalk"
  filter: null
  enabled: true
  steps:
    - op: copy
      input: "sidewalk_mutable_ready"
      output: "sidewalk_mutable_backup"
    - op: union
      inputs: ["nyzd_ready", "dep_gi_assets_ready", "curb_cuts_ready", "subway_lines_ready", "workorders_ready", "treeandsite_ready", "grass_shrub_ready"]
      output: "no_plant_zones_union"
    - op: clip
      input: "sidewalk_mutable_ready"
      clip_with: "no_plant_zones_union"
      output: "sidewalk_availability_ready"
    - op: create_points  # Custom: Fishnet or generate along lines
      input: "sidewalk_availability_ready"
      method: "fishnet"  # Or your rank_dominant
      output: "plant_pts_1"
    - op: spatial_join
      target: "plant_pts_1"
      join_features: "sidewalk_availability_ready"  # Plus political_boundary_ready?
      output: "plant_pts_ready"

big_scripts:  # Optional: Tie in nostanding/rank/curb (run after prep)
  nostanding:
    enabled: true
    input_csv: "street_sign_raw"  # Or path
    sw_layer: "sidewalk_mutable_ready"
    out_layer: "no_standing_ready"

  # rank_dominant, curb - as before
```

## data/gpkg/_gpkg.md

```markdown
# Data

## Data sources will be generated in this folder. Will be included in the gitignore because it doesn't need to be on github.
```

## data/table/_table.md

```markdown
# Data

## Helping data tables will be generated in this folder. Will be included in the gitignore because it doesn't need to be on github.
```

## docs/todo.md

```markdown
# TO DO 
## 7/6/2025
- [ ] I removed some items in user.example.yaml, so I need to make sure those removed variables aren't used elsewhere. 
- [ ] Scafffold a main.py within stp. Needs to follow the logic of the primary scope in README
- [ ] I need to find a way to utilize .env corerctly. Perhaps the best thing is to run a script on open, to check if .env is there, else create folder, create yaml with default inputs. 
- [ ] List out the steps of the project within the stp readme. 
- [ ] Parametize the pipeline for users. If they need to change when union happens in the model, figure it out.
- [ ] Create gui, for user to input parameters, select operations, input local features if they have it.
```

## License

```text
All Rights Reserved.

Copyright (c) 2025 Shawn Ganz.

This repository and its contents may not be used, copied, or distributed in any form without explicit written permission from the owner.

Unauthorized use, reproduction, or distribution is strictly prohibited.
```

## pyproject.toml

```text
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "stp"
version = "0.1.0"
description = "Street Tree Planting Analysis tools"
authors = [ { name="Shawn Ganz", email="ganz.shawn@gmail.com" } ]
readme = "README.md"
license = { file="LICENSE" }
requires-python = ">=3.10"
dependencies = [
    "geopandas>=0.14.0",
    "pandas>=2.0",
    "shapely>=2.0",
    "sqlalchemy>=2.0",
    "requests>=2.0",
    "pyproj",
    "psycopg2-binary",
]

[project.scripts]  # CLI entry points!
stp-download = "stp.download:main"  # Add a main() function to download.py if needed
stp-clean = "stp.data_cleaning:main"  # Same for others
stp-inventory = "stp.fields_inventory:main"
```

## README.md

````markdown
# Street Tree Planting Analysis (STP)

Scripts and tools for downloading, processing, and analyzing New York City street tree and sidewalk data.

---

## Table of Contents

* [Installation](#installation)
* [Environment Setup](#environment-setup)
* [Configuration](#configuration)
* [Folder Structure](#folder-structure)
* [Contributing](#contributing)
* [License](#license)

---

## Installation

1. **Clone the repository:**

   ```bash
   git clone https://github.com/shawnganznyc/Street_Tree_Planting_Analysis.git
   cd Street_Tree_Planting_Analysis
   ```

2. **Install dependencies**
pip install -r requirements.txt

## Configuration

1. **Edit configuration files:**

   * `config/defaults.yaml` ‚Äî project-wide constants (safe to commit)
   * `config/user.yaml` ‚Äî user-specific secrets/overrides (do **NOT** commit)

2. **On first use:**

   * Copy `config/user.example.yaml` to `config/user.yaml` and fill in any required secrets (e.g. Socrata app token).
   * Ensure `config/user.yaml` is listed in `.gitignore`.

---

## Folder Structure

```
config/      - Configuration files (defaults, user secrets)
src/stp/     - Main Python package (cleaning, fetchers, storage, etc.)
Data/        - Downloaded/generated data (not tracked in version control)
tests/       - Unit and integration tests
```

---

## Scope

1. Read user parameters
2. Download files
3. Create GIS assets if specified/possible
4. Clean
   * Read user settings per item to figure out what fields to keep. 
5. Manipulate
   * Shapes undergo geoprocessing functions
      * User specified options
         * ex: Buffer, 20 ft. 
   * Pipeline functions
      * union
      * clip
      * merge
      * spatial join
      * custom operations
6. Combine
7. Output
   * Number of potential trees in the area. 

# Primary Scope

1. Read established parameters
2. Download files from arcgis/nyc opendata
3. Convert json into geojson point/polygons
4. Clean files of unecessary fields
5. Apply buffers, filters, custom scripts. 
6. Create a sidewalk polyline using polygon buffer
   - Copy non mutable sidewalk polyline
7. Merge all polygons where plantings cannot occur. 
8. Clip do not plant locations with sidewalk polyline. 
9. Use non mutable sidewalk polyline to use for traffic signs / parking rules
   - Build no parking zones
      - Classify no parking into bins: 
         - No Parking
         - No Standing
            - No Standing Taxi
            - No Standing Truck Loading
            - No Standing (something else, forgot)
   - Build sidewalk vehicle parking times into each sidewalk 
   - Build mta no bus locations/rules
10. Generate potential planting locations
11. Join potential planting location with sidewalk information
12. Done.

# Current Goal
- Functioning python pipeline
   - Pipeline already developed on arcpy.
- User parameter control
   - Allow user to manipulate variables/parameters. 
      - Atm the original project use parameters and variables are somewhat set. 
- Automatic pipeline based on geography
   - All inputs need to be faithful to the pipeline, otherwise errors will occur
   - Template settings for all major cities? 

## Contributing

Pull requests and issues are welcome. Please open an issue to discuss major changes before submitting a PR.

---

## License

This project is open-source. See `LICENSE` for details.

---

**Tip:**
All user secrets and API keys go in `config/user.yaml` (never committed).
Project settings and defaults live in `config/defaults.yaml`.

---

Let me know if you want any sections expanded or customized for your onboarding workflow.
````

## requirements.txt

```text
fiona==1.10.1
geopandas==1.1.1
pandas==2.3.0
python-dotenv==1.1.1
PyYAML==6.0.2
PyYAML==6.0.2
Requests==2.32.4
Shapely==2.1.1
SQLAlchemy==2.0.41
```

## scrap/01.md

```markdown
- NYZD  
  - Description: Zoning districts where planting is prohibited  
  - Filters: `"ZONE_DIST" IN ('M1','M2','M3','IG','IH')`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `nyzd_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `nyzd_ready`

- DEP_GI_Assets  
  - Description: Green infrastructure assets to avoid  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `dep_gi_assets_copy`  
    - Operation b: Buffer  
      - Params: distance = 20 Feet  
      - Output: `dep_gi_assets_ready`

- Sidewalk  
  - Description: Raw sidewalk polygons, split for different logic  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `sidewalk_copy`  
    - Operation b: Polygon‚ÜíPolyline  
      - Params: input = `sidewalk_copy`  
      - Output: `sidewalk_1`  
    - Operation c: Split immutable/mutable  
      - Params: input = `sidewalk_1`  
      - Outputs: `sidewalk_immutable_ready`, `sidewalk_mutable_ready`

- Curb_Cuts  
  - Description: Sidewalk curb-cut intersections  
  - Filters: `SUB_FEATURE_CODE = 222700`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `curb_cuts_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `curb_cuts_1`  
    - Operation c: Buffer  
      - Params: input = `curb_cuts_1`, distance = 30 Feet  
      - Output: `curb_cuts_ready`

- Subway_Lines  
  - Description: Subway buffers to exclude planting near tracks  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `subway_lines_copy`  
    - Operation b: Buffer  
      - Params: distance = 80 Feet  
      - Output: `subway_lines_ready`

- Workorders  
  - Description: Active DOT work orders  
  - Filters: `STATUS <> 'Cancelled'`  
  - Operations  
    - Operation a: CopyFeatures (ExportTable/Select)  
      - Output: `workorders_copy`  
    - Operation b: XYTableToPoint  
      - Params: input = `workorders_copy`  
      - Output: `workorders_1`  
    - Operation c: Buffer  
      - Params: input = `workorders_1`, distance = 25 Feet  
      - Output: `workorders_ready`

- TreeandSite  
  - Description: Existing tree locations to avoid  
  - Filters: `SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')`  
  - Operations  
    - Operation a: CopyFeatures (ExportTable/Select)  
      - Output: `treeandsite_copy`  
    - Operation b: XYTableToPoint  
      - Params: input = `treeandsite_copy`  
      - Output: `treeandsite_1`  
    - Operation c: Buffer  
      - Params: input = `treeandsite_1`, distance = 25 Feet  
      - Output: `treeandsite_ready`

- Grass_Shrub  
  - Description: 2017 land-use raster converted to shrub/grass polygons  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures (Copy Raster)  
      - Output: `grass_shrub_copy`  
    - Operation b: RasterToPolygon_conversion  
      - Params: simplify = NO_SIMPLIFY  
      - Output: `grass_shrub_1`  
    - Operation c: DeleteField  
      - Params: fields = `["gridcode","Id"]`  
      - Output: `grass_shrub_ready`

- Political_Boundary  
  - Description: Union of all political boundaries for spatial join  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `political_boundary_copy`  
    - Operation b: Union_analysis  
      - Params: inputs = borough, community boards, council, senate, assembly, NTA, tracts  
      - Output: `political_boundary_1`  
    - Operation c: DeleteField  
      - Params: fields = all `FID_*`  
      - Output: `political_boundary_ready`

- Street_Centerline  
  - Description: Simplify street geometry and buffer vertices  
  - Filters: `L_LOW_HN IS NOT NULL`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `street_centerline_copy`  
    - Operation b: SimplifyLine_cartography  
      - Params: method = POINT_REMOVE, tolerance = 1 Feet  
      - Output: `street_centerline_1`  
    - Operation c: FeatureVerticesToPoints  
      - Params: input = `street_centerline_1`  
      - Output: `street_centerline_2`  
    - Operation d: Buffer  
      - Params: input = `street_centerline_2`, distance = 40 Feet  
      - Output: `street_centerline_ready`

- DEP_Hydrants  
  - Description: Hydrant proximity adjustments  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `dep_hydrants_copy`  
    - Operation b: GenerateNearTable  
      - Params: distance = 10 Feet  
      - Output: `dep_hydrants_1`  
    - Operation c: MoveStreetSigns (custom)  
      - Params: as defined in script  
      - Output: `dep_hydrants_2`  
    - Operation d: Buffer  
      - Params: input = `dep_hydrants_2`, distance = 3 Feet  
      - Output: `dep_hydrants_ready`

- Curb_Cuts_Driveways  
  - Description: Driveway curb cuts to exclude planting  
  - Filters: `SUB_FEATURE_CODE = 222600`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `curb_cuts_driveways_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `curb_cuts_driveways_1`  
    - Operation c: Buffer  
      - Params: input = `curb_cuts_driveways_1`, distance = 15 Feet  
      - Output: `curb_cuts_driveways_ready`

## Final first step

- Final Clip  
  - Description: Compute allowable planting points within clipped sidewalk  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures (non-mutable backup)  
      - Params: input = `sidewalk_mutable_ready`  
      - Output: `sidewalk_mutable_backup`  
    - Operation b: Union no-plant zones  
      - Params: inputs = nyzd_ready, dep_gi_assets_ready, curb_cuts_ready, subway_lines_ready, workorders_ready, treeandsite_ready, grass_shrub_ready  
      - Output: `no_plant_zones_union`  
    - Operation c: Clip_analysis  
      - Params: input = `sidewalk_mutable_ready`, clip_features = `no_plant_zones_union`  
      - Output: `sidewalk_availability_ready`  
    - Operation d: Create planting points  
      - Params: method = CreateFishnet or custom  
      - Output: `plant_pts_1`  
    - Operation e: SpatialJoin  
      - Params: target = `plant_pts_1`, join_features = `sidewalk_availability_ready`  
      - Output: `plant_pts_ready`
```

## scrap/02.json

```json
[
    {
      "id": "trees",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/hn5i-inap.json",
      "schema": "political"
    },
    {
      "id": "work_orders",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/bdjm-n7q4.json",
      "schema": "political"
    },
    {
      "id": "planting_spaces",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/82zj-84is.json",
      "schema": "political"
    },
    {
      "id": "street_sign",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/qt6m-xctn.json",
      "schema": "political"
    },
    {
      "id": "hydrants",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/5bgh-vtsn.json",
      "schema": "political"
    },
    {
      "id": "green_infrastructure",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/df32-vzax.json",
      "schema": "political"
    },
    {
      "id": "subway_lines",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services.arcgis.com/ue9rwulIoeLEI9bj/arcgis/rest/services/NYC_SubwayLines/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "borough",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "community_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Community_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "council_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_City_Council_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "congressional_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/ArcGIS/rest/services/NYC_Congressional_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "senate_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_State_Senate_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "assembly_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_State_Assembly_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "community_tabulations",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Community_District_Tabulation_Areas_2020/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "neighborhood_tabulations",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Neighborhood_Tabulation_Areas_2020/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "census_tracts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Tracts_for_2020_US_Census/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "census_blocks",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Blocks_for_2020_US_Census/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "zoning_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyzd/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "commercial_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyco/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "special_purpose_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nysp/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "pluto",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/MAPPLUTO/FeatureServer/0",
      "schema": "land_use"
    },
    {
      "id": "street_center",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/DCM_Street_Center_Line/FeatureServer/0",
      "schema": "infrastructure"
    },
    {
      "id": "curb",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Curb_2022/FeatureServer/4",
      "schema": "infrastructure"
    },
    {
      "id": "curb_cut",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/ArcGIS/rest/services/Curb_Cut_2022/FeatureServer/5",
      "schema": "infrastructure"
    },
    {
      "id": "sidewalk",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Sidewalk_2022/FeatureServer/22",
      "schema": "infrastructure"
    }
  ]
```

## scrap/03.md

```markdown
2.1 Define source paths & queries:
   - All this information can be found within root/config/sources.json
    - trees
        - type: json
        - filter: `SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')` 

    - work_orders
        - type: json
        - filter: `STATUS <> 'Cancelled'`

    - street_sign
        - type: json
        - filter: *Need help*

    - hydrants
        - type: json
        - filter: None

    - green_infrastructure
        - type: json

    - subway_lines
        - shapefile

    - borough
        - shapefile

    - community_districts
        - shapefile

    - council_districts
        - shapefile

    - congressional_districts
        - shapefile

    - senate_districts
        - shapefile

    - assembly_districts
        - shapefile

    - community_tabulations
        - shapefile

    - neighborhood_tabulations
        - shapefile

    - census_tracts
        - shapefile

    - census_blocks
        - shapefile

    - zoning_districts
        - shapefile

    - commercial_districts
        - shapefile

    - special_purpose_districts
        - shapefile

    - pluto
        - shapefile

    - street_center
        - shapefile

    - curb
        - shapefile

    - curb_cut
        - shapefile

    - sidewalk
        - shapefile
```

## scrap/tokens.py

```python
import pathlib, tiktoken

enc  = tiktoken.get_encoding("cl100k_base")
root = pathlib.Path(r"C:\Projects\stp\src")

total = 0
for p in root.rglob("*.py"):
    try:
        text = p.read_text(encoding="utf‚Äë8")
    except UnicodeDecodeError:
        # last‚Äëditch: read as Latin‚Äë1 and ignore bad bytes
        text = p.read_text(encoding="latin‚Äë1", errors="ignore")
    total += len(enc.encode(text))

print(f"{total:,} tokens in {root}")
```

## scrap/zip.py

```python
"""
Zip up this project (no enclosing folder), excluding .git, .venv, env,
project.zip, and any .DS_Store files.
"""
import os
import zipfile

# Files or folders to skip entirely
EXCLUDES = {'.git', '.venv', 'env', 'project.zip'}
# Project-root name for the output archive
OUTPUT = 'project.zip'


def should_exclude(path: str) -> bool:
    """ Exclusions"""
    parts = path.split(os.sep)
    # skip any path containing an excluded folder
    if any(p in EXCLUDES for p in parts):
        return True
    # skip macOS DS_Store files
    if path.endswith('.DS_Store'):
        return True
    return False


def main() -> None:
    """ runs zip"""
    # remove old archive if it exists
    try:
        os.remove(OUTPUT)
    except OSError:
        pass

    with zipfile.ZipFile(OUTPUT, 'w', zipfile.ZIP_DEFLATED) as zf:
        for root, dirs, files in os.walk('.'):
            # prune dirs in-place so we never descend into .git/.venv/etc.
            dirs[:] = [d for d in dirs if d not in EXCLUDES]
            for fname in files:
                full = os.path.join(root, fname)
                if should_exclude(full):
                    continue
                # store relative path so no leading "./"
                arc = os.path.relpath(full, '.')
                zf.write(full, arc)


if __name__ == '__main__':
    main()
```

## secrets/secrets.yaml

```yaml
Socrate api: "7dUkig8gydigidDN6G638J8Lr"
```

## src/arcpy/arcpy converted/curb.py

```python
"""Generate curb polygons around lines using GeoPandas/Shapely."""
import math
from pathlib import Path

import geopandas as gpd
import yaml
from shapely.geometry import Polygon

def get_dominant_segment_angle(line):
    """Return angle (radians) of the longest segment in a LineString."""
    max_len = 0.0
    best_angle = 0.0
    coords = list(line.coords)
    for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):
        dx = x2 - x1
        dy = y2 - y1
        seg_len = math.hypot(dx, dy)
        if seg_len > max_len:
            max_len = seg_len
            best_angle = math.atan2(dy, dx)
    return best_angle

def generate_polygons(lines_gdf, extension_distance, buffer_width):
    """Return GeoDataFrame of rectangles around each line."""
    polys = []
    for line in lines_gdf.geometry:
        if line is None or len(line.coords) < 2:
            continue

        angle = get_dominant_segment_angle(line)
        sx, sy = line.coords[0]
        ex, ey = line.coords[-1]

        # Extend line ends
        sx -= extension_distance * math.cos(angle)
        sy -= extension_distance * math.sin(angle)
        ex += extension_distance * math.cos(angle)
        ey += extension_distance * math.sin(angle)

        # Buffer offset
        dx = (buffer_width / 2.0) * math.sin(angle)
        dy = (buffer_width / 2.0) * math.cos(angle)

        points = [
            (sx - dx, sy + dy),
            (ex - dx, ey + dy),
            (ex + dx, ey - dy),
            (sx + dx, sy - dy),
        ]
        polys.append(Polygon(points))

    return gpd.GeoDataFrame({"geometry": polys}, crs=lines_gdf.crs)

def main():
    """Entry point for CLI usage: build curb buffer polygons."""
    base_dir = Path.cwd()
    cfg_path = base_dir / "config" / "config.yaml"
    with open(cfg_path) as f:
        config = yaml.safe_load(f)

    curb_cfg = config.get("curb", {})
    ext_dist = float(curb_cfg.get("extension_distance", 0))
    buff_width = float(curb_cfg.get("buffer_width", 0))

    out_dir = Path(config.get("output_shapefiles", "Data/shapefiles"))
    gpkg = out_dir / "project_data.gpkg"

    lines = gpd.read_file(gpkg, layer="curb")
    polys = generate_polygons(lines, ext_dist, buff_width)

    polys.to_file(gpkg, layer="curb_buffer", driver="GPKG")
    print(f"‚úÖ wrote {gpkg} layer 'curb_buffer'")

    return gpkg


if __name__ == "__main__":
    main()
```

## src/arcpy/arcpy converted/nostandingy.py

```python
"""GeoPandas version of nostanding.py: Flag no-standing areas on sidewalks."""

import logging

import geopandas as gpd
import pandas as pd
import yaml
from shapely.affinity import translate
from shapely.geometry import Point
from shapely.ops import snap

logging.basicConfig(level=logging.INFO)  # Simple logging


def classify(raw):
    """Short code for sign text (same as original)."""
    txt = str(raw).upper()
    if "NO STANDING" in txt:
        return "NSTAND"
    if "NO PARKING" in txt:
        return "NPARK"
    if "HMP" in txt:
        return "HMP"
    if "TAXI" in txt or "HOTEL" in txt or "LOADING" in txt or "PASSENGER" in txt:
        return "CURBSIDE"
    return "OTHER"


def load_filter(csv_path, desc_f, side_f):
    """Clean and filter sign DataFrame (same as original, but returns GDF)."""
    df = pd.read_csv(csv_path)
    df[side_f] = df[side_f].astype(str).str.strip().str.upper().str[0]
    df = df[df[side_f].isin(['N', 'S', 'E', 'W'])]
    keep = "NO STANDING|NO PARKING|HMP|TAXI|HOTEL|LOADING|PASSENGER"
    df = df[df[desc_f].str.upper().str.contains(keep, na=False)]
    df = df[pd.to_numeric(df["sign_x_coord"], errors='coerce').notna()]
    df = df[pd.to_numeric(df["sign_y_coord"], errors='coerce').notna()]
    df["x_r"] = df["sign_x_coord"].round(1)
    df["y_r"] = df["sign_y_coord"].round(1)
    df = df.drop_duplicates(subset=["x_r", "y_r"]).reset_index(drop=True)

    def _pa(s):
        s = str(s).upper()
        if "<->" in s:
            return "<->"
        elif "-->" in s or "->" in s:
            return "->"
        elif "<--" in s or "<-" in s:
            return "<-"
        return None
    df["parsed_arrow"] = df[desc_f].apply(_pa)
    df = df[df["parsed_arrow"].notna()]
    df["sign_type"] = df[desc_f].map(classify)
    df["geometry"] = df.apply(lambda row: Point(row["sign_x_coord"], row["sign_y_coord"]), axis=1)
    gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:2263')
    logging.info(f"Loaded and filtered {len(gdf)} signs.")
    return gdf


def shift_points(gdf, side_f, shift_ft):
    """Shift points off curb based on side."""
    def shift_pt(pt, side):
        if side == 'N':
            return translate(pt, 0, shift_ft)
        elif side == 'S':
            return translate(pt, 0, -shift_ft)
        elif side == 'E':
            return translate(pt, shift_ft, 0)
        elif side == 'W':
            return translate(pt, -shift_ft, 0)
        return pt
    gdf['geometry'] = gdf.apply(lambda row: shift_pt(row.geometry, row[side_f]), axis=1)
    logging.info("Shifted signs.")
    return gdf


def snap_to_sidewalks(signs_gdf, sw_gdf, tolerance=60):
    """Snap signs to nearest sidewalk within tolerance feet."""
    signs_gdf = gpd.sjoin_nearest(signs_gdf, sw_gdf, distance_col='near_dist', max_distance=tolerance)
    signs_gdf = signs_gdf[signs_gdf['near_dist'].notna()]  # Drop orphans
    
    def snap_geom(row):
        if pd.isna(row['near_dist']):
            return row.geometry
        line = sw_gdf.loc[row['index_right']].geometry
        return snap(row.geometry, line, tolerance)
    signs_gdf['geometry'] = signs_gdf.apply(snap_geom, axis=1)
    logging.info(f"Snapped {len(signs_gdf)} signs to sidewalks.")
    return signs_gdf


def segment_compass(seg_geom, sign_pt):
    """Compass direction relative to sign (north/south/east/west)."""
    dx = seg_geom.centroid.x - sign_pt.centroid.x
    dy = seg_geom.centroid.y - sign_pt.centroid.y
    if abs(dx) >= abs(dy):
        return "east" if dx > 0 else "west"
    return "north" if dy > 0 else "south"


def erase_gaps_and_flag(sw_gdf, signs_gdf, side_f):
    """Erase 3ft gaps at signs, explode, flag no_stand."""
    gaps = signs_gdf.buffer(1.5)  # 3ft diameter
    sw_split = sw_gdf.difference(gaps.unary_union)
    sw_split = sw_split.explode(ignore_index=True)  # Multipart to singlepart
    sw_split = sw_split[sw_split.length > 0.05]  # Drop slivers

    # Spatial join segments to signs (within 2ft)
    joined = gpd.sjoin(sw_split, signs_gdf, how='left', predicate='within', distance=2)
    
    # Add compass and flag
    arrow_to_compass = {
        "E": {"<-": "north", "->": "south"},
        "W": {"<-": "south", "->": "north"},
        "N": {"<-": "west",  "->": "east"},
        "S": {"<-": "east",  "->": "west"},
    }

    def get_compass(row):
        if row['parsed_arrow'] == "<->":
            return "both"
        curb = row[side_f].strip().upper() if pd.notna(row[side_f]) else ""
        arr = row['parsed_arrow'].strip() if pd.notna(row['parsed_arrow']) else ""
        return arrow_to_compass.get(curb, {}).get(arr)
    joined['compass'] = joined.apply(get_compass, axis=1)
    
    def get_flag(row):
        if row['compass'] == "both":
            return 1
        seg_side = segment_compass(row.geometry, signs_gdf.loc[row['index_right']].geometry)
        return 1 if seg_side == row['compass'] else 0
    joined['no_stand'] = joined.apply(get_flag, axis=1)
    
    # Aggregate max no_stand per segment (groupby)
    aggregated = joined.groupby(joined.index)['no_stand'].max().reset_index()
    sw_split['no_stand'] = aggregated['no_stand']
    
    logging.info("Erased gaps and flagged segments.")
    return sw_split


def main(csv_path, sw_path, out_path, desc_f='desc', side_f='side', shift_ft=5.0):
    """Run the full no-standing process."""
    try:
        with open('config/config.yaml') as f:  # Use your config
            config = yaml.safe_load(f)
        # Override with config if needed
        
        signs_gdf = load_filter(csv_path, desc_f, side_f)
        sw_gdf = gpd.read_file(sw_path)  # e.g., GeoPackage layer
        signs_gdf = shift_points(signs_gdf, side_f, shift_ft)
        signs_gdf = snap_to_sidewalks(signs_gdf, sw_gdf)
        final_gdf = erase_gaps_and_flag(sw_gdf, signs_gdf, side_f)
        final_gdf.to_file(out_path, driver='GPKG')
        logging.info(f"Done! Output: {out_path}")
    except Exception as e:
        logging.error(f"Oops: {e}")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Process no-standing signs.")
    parser.add_argument('--csv', default='data/signs.csv', help='Sign CSV path')
    parser.add_argument('--sw', default='data/project_data.gpkg|layer=sidewalks', help='Sidewalk GPKG')
    parser.add_argument('--out', default='data/no_stand.gpkg', help='Output GPKG')
    args = parser.parse_args()
    main(args.csv, args.sw, args.out)
```

## src/arcpy/arcpy converted/rank.py

```python
"""GeoPandas version of rank_dominant_working.py: Prune planting points."""

import logging
import geopandas as gpd
from shapely import line_interpolate_point, length

logging.basicConfig(level=logging.INFO)


def generate_points_along_lines(lines_gdf, spacing):
    """Create points every 'spacing' feet along lines."""
    points = []
    for idx, row in lines_gdf.iterrows():
        line = row.geometry
        dist = 0
        while dist < length(line):
            pt = line_interpolate_point(line, dist)
            points.append({'geometry': pt, 'parent_fid': idx, 'parent_len': length(line)})
            dist += spacing
    pts_gdf = gpd.GeoDataFrame(points, crs=lines_gdf.crs)
    logging.info(f"Generated {len(pts_gdf)} points.")
    return pts_gdf


def resolve_conflicts(pts_gdf, lines_gdf, spacing, buffer_dist, max_iter=3):
    """Iteratively prune conflicting points."""
    for iter_n in range(1, max_iter + 1):
        # Find near pairs (like GenerateNearTable)
        joined = gpd.sjoin_nearest(pts_gdf, pts_gdf, distance_col='dist', max_distance=buffer_dist)
        joined = joined[joined.index != joined['index_right']]  # No self-joins
        
        if joined.empty:
            break
        
        # Rank and pick winners/losers (longer parent wins)
        winners, losers = set(), set()
        for _, row in joined.iterrows():
            i, j = row.name, row['index_right']
            len_i, len_j = pts_gdf.loc[i]['parent_len'], pts_gdf.loc[j]['parent_len']
            if len_i > len_j or (len_i == len_j and i < j):
                winners.add(i)
                losers.add(j)
            else:
                winners.add(j)
                losers.add(i)
        
        if not losers:
            break
        
        # Buffer losers and erase from lines
        loser_buffers = pts_gdf.loc[list(losers)].buffer(buffer_dist - 0.01)
        lines_gdf = lines_gdf.difference(loser_buffers.unary_union)
        lines_gdf = lines_gdf[lines_gdf.length >= 3]  # Filter short lines
        
        # Regenerate points
        pts_gdf = generate_points_along_lines(lines_gdf, spacing)
    
    logging.info(f"Resolved after {iter_n} iterations.")
    return pts_gdf, lines_gdf


def main(line_path, out_pts_path, out_lines_path, spacing=25.0, buffer_dist=30.0):
    "Main"
    try:
        lines_gdf = gpd.read_file(line_path)
        pts_gdf = generate_points_along_lines(lines_gdf, spacing)
        final_pts, final_lines = resolve_conflicts(pts_gdf, lines_gdf, spacing, buffer_dist)
        final_pts.to_file(out_pts_path)
        final_lines.to_file(out_lines_path)
        logging.info("Done pruning!")
    except Exception as e:
        logging.error(f"Oops: {e}")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Prune planting points.")
    parser.add_argument('--lines', default='data/sidewalks.gpkg', help='Input lines GPKG')
    parser.add_argument('--out_pts', default='data/plant_points.gpkg', help='Output points')
    parser.add_argument('--out_lines', default='data/pruned_lines.gpkg', help='Output lines')
    parser.add_argument('--spacing', type=float, default=25.0)
    parser.add_argument('--buffer', type=float, default=30.0)
    args = parser.parse_args()
    main(args.lines, args.out_pts, args.out_lines, args.spacing, args.buffer)
```

## src/arcpy/arcpy depracated/arcpy_nostanding.py

```python
import os
import pandas as pd
import arcpy

arcpy.env.overwriteOutput = True
scratch = arcpy.env.scratchGDB

def classify(raw):
    """Return a short code describing the sign text."""
    txt = str(raw).upper()
    if "NO STANDING" in txt:
        return "NSTAND"
    if "NO PARKING" in txt:
        return "NPARK"
    if "HMP" in txt:
        return "HMP"
    if "TAXI" in txt or "HOTEL" in txt or "LOADING" in txt or "PASSENGER" in txt:
        return "CURBSIDE"
    return "OTHER"

def load_filter(csv_path, desc_f, side_f):
    """Return cleaned DataFrame of sign records filtered by text and side."""
    df = pd.read_csv(csv_path)
    
    # normalize N/S/E/W
    df[side_f] = (df[side_f]
                  .astype(str).str.strip()
                  .str.upper().str[0]
                  .where(lambda s: s.isin(list("NSEW"))))
    df = df[df[side_f].notna()]

    # keep only our keywords
    keep = "NO STANDING|NO PARKING|HMP|TAXI|HOTEL|LOADING|PASSENGER"
    df = df[df[desc_f].str.upper().str.contains(keep, na=False)]

    # valid coords
    df = df[pd.to_numeric(df["sign_x_coord"], "coerce").notna()]
    df = df[pd.to_numeric(df["sign_y_coord"], "coerce").notna()]

    # dedupe on 1-ft grid
    df["x_r"] = df["sign_x_coord"].round(1)
    df["y_r"] = df["sign_y_coord"].round(1)
    df = df.drop_duplicates(subset=["x_r", "y_r"]).reset_index(drop=True)

    # parse arrow glyphs
    def _pa(s):
        s = str(s).upper()
        if "<->" in s:
            return "<->"
        elif "-->" in s or "->" in s:
            return "->"
        elif "<--" in s or "<-" in s:
            return "<-"
    df["parsed_arrow"] = df[desc_f].apply(_pa)
    df = df[df["parsed_arrow"].notna()]

    return df

# helper that classifies the segment side
def segment_compass(seg_geom, sign_pt):
    """Return 'north', 'south', 'east', or 'west' relative to sign_pt."""
    dx = seg_geom.centroid.X - sign_pt.centroid.X
    dy = seg_geom.centroid.Y - sign_pt.centroid.Y
    if abs(dx) >= abs(dy):  # horizontal distance dominates or ties
        return "east" if dx > 0 else "west"
    return "north" if dy > 0 else "south"

# ‚îÄ‚îÄ 0. Params ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 0: csv_path, 1: sw_fc, 2: cen_fc, 3: out_fc, 4: desc_f, 5: side_f, 6: shift_ft
csv_path, sw_fc, cen_fc, out_fc, desc_f, side_f, shift_ft = [
    arcpy.GetParameterAsText(i) for i in range(7)
]
shift_ft = float(shift_ft)

# ‚îÄ‚îÄ 0.1 Copy & integrate centerlines (don‚Äôt mutate source)‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
cen_work = os.path.join(scratch, "cen_work")
arcpy.management.CopyFeatures(cen_fc, cen_work)
arcpy.management.Integrate(cen_work, "0.01 Feet")

# ‚îÄ‚îÄ 1.  CSV ‚Üí point feature class  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
df = load_filter(csv_path, desc_f, side_f)
df["sign_type"] = df[desc_f].map(classify)
df["jid"] = df.index
tmp_csv = os.path.join(scratch, "clean_signs.csv")
df.to_csv(tmp_csv, index=False,
          columns=["jid", "sign_x_coord", "sign_y_coord", side_f, "parsed_arrow","sign_type"])
arcpy.AddMessage(f"Cleaned signs CSV ‚Üí {tmp_csv}")

sr = arcpy.SpatialReference(2263)          # NY State Plane Feet
signs = os.path.join(scratch, "signs_pts")
arcpy.management.XYTableToPoint(tmp_csv, signs,
                                "sign_x_coord", "sign_y_coord",
                                coordinate_system=sr)
arcpy.AddMessage(f"Signs ‚Üí point FC: {signs}")

# ‚îÄ‚îÄ 2.  Curb offset  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
signs_shifted = os.path.join(scratch, "signs_shifted")
arcpy.management.CopyFeatures(signs, signs_shifted)

with arcpy.da.UpdateCursor(signs_shifted, ["SHAPE@", side_f]) as cur:
    for shp, sd in cur:
        dx, dy = {"N": (0,  shift_ft),
                  "S": (0, -shift_ft),
                  "E": ( shift_ft, 0),
                  "W": (-shift_ft, 0)}.get((sd or "").upper(), (0, 0))
        pt = shp.centroid
        new_pt = arcpy.PointGeometry(arcpy.Point(pt.X + dx, pt.Y + dy), sr)
        cur.updateRow([new_pt, sd])

arcpy.AddMessage("‚ÜîÔ∏è  Shifted signs off original location.")

# ‚îÄ‚îÄ 3.  Build working sidewalk copy  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
arcpy.env.overwriteOutput = True
sw_work = os.path.join(scratch, "sw_work")
if arcpy.Exists(sw_work):
    arcpy.Delete_management(sw_work)
arcpy.management.CopyFeatures(sw_fc, sw_work)
arcpy.AddMessage(f"Copied sidewalks ‚Üí {sw_work}")

# ‚îÄ‚îÄ 4.  Snap curb‚Äëshifted signs to their sidewalk  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
signs_snapped_sw = os.path.join(scratch, "signs_snapped_sw")
arcpy.management.CopyFeatures(signs_shifted, signs_snapped_sw)

near_sidewalk = os.path.join(scratch, "near_sidewalk")
arcpy.analysis.GenerateNearTable(signs_snapped_sw, sw_work,
                                 near_sidewalk, search_radius="60 Feet",
                                 closest="CLOSEST")

arcpy.management.JoinField(signs_snapped_sw,      # target
                           "OBJECTID",            # sign OID
                           near_sidewalk,         # near table
                           "IN_FID",              # join key
                           ["NEAR_FID"])          # adds curb sidewalk OID
arcpy.AddMessage("üìå  Joined NEAR_FID onto curb‚Äëshifted signs.")

# lookup:  sidewalk OID  ‚Üí  geometry
sw_geom = {oid: g for oid, g in arcpy.da.SearchCursor(sw_work, ["OID@", "SHAPE@"])}

with arcpy.da.UpdateCursor(signs_snapped_sw, ["SHAPE@", "NEAR_FID"]) as cur:
    for shp, nid in cur:
        seg = sw_geom.get(nid)
        if seg:
            meas = seg.measureOnLine(shp.centroid)
            cur.updateRow([seg.positionAlongLine(meas), nid])

arcpy.AddMessage(f"Snapped signs ‚Üí sidewalks: {signs_snapped_sw}")

# ‚îÄ‚îÄ 4.2  Filter out signs that never snapped to a sidewalk ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# A.  signs whose NEAR_FID is NULL  ‚Üí  sign_errors
sign_errors = os.path.join(scratch, "sign_errors")
arcpy.analysis.Select(signs_snapped_sw, sign_errors, "NEAR_FID IS NULL")

# B.  signs we want to ignore (e.g. sign_type = 'NPARK')  ‚Üí  sign_skip
sign_skip = os.path.join(scratch, "sign_skip")
arcpy.analysis.Select(signs_snapped_sw, sign_skip, "sign_type = 'NPARK'")

# C.  delete both kinds from the working sign layer
with arcpy.da.UpdateCursor(signs_snapped_sw,
        ["NEAR_FID", "sign_type"]) as cur:

    for nid, stype in cur:
        if nid is None or stype == "NPARK":
            cur.deleteRow()

cnt_orphan = int(arcpy.management.GetCount(sign_errors)[0])
cnt_skip   = int(arcpy.management.GetCount(sign_skip)[0])

arcpy.AddMessage(f"üõà  {cnt_orphan} orphans ‚Üí sign_errors; "
                 f"{cnt_skip} NPARK signs ‚Üí sign_skip.")

# ------------------------------------------------------------
# 4.  Keep only sidewalks that have at least one matched sign
# ------------------------------------------------------------
# 4‚ÄëA  sid list from NEAR_FID
ids = {oid for oid, in arcpy.da.SearchCursor(signs_snapped_sw, ["NEAR_FID"])
        if oid is not None}

if not ids:
    arcpy.AddWarning("No sidewalks matched any sign within 60‚ÄØft ‚Äî nothing to process.")
    raise SystemExit()

id_sql = f"OBJECTID IN ({','.join(map(str, ids))})"

sw_work_has_sign = os.path.join(scratch, "sw_work_has_sign")
arcpy.analysis.Select(sw_work, sw_work_has_sign, id_sql)

arcpy.AddMessage(f"Sidewalks with signs ‚Üí {sw_work_has_sign} ({len(ids)})")

# join the side code from sidewalk layer onto the sign layer
arcpy.management.JoinField(signs_snapped_sw,       # target
                           "NEAR_FID",             # key on sign
                           sw_work_has_sign,       # source sidewalks
                           "OBJECTID",
                           [side_f])               # e.g. SIDE_CODE

# add a compass field and populate it
arcpy.management.AddField(signs_snapped_sw, "COMPASS", "TEXT", field_length=5)

arrow_to_compass = {
    "E": {"<-": "north", "->": "south"},
    "W": {"<-": "south", "->": "north"},
    "N": {"<-": "west",  "->": "east"},
    "S": {"<-": "east",  "->": "west"},
}

with arcpy.da.UpdateCursor(signs_snapped_sw,
        [side_f, "parsed_arrow", "COMPASS"]) as cur:

    for curb, arr, comp in cur:
        curb = (curb or "").strip().upper()
        arr  = (arr  or "").strip()

        if arr == "<->":
            comp = "both"
        else:
            comp = arrow_to_compass.get(curb, {}).get(arr)

        cur.updateRow([curb, arr, comp])

# ------------------------------------------------------------
# 5.  Erase a ¬±1.5‚ÄØft window around each sign (buffer‚Äëerase)
# ------------------------------------------------------------

# 5‚ÄëA  buffer the signs once
sign_buf = os.path.join(scratch, "sign_buf")
arcpy.analysis.Buffer(signs_snapped_sw, sign_buf, "1.5 Feet",
                      dissolve_option="NONE")

# Erase a ¬±1.5‚ÄØft window around each sign
sw_split_raw = os.path.join(scratch, "sw_split_raw")
arcpy.analysis.PairwiseErase(sw_work_has_sign, sign_buf, sw_split_raw)
arcpy.AddMessage(f"Erased 3‚ÄØft gaps at signs ‚Üí {sw_split_raw}")

# explode multipart to singlepart
sw_split = os.path.join(scratch, "sw_split")              # final working copy
arcpy.management.MultipartToSinglepart(sw_split_raw, sw_split)

# drop zero‚Äëlength slivers
sliver_sql = "SHAPE_Length < 0.05"          # adjust threshold if needed
arcpy.management.MakeFeatureLayer(sw_split, "split_lyr", sliver_sql)
if int(arcpy.management.GetCount("split_lyr")[0]) > 0:
    arcpy.management.DeleteFeatures("split_lyr")
arcpy.management.Delete("split_lyr")

# ------------------------------------------------------------
# 6.  Flag before / after / both via COMPASS logic
# ------------------------------------------------------------

# --- sign geometry lookup (needed inside the cursor) ----------
sign_geom = {oid: geom for oid, geom
             in arcpy.da.SearchCursor(signs_snapped_sw,
                                      ["OBJECTID", "SHAPE@"])}

# 1) ONE‚ÄëTO‚ÄëMANY spatial join: every segment ‚Üî every sign ‚â§ 2‚ÄØft
sw_join = os.path.join(scratch, "sw_split_joined")
arcpy.analysis.SpatialJoin(
    target_features   = sw_split,          # exploded single‚Äëpart segments
    join_features     = signs_snapped_sw,  # snapped signs (with COMPASS)
    out_feature_class = sw_join,
    join_operation    = "JOIN_ONE_TO_MANY",
    match_option      = "WITHIN_A_DISTANCE",
    search_radius     = "2 Feet"
)

# rename join keys for clarity
arcpy.management.AlterField(sw_join, "TARGET_FID", new_field_name="SEG_ID")
arcpy.management.AlterField(sw_join, "JOIN_FID",   new_field_name="SIGN_OID")
arcpy.management.JoinField(sw_join,          # target = sw_join rows
                           "SIGN_OID",       # key in sw_join
                           signs_snapped_sw, # source signs
                           "OBJECTID",       # key in signs
                           ["sign_type"])    # field(s) to copy

# add the flag field *before* the cursor
arcpy.management.AddField(sw_join, "no_stand", "SHORT")

# 2) row‚Äëby‚Äërow flag
with arcpy.da.UpdateCursor(
        sw_join,
        ["SEG_ID", "SIGN_OID", "COMPASS", "no_stand", "SHAPE@"]) as cur:

    for seg_id, soid, comp, flag, seg in cur:
        if comp == "both":
            flag = 1
        else:
            sign_pt  = sign_geom.get(soid)
            seg_side = segment_compass(seg, sign_pt)
            flag     = int(seg_side == comp) if seg_side else 0
        cur.updateRow([seg_id, soid, comp, flag, seg])

# 3) collapse duplicates: MAX(no_stand) per segment
stat_tbl = os.path.join(scratch, "seg_flag_stat")
arcpy.analysis.Statistics(
        sw_join, stat_tbl,
        [["no_stand", "MAX"]],
        case_field="SEG_ID")

arcpy.management.AlterField(stat_tbl, "MAX_no_stand",
                            new_field_name="no_stand")

# choose "MIN_SIGN_TYPE" = the alphabetically first type for that segment
type_tbl = os.path.join(scratch, "seg_type_stat")
arcpy.analysis.Statistics(
    sw_join, type_tbl,
    [["sign_type", "MIN"]],          # MIN, MAX, or COUNT
    case_field="SEG_ID")

arcpy.management.AlterField(type_tbl, "MIN_sign_type",
                            new_field_name="sign_type")

# join both flag + type back onto sw_split
arcpy.management.JoinField(sw_split, "OBJECTID",
                           stat_tbl,  "SEG_ID", ["no_stand"])
arcpy.management.JoinField(sw_split, "OBJECTID",
                           type_tbl,  "SEG_ID", ["sign_type"])

# 4) join the final flag back to the single‚Äëpart sidewalk layer
#    remove any placeholder no_stand first
if "no_stand" in [f.name for f in arcpy.ListFields(sw_split)]:
    arcpy.management.DeleteField(sw_split, ["no_stand"])

arcpy.management.JoinField(sw_split,          # target = segments
                           "OBJECTID",        # key in sw_split
                           stat_tbl,          # source table
                           "SEG_ID",          # key in stats table
                           ["no_stand"])

arcpy.AddMessage("‚úÖ  Segments flagged via COMPASS logic")

# ------------------------------------------------------------
# 8.  Clean up & export
# ------------------------------------------------------------
arcpy.management.CopyFeatures(sw_split, out_fc)     # overwriteOutput should be True
arcpy.AddMessage(f"üéâ  Final no‚Äëstanding layer ‚Üí {out_fc}")
```

## src/arcpy/arcpy depracated/arcpy_rank_dominant_working.py

```python
import arcpy
from pathlib import Path
import pandas as pd
from collections import defaultdict

arcpy.ImportToolbox(r"D:\ArcGIS\Projects\Street_Tree_Planting_Analysis\Street_Tree_Planting_Analysis.atbx", "stp")

def generate_initial_points(line_fc, spacing, work_gdb):
    """Copy input lines, then create and rank an initial set of points."""
    lines_working = str(work_gdb / "lines_working")
    if arcpy.Exists(lines_working):
        arcpy.management.Delete(lines_working)
    
    arcpy.management.CopyFeatures(str(line_fc), lines_working)

    pts = str(work_gdb / "pts_0")
    arcpy.stp.generatepoints(lines_working, str(spacing), pts)
    arcpy.stp.addrankfields(pts, lines_working)

    return lines_working, pts

def identify_non_conflicting_points(pts, spacing, work_gdb):
    """Remove points that intersect within 'spacing' feet of each other."""
    pts_work = str(work_gdb / "pts_0_working")
    arcpy.management.CopyFeatures(pts, pts_work)

    conflicts_fc = str(work_gdb / "potential_conflicts")
    arcpy.analysis.SpatialJoin(
        target_features=pts_work,
        join_features=pts_work,
        out_feature_class=conflicts_fc,
        join_operation="JOIN_ONE_TO_MANY",
        match_option="INTERSECT",
        search_radius=f"{spacing} Feet"
    )

    # Filter out self-joins so only overlapping points remain

    true_conflicts = str(work_gdb / "true_conflicts")
    arcpy.management.MakeFeatureLayer(conflicts_fc, "conflicts_lyr")
    arcpy.management.SelectLayerByAttribute("conflicts_lyr", "NEW_SELECTION", '"PARENT_OID" <> "PARENT_OID_1"')
    arcpy.management.CopyFeatures("conflicts_lyr", true_conflicts)

    conflict_ids = {row[0] for row in arcpy.da.SearchCursor(true_conflicts, ["TARGET_FID"])}

    # Remove points whose OID appears in the conflict table
    good_boys_fc = str(work_gdb / "pts_0_cleaned")
    with arcpy.da.UpdateCursor(pts_work, ["OBJECTID"]) as cursor:
        for row in cursor:
            if row[0] in conflict_ids:
                cursor.deleteRow()

    arcpy.management.CopyFeatures(pts_work, good_boys_fc)
    return good_boys_fc

def resolve_conflicts_iteratively(pts, lines_working, spacing, buffer_dist, work_gdb, max_iterations=3):
    """Iteratively buffer and erase lines to remove conflicting planting points."""
    iter_n = 0
    line_suppress_map = defaultdict(list)

    while True:
        iter_n += 1
        if iter_n > max_iterations:
            arcpy.AddWarning("Max iterations reached.")
            break
        
        line_suppress_map = defaultdict(list)

        near_tbl = str(work_gdb / f"near_{iter_n}")
        arcpy.analysis.GenerateNearTable(
            pts, pts, near_tbl, f"{buffer_dist} Feet", "NO_LOCATION",
            closest="ALL", method="PLANAR"
        )  # produces pairwise distances between points

        if int(arcpy.management.GetCount(near_tbl)[0]) == 0:
            break

        df = pd.DataFrame(arcpy.da.TableToNumPyArray(near_tbl, ["IN_FID", "NEAR_FID"]))
        df = df[df.IN_FID < df.NEAR_FID]

        rank = {}
        with arcpy.da.SearchCursor(pts, ["OID@", "PARENT_LEN", "PARENT_FID"]) as cur:
            for oid, ln_len, ln_fid in cur:
                rank[oid] = (ln_len, ln_fid)

        winners, losers = set(), set()
        for _, row in df.iterrows():
            i, j = int(row.IN_FID), int(row.NEAR_FID)
            if i not in rank or j not in rank or rank[i][1] == rank[j][1]:
                continue
            r_i, r_j = rank[i][0], rank[j][0]
            if r_i > r_j or (r_i == r_j and i < j):
                winners.add(i)
                losers.add(j)
                line_suppress_map[rank[j][1]].append(i)
            else:
                winners.add(j)
                losers.add(i)
                line_suppress_map[rank[i][1]].append(j)

        if not winners:
            break

        # Points to buffer are the losers grouped by parent line
        suppression_pts = list({oid for pts in line_suppress_map.values() for oid in pts})
        existing_oids = {row[0] for row in arcpy.da.SearchCursor(pts, ["OID@"])}
        suppression_pts = [oid for oid in suppression_pts if oid in existing_oids]

        if not suppression_pts:
            continue

        suppression_pt_layer = arcpy.management.MakeFeatureLayer(pts, "suppression_pts")[0]
        oid_field = arcpy.Describe(suppression_pt_layer).OIDFieldName
        arcpy.management.SelectLayerByAttribute(
            suppression_pt_layer, "NEW_SELECTION",
            f"{oid_field} IN ({','.join(map(str, suppression_pts))})"
        )

        # Defensive check before buffering
        count = int(arcpy.management.GetCount(suppression_pt_layer)[0])
        arcpy.AddMessage(f"üß™ Suppression point layer count: {count}")
        if count == 0:
            arcpy.AddMessage("‚ö†Ô∏è No suppression points selected ‚Äî skipping regeneration.")
            continue  # skip this iteration

        # Now it's safe to buffer
        buffer_fc = work_gdb / f"iter_{iter_n}_winner_buffers"
        arcpy.analysis.PairwiseBuffer(suppression_pt_layer, str(buffer_fc), f"{buffer_dist - 0.01} Feet", dissolve_option="ALL")

        losing_lines = list(line_suppress_map.keys())
        losing_line_layer = arcpy.management.MakeFeatureLayer(lines_working, "losing_losing_line_layer")[0]
        oid_field = arcpy.Describe(losing_line_layer).OIDFieldName
        arcpy.management.SelectLayerByAttribute(
            losing_line_layer, "NEW_SELECTION", f"{oid_field} IN ({','.join(map(str, losing_lines))})"
        )

        arcpy.AddMessage(f"üß™ Trying to erase these lines: {losing_lines[:5]}... (total: {len(losing_lines)})")

        count = int(arcpy.management.GetCount(str(losing_line_layer))[0])
        arcpy.AddMessage(f"üìä Selected lines for erase: {count}")
        if count == 0:
            arcpy.AddWarning("‚ö†Ô∏è No losing lines selected ‚Äî skipping erase this round.")
            continue

        # Optional geometry repair
        arcpy.management.RepairGeometry(str(buffer_fc), "DELETE_NULL")

        # Run erase
        trimmed_losing_lines = work_gdb / f"iter_{iter_n}_trimmed_losing_lines"
        arcpy.analysis.PairwiseErase(losing_line_layer, str(buffer_fc), str(trimmed_losing_lines))

        # Select survivor lines (NOT in losing_lines)
        survivor_lines = work_gdb / f"iter_{iter_n}_survivor_lines"
        arcpy.management.MakeFeatureLayer(lines_working, "lines_working_lyr")
        arcpy.management.SelectLayerByAttribute(
            "lines_working_lyr", "NEW_SELECTION",
            f"{oid_field} NOT IN ({','.join(map(str, losing_lines))})"
        )
        arcpy.management.CopyFeatures("lines_working_lyr", str(survivor_lines))

        # Merge survivors + trimmed losers
        merged_lines = work_gdb / f"iter_{iter_n}_merged_lines"
        arcpy.management.Merge([str(survivor_lines), str(trimmed_losing_lines)], str(merged_lines))

        # Filter out final lines under X feet before generating points
        min_line_length = 3
        lines_filtered = str(work_gdb / f"iter_{iter_n}_filtered_lines")

        # Create in-memory layer from merged result
        filtered_layer = arcpy.management.MakeFeatureLayer(str(merged_lines), f"merged_lines_lyr_{iter_n}")[0]

        # Select only lines that meet minimum length
        arcpy.management.SelectLayerByAttribute(
            filtered_layer, "NEW_SELECTION",
            f"Shape_Length >= {min_line_length}"
        )

        # Save filtered version to new feature class
        arcpy.management.CopyFeatures(filtered_layer, lines_filtered)

        # Set for next round
        lines_working = lines_filtered

        # Step: Regenerate points
        pts = str(work_gdb / f"pts_{iter_n}_regenerated")
        arcpy.stp.generatepoints(lines_working, str(spacing), pts)
        arcpy.stp.addrankfields(pts, lines_working)

    return None, None, iter_n, lines_working

def rank_dominant_prune(line_fc: str, out_pts_fc: str, final_lines_fc: str, spacing: float, buffer_dist: float) -> None:
    """High level prune workflow used by the ArcGIS tool."""
    work_gdb = Path(arcpy.env.scratchGDB)
    arcpy.env.workspace = str(work_gdb)
    arcpy.env.overwriteOutput = True

    # Step 1: Generate working copy of input lines and initial points
    lines_working, pts = generate_initial_points(line_fc, spacing, work_gdb)

    # Step 2: Run all pruning ‚Äî this mutates lines_working
    _, _, _, final_lines = resolve_conflicts_iteratively(
        pts, lines_working, spacing, buffer_dist, work_gdb
    )

    # Step 3: Generate final planting points from clean geometry
    arcpy.stp.generatepoints(str(final_lines), str(spacing), out_pts_fc)
    arcpy.AddMessage(f"üå≥ Final planting points generated from trimmed geometry: {out_pts_fc}")

    # Step 4: Generate final plantable sidewalk from clean geometry
    arcpy.management.CopyFeatures(final_lines, final_lines_fc)
    arcpy.AddMessage(f"üóÇ Final pruned sidewalk geometry saved to: {final_lines_fc}")

# ArcGIS Tool Entry
if __name__ == "__main__":
    line_fc = arcpy.GetParameterAsText(0)
    out_pts_fc = arcpy.GetParameterAsText(1)
    final_lines_fc = arcpy.GetParameterAsText(2)
    spacing = float(arcpy.GetParameterAsText(3))
    buffer_dist = float(arcpy.GetParameterAsText(4))

    rank_dominant_prune(line_fc, out_pts_fc, final_lines_fc, spacing, buffer_dist)
```

## src/arcpy/arcpy depracated/arcpy_Second_Step_Alternative.py

```python
# -*- coding: utf-8 -*-
"""
Generated by ArcGIS ModelBuilder on : 2025-06-10 11:33:01
"""
import arcpy

def SecondStepAlternative():  # Second_Step_Alternative
    """ArcPy model step generating alternative curb buffers."""

    # To allow overwriting outputs change overwriteOutput option to True.
    arcpy.env.overwriteOutput = False

    # Model Environment settings
    with arcpy.EnvManager(scratchWorkspace="D:\\ArcGIS\\Projects\\Street_Tree_Planting_Analysis\\Scrap.gdb"):
        SIDEWALK_3_ = "D:\\ArcGIS\\Data\\FileGDB-data-Planimetric_2022_AGOL_Link.gdb\\SIDEWALK"
        PLUTO = "D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped"
        Field_Map = "Borough \"Borough\" true false false 2 Text 0 0,First,#,D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped,Borough,0,1;Block \"Block\" true false false 4 Long 0 0,First,#,D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped,Block,-1,-1;Lot \"Lot\" true false false 2 Short 0 0,First,#,D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped,Lot,-1,-1;CD \"CD\" true true false 2 Short 0 0,First,#,D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped,CD,-1,-1"

        # Process: Copy Pluto (Export Features) (conversion)
        PLUTO_Copy = fr"{arcpy.env.scratchGDB}\Pluto_Copy"
        arcpy.conversion.ExportFeatures(in_features=PLUTO, out_features=PLUTO_Copy, field_mapping=Field_Map)

        # Process: Calculate Unique Blocks (Calculate Field) (management)
        PLUTO_Unique_Blocks = arcpy.management.CalculateField(in_table=PLUTO_Copy, field="Unique_Blocks", expression="Concatenate($feature.Borough,$feature.Block)", expression_type="ARCADE")[0]

        # Process: Pairwise Dissolve (Pairwise Dissolve) (analysis)
        PLUTO_Dissolve = fr"{arcpy.env.scratchGDB}\PLUTO_Dissolve"
        arcpy.analysis.PairwiseDissolve(in_features=PLUTO_Unique_Blocks, out_feature_class=PLUTO_Dissolve, dissolve_field=["Unique_Blocks"], multi_part="SINGLE_PART")

        # Process: Sidewalk Erase (Pairwise Erase) (analysis)
        Output_Feature_Class = fr"{arcpy.env.scratchGDB}\SIDEWALK_Erase"
        arcpy.analysis.PairwiseErase(in_features=SIDEWALK_3_, erase_features=PLUTO_Dissolve, out_feature_class=Output_Feature_Class)

        # Process: Collapse Hydro Polygon (Collapse Hydro Polygon) (cartography)
        Sidewalk_Collapse = fr"{arcpy.env.scratchGDB}\Sidewalk_Collapse"
        Output_Polygon_Feature_Class = ""
        Sidewalk_Collapse_InPoly_DecodeID, Sidewalk_Collapse_InLine_DecodeID = arcpy.cartography.CollapseHydroPolygon(in_features=[Output_Feature_Class], out_line_feature_class=Sidewalk_Collapse, out_poly_feature_class=Output_Polygon_Feature_Class)

        # Process: Multipart To Singlepart (Multipart To Singlepart) (management)
        PLUTO_Singlepart = fr"{arcpy.env.scratchGDB}\PLUTO_Singlepart"
        arcpy.management.MultipartToSinglepart(in_features=PLUTO_Dissolve, out_feature_class=PLUTO_Singlepart)

        # Process: Add Unique Block Parts (Add Field) (management)
        PLUTO_Unique = arcpy.management.AddField(in_table=PLUTO_Singlepart, field_name="Unique_Block_Parts", field_type="TEXT")[0]

        # Process: Calculate Unique PLUTO Blocks (Calculate Field) (management)
        Pluto_Blocks_2_ = arcpy.management.CalculateField(in_table=PLUTO_Unique, field="Unique_Block_Parts", expression="str(!Unique_Blocks!)+str(!OBJECTID!)")[0]

        # Process: Pluto Blocks Copy (Copy Features) (management)
        Pluto_Working = "D:\\ArcGIS\\Projects\\Street_Tree_Planting_Analysis\\Scrap.gdb\\Pluto_Blocks"
        arcpy.management.CopyFeatures(in_features=Pluto_Blocks_2_, out_feature_class=Pluto_Working)

        # Process: Pluto + Buffer (Pairwise Buffer) (analysis)
        Pluto_Buffer' = fr"{arcpy.env.scratchGDB}\Pluto_Buffer"
        arcpy.analysis.PairwiseBuffer(in_features=Pluto_Blocks_2_, out_feature_class=Pluto_Buffer', buffer_distance_or_field="25 Feet")

        # Process: Pairwise Erase (Pairwise Erase) (analysis)
        Pluto_Prep = fr"{arcpy.env.scratchGDB}\Pluto_Buffer_Topology"
        arcpy.analysis.PairwiseErase(in_features=Pluto_Buffer', erase_features=PLUTO_Dissolve, out_feature_class=Pluto_Prep)

        # Process: Pairwise Intersect (Pairwise Intersect) (analysis)
        Sidewalk_Pluto = "D:\\ArcGIS\\Projects\\Street_Tree_Planting_Analysis\\Street_Tree_Planting_Analysis.gdb\\Sidewalk_Pluto"
        arcpy.analysis.PairwiseIntersect(in_features=[Sidewalk_Collapse, Pluto_Prep], out_feature_class=Sidewalk_Pluto)

        # Process: Delete Field (2) (Delete Field) (management)
        Sidewalk_Pluto_3_ = arcpy.management.DeleteField(in_table=Sidewalk_Pluto, drop_field=["FID_Sidewalk_Collapse", "InPoly_ID", "InPoly_FID", "InLine_ID", "InLine_FID", "COLLAPSED", "FID_Pluto_Buffer_Topology_Erase_Clean", "ORIG_FID"])[0]

if __name__ == '__main__':
    # Global Environment settings
    with arcpy.EnvManager(autoCommit=1000, baUseDetailedAggregation=False, cellAlignment="DEFAULT", 
                          cellSize="MAXOF", cellSizeProjectionMethod="CONVERT_UNITS", coincidentPoints="MEAN", 
                          compression="LZ77", maintainAttachments=True, maintainSpatialIndex=False, 
                          matchMultidimensionalVariable=True, nodata="NONE", outputMFlag="Same As Input", 
                          outputZFlag="Same As Input", preserveGlobalIds=False, pyramid="PYRAMIDS -1 NEAREST DEFAULT 75 NO_SKIP NO_SIPS", 
                          qualifiedFieldNames=True, randomGenerator="0 ACM599", rasterStatistics="STATISTICS 1 1", 
                          resamplingMethod="NEAREST", terrainMemoryUsage=False, tileSize="128 128", 
                          tinSaveVersion="CURRENT", transferDomains=False, transferGDBAttributeProperties=False, 
                          unionDimension=False, workspace="D:\\ArcGIS\\Projects\\Street_Tree_Planting_Analysis\\Scrap.gdb"):
        SecondStepAlternative()
```

## src/stp/cli/stp_pipeline.py

```python
import json
import logging
from pathlib import Path

import geopandas as gpd  # For handling spatial data

from core.config import get_setting  # To load config values
from fetch.arcgis import fetch_arcgis_vector  # ArcGIS fetcher
from fetch.csv import fetch_csv_direct  # CSV fetcher (if needed)
from fetch.geojson import fetch_geojson_direct  # GeoJSON fetcher
from stp.fetch.socrata import fetch_  # Socrata fetcher
from stp.fetch.socrata import dispatch_socrata_table
# Add more fetchers as needed from stp/fetch/



def load_sources() -> list[dict]:
    """Load the list of data sources from config/sources.json."""
    sources_path = Path("config/sources.json")
    if not sources_path.exists():
        raise FileNotFoundError(f"Config file not found: {sources_path}")
    with sources_path.open(encoding="utf-8") as file:
        return json.load(file)


def setup_logging_and_output() -> Path:
    """Set up logging and return the raw data output directory."""
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s - %(levelname)s - %(message)s")
    output_dir = Path(get_setting("paths.output.raw", "data/raw"))
    output_dir.mkdir(parents=True, exist_ok=True)
    return output_dir


FETCHERS = {
    "socrata": fetch_socrata_json,
    "arcgis": fetch_arcgis_vector,
    "csv": fetch_csv_direct,
    "geojson": fetch_geojson_direct,
    # Add more as needed, e.g., "gdb": fetch_gdb, etc.
}


def download_source(source: dict, output_dir: Path) -> None:
    """Download and save a single source."""
    source_id = source["id"]
    url = source["url"]
    source_type = source.get("source_type", "").lower()
    fmt = source.get("format", "").lower()

    if source_type not in FETCHERS:
        logging.warning("Skipping unknown source type: %s (%s)",
                        source_type, source_id)
        return

    logging.info("Downloading %s from %s...", source_id, url)
    fetch_fn = FETCHERS[source_type]

    # Fetch the data (returns list of (name, gdf, epsg) tuples)
    results = fetch_fn(url)  # Add params like app_token if needed

    for raw_name, gdf, epsg in results:
        if gdf is None or gdf.empty:
            logging.warning("No data for %s", raw_name)
            continue

        # Save as GeoJSON (simple format; can change to GPKG later)
        save_path = output_dir / f"{source_id}.geojson"
        gdf.to_file(save_path, driver="GeoJSON")
        logging.info("Saved %s (EPSG: %s) to %s", source_id, epsg, save_path)


def main() -> None:
    """Main pipeline entry: Download all sources."""
    output_dir = setup_logging_and_output()
    sources = load_sources()

    for source in sources:
        download_source(source, output_dir)

    logging.info("Download complete! Files in %s", output_dir)


if __name__ == "__main__":
    main()
```

## src/stp/core/config.py

```python
"""
Config loader for STP.

Loads YAML defaults, optional user overrides, and environment variables
with deep-merge logic and caching via lru_cache.
Provides get_setting() and get_constant() for dot-path access.
Also loads and validates workflow.yaml for pipeline orchestration.
"""

from __future__ import annotations

import os
import logging
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict

import yaml
from dotenv import load_dotenv

logging.basicConfig(level=logging.INFO)

# Load environment variables from .env into os.environ (if .env exists)
load_dotenv()


def load_user_config() -> Dict[str, Any]:
    """Load critical overrides from environment (.env or system env)."""
    # Pull API keys and database creds directly from environment variables
    return {
        "api_key": os.getenv("API_KEY"),
        "db_user": os.getenv("DB_USER"),
        "db_pass": os.getenv("DB_PASS"),
    }


def _deep_get(mapping: Dict[str, Any], keys: list[str]) -> Any:
    """Walk a nested dict by a list of keys; return None if any key is missing."""
    current: Any = mapping
    for key in keys:
        if not isinstance(current, dict):
            # If current level isn't a dict, path is invalid
            return None
        current = current.get(key)
    return current


def _merge(base: Dict[str, Any], update: Dict[str, Any]) -> Dict[str, Any]:
    """Recursively deep-merge two dicts: nested dicts merge, others overwrite."""
    # Create a shallow copy so we don't mutate the original base dict
    result = dict(base)
    for key, val in update.items():
        # If both base and update have dicts at this key, merge them recursively
        if key in result and isinstance(result[key], dict) and isinstance(val, dict):
            result[key] = _merge(result[key], val)
        else:
            # Otherwise, the update value replaces or adds to the result
            result[key] = val
    return result


@lru_cache(maxsize=1)
def _load_defaults() -> Dict[str, Any]:
    """Load defaults.yaml once and cache it for fast subsequent access."""
    # Locate the repo root relative to this file
    root = Path(__file__).resolve().parents[2]
    path = root / "config" / "defaults.yaml"
    with open(path, encoding="utf-8") as f:
        # Safe-load YAML; return empty dict if file is empty
        return yaml.safe_load(f) or {}


@lru_cache(maxsize=1)
def _load_overrides() -> Dict[str, Any]:
    """Load user.yaml overrides once and cache; return empty dict if absent."""
    root = Path(__file__).resolve().parents[2]
    path = root / "config" / "user.yaml"
    if path.exists():
        with open(path, encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    # No overrides file means no override entries
    return {}


@lru_cache(maxsize=1)
def load_config() -> Dict[str, Any]:
    """Merge defaults and user overrides into one config dict (cached)."""
    defaults = _load_defaults()
    overrides = _load_overrides()
    # Combine with override taking precedence
    return _merge(defaults, overrides)


def get_setting(
    key: str,
    default: Any | None = None,
    required: bool = False,
    env_override: bool = True,
) -> Any:
    """
    Retrieve a config value using dot-path lookup with this precedence:
      1. Environment variable (if env_override=True)
      2. user.yaml overrides
      3. defaults.yaml
      4. provided default arg

    Raises if 'required' is True and resulting value is missing or placeholder.
    """
    # 1) Check environment variables first (e.g., 'DB_HOST' for 'db.host')
    if env_override:
        env_key = key.upper().replace('.', '_')
        if (val := os.getenv(env_key)) is not None:
            return val

    # 2) Load merged config and attempt lookup in overrides then defaults
    keys = key.split('.')
    value = _deep_get(_load_overrides(), keys)
    if value is None:
        value = _deep_get(_load_defaults(), keys)
    if value is None:
        # 3) Fall back to function default if still missing
        value = default

    # 4) If marked required but missing or placeholder, error out
    if required and (value is None or value == "REPLACE_ME"):
        raise RuntimeError(f"Missing required setting: {key}")
    return value


def get_constant(key: str, default: Any | None = None) -> Any:
    """Fetch a constant from defaults.yaml only, ignoring user overrides."""
    keys = key.split('.')
    value = _deep_get(_load_defaults(), keys)
    # If not found, use provided default
    return default if value is None else value


@lru_cache(maxsize=1)
def load_workflow(path: str = 'config/workflow.yaml') -> Dict[str, Any]:
    """
    Parse workflow.yaml, merge with config globals (e.g., from defaults.yaml),
    and perform basic validation. This drives the pipeline orchestrator.
    Cached for efficiency like load_config().
    """
    root = Path(__file__).resolve().parents[2]
    full_path = root / path
    with open(full_path, 'r', encoding='utf-8') as f:
        workflow = yaml.safe_load(f)

    # Merge globals from the main config (e.g., EPSG defaults, limits)
    config = load_config()
    workflow['globals'] = config  # Merge the full config for broader access

    # Basic validation to catch errors early (expand as needed)
    required_sections = ['sources', 'prep_ops']  # Add 'final_clip', 'big_scripts' if always needed
    for section in required_sections:
        if section not in workflow:
            raise ValueError(f"workflow.yaml missing '{section}' section")
    for dataset_id, dataset in workflow['sources'].items():
        if 'url' not in dataset or 'format' not in dataset:
            logging.warning("Source %s missing 'url' or 'format'", dataset_id)

    logging.info("Parsed and merged workflow.yaml with config globals")
    return workflow


__all__ = [
    "load_user_config",
    "load_config",
    "get_setting",
    "get_constant",
    "load_workflow"
    ]
```

## src/stp/core/http.py

```python
"""Simple HTTP client helpers."""

from typing import Optional

import requests

_session = requests.Session()


def fetch_bytes(url: str, session: Optional[requests.Session] = None) -> bytes:
    """Return response content for GET request."""
    sess = session or _session
    resp = sess.get(url)
    resp.raise_for_status()
    return resp.content
```

## src/stp/core/settings.py

```python
"""Global constants for the STP package, pulled from config."""

from config import get_constant

DEFAULT_EPSG = get_constant('epsg.default', 4326)
NYSP_EPSG = get_constant('epsg.nysp', 2263)

# TODO: All done! No more hardcodes‚Äîconstants now come from defaults.yaml.
# Remove this file later if you want to access directly via get_constant everywhere.
```

## src/stp/fetch/arcgis.py

```python
"""Fetchers for ArcGIS REST services."""

from __future__ import annotations

import functools
from io import BytesIO
import logging
import os
from pathlib import Path
from typing import List, Optional, Tuple

import geopandas as gpd

from stp.core.config import load_config  # YAML loader/merger
from stp.core.http import fetch_bytes  # For retries/timeout
from stp.storage.file_storage import sanitize_layer_name

from ._optional_deps import DATAFRAME_LIKE

__all__ = ["fetch_arcgis_vector", "fetch_arcgis_table", "load_workflow_cached", "get_layer_name"]

logger = logging.getLogger(__name__)


def _build_query_url(service_url: str, as_geojson: bool = True) -> str:
    """Return an ArcGIS REST query URL for *service_url*."""
    base = service_url.rstrip("/")
    if not base.lower().endswith("query"):
        base = f"{base}/query"
    params = "where=1%%3D1&outFields=*&returnGeometry=true"
    if as_geojson:
        params += "&outSR=4326&f=geojson"
    else:
        params += "&f=json"
    return f"{base}?{params}"


def fetch_arcgis_vector(
    service_url: str,
) -> List[Tuple[str, gpd.GeoDataFrame, int, int]]:
    """Fetch vector data from an ArcGIS FeatureServer layer."""
    url = _build_query_url(service_url, as_geojson=True)
    data = fetch_bytes(url)
    gdf = gpd.read_file(BytesIO(data))
    epsg = gdf.crs.to_epsg() or DEFAULT_EPSG
    layer_name = sanitize_layer_name(Path(service_url).stem)
    return [(layer_name, gdf, epsg, 4326)]


def fetch_arcgis_table(
    service_url: str,
) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Fetch a non-spatial table from an ArcGIS service."""
    url = _build_query_url(service_url, as_geojson=True)
    data = fetch_bytes(url)
    gdf = gpd.read_file(BytesIO(data))
    gdf.set_crs(epsg=DEFAULT_EPSG, inplace=True)
    layer_name = sanitize_layer_name(Path(service_url).stem)
    return [(layer_name, gdf, DEFAULT_EPSG)]


_VECTOR_FORMATS = {"shapefile", "geojson", "feature"}  # Expanded


@functools.lru_cache(maxsize=128)
def load_workflow_cached() -> dict:
    """Cached load of workflow.yaml."""
    return load_config("workflow.yaml")


DEFAULT_EPSG = load_workflow_cached().get("default_epsg", 4326)


TTL = int(os.getenv("ARC_LAYER_NAME_TTL", "3600"))
@functools.lru_cache(maxsize=128 if TTL else 0)
def get_layer_name(url: str) -> str:
    """Fetch layer name from service metadata."""
    base_url = url.split("?", 1)[0]  # Strip params for cache stability
    meta_url = f"{base_url}?f=json"
    try:
        data = fetch_json_with_retry(meta_url)  # Use retry helper
        return data.get("name", "unnamed_layer")
    except Exception as exc:
        logger.warning(
            "Failed to fetch layer name from %s: %s", meta_url, exc
        )
        return base_url.split("/")[-2]  # Fallback


def _decide_vector_mode(
    *,
    yaml_has_geometry: bool | None,
    fmt: str,
    runtime_override: bool | None,
) -> bool:
    """
    Return True if we should treat the endpoint as a vector layer.
    Priority: yaml flag > runtime override > format inference.
    """
    if yaml_has_geometry is not None:
        return yaml_has_geometry
    if runtime_override is not None:
        return runtime_override
    return fmt in _VECTOR_FORMATS


def fetch_arcgis(
    source_id: str, is_vector: Optional[bool] = None, **kwargs
) -> List[Tuple[str, DATAFRAME_LIKE, Optional[int]]]:
    """
    Dispatcher for ArcGIS fetches.

    Loads config from workflow.yaml, determines vector/table via
    'has_geometry' (primary), format inference, or is_vector override.
    Delegates to helpers. Helpers should accept filter_query/max_records
    or use **_ guard.

    Parameters
    ----------
    source_id : str
        Source ID from workflow.yaml (e.g., 'sidewalk').
    is_vector : bool, optional
        Runtime override for vector/table.
    **kwargs : dict
        Passed to helpers (e.g., custom params).

    Returns
    -------
    List[Tuple[str, DataFrameLike, Optional[int]]]
        (layer_name, DataFrame/GeoDataFrame/None, src_epsg) tuples.
        src_epsg is None for non-spatial tables.
    """
    wf = load_workflow_cached()
    try:
        src = wf["sources"][source_id]
    except KeyError as exc:
        raise ValueError(f"Unknown source_id '{source_id}'") from exc

    url = src.get("url")
    if not url:
        raise ValueError(f"No URL for '{source_id}' in workflow.yaml")

    fmt = src.get("format", "").lower()
    filter_query = src.get("filter")
    max_records = src.get("limits", {}).get("arcgis_default_max_records", 1000)

    use_vector = _decide_vector_mode(
        yaml_has_geometry=src.get("has_geometry"),
        fmt=fmt,
        runtime_override=is_vector,
    )

    layer_name = get_layer_name(url)

    if use_vector:
        results = fetch_arcgis_vector(
            url,
            filter_query=filter_query,
            max_records=max_records,
            **kwargs,
        )
    else:
        results = fetch_arcgis_table(
            url,
            filter_query=filter_query,
            max_records=max_records,
            **kwargs,
        )
        results = [(n, df, None) for n, df, _ in results]

    rows = sum(len(df) for _, df, _ in results if df is not None)
    logger.info("Fetched %d rows from %s (%s)", rows, source_id, "vector" if use_vector else "table")

    return results
```

## src/stp/fetch/csv.py

```python
"""CSV direct download helper."""

from io import BytesIO
from pathlib import Path
from typing import List, Tuple
import logging

import geopandas as gpd
import pandas as pd
from pandas.errors import ParserError
from shapely.geometry import Point

from ..core import http
from ..storage.file_storage import sanitize_layer_name
from ..core import DEFAULT_EPSG #TODO use config.py to get default values

logger = logging.getLogger(__name__)


def fetch_csv_direct(url: str) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Download and parse a CSV URL."""
    data = http.fetch_bytes(url)
    try:
        df = pd.read_csv(BytesIO(data))
    except ParserError as err:
        logger.warning("CSV parse failed for %s: %s", url, err)
        return []
    if "latitude" not in df.columns or "longitude" not in df.columns:
        return []
    geometry = [Point(xy) for xy in zip(df.longitude, df.latitude)]
    gdf = gpd.GeoDataFrame(
        df.drop(columns=["latitude", "longitude"]),
        geometry=geometry,
        crs=f"EPSG:{DEFAULT_EPSG}",
    )
    layer_name = sanitize_layer_name(Path(url).stem)
    return [(layer_name, gdf, DEFAULT_EPSG)]
```

## src/stp/fetch/download.py

```python
"""Dispatcher for direct dataset downloads."""

from typing import List, Tuple

import geopandas as gpd

from .geojson import fetch_geojson_direct
from .csv import fetch_csv_direct


def fetch_direct(url: str) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Fetch data from *url* using the appropriate fetcher."""
    lower = url.lower()
    if lower.endswith((".geojson", ".json")):
        return fetch_geojson_direct(url)
    if lower.endswith(".csv"):
        return fetch_csv_direct(url)
    raise ValueError(f"Unsupported format: {url}")
```

## src/stp/fetch/gdb.py

```python
"""Fetchers for zipped shapefiles or geodatabases."""

from __future__ import annotations

from pathlib import Path
from typing import List, Tuple
from tempfile import TemporaryDirectory
import zipfile

import fiona
import geopandas as gpd

from .. import http_client
from ..storage.file_storage import sanitize_layer_name
from ..settings import DEFAULT_EPSG

__all__ = ["fetch_gdb_or_zip"]


def fetch_gdb_or_zip(url: str) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Download a zipped archive and extract layers."""
    data = http_client.fetch_bytes(url)
    results: List[Tuple[str, gpd.GeoDataFrame, int]] = []
    with TemporaryDirectory() as tmpdir:
        zip_path = Path(tmpdir) / "data.zip"
        with open(zip_path, "wb") as fh:
            fh.write(data)
        with zipfile.ZipFile(zip_path, "r") as zf:
            zf.extractall(tmpdir)
        for shp in Path(tmpdir).rglob("*.shp"):
            gdf = gpd.read_file(shp)
            epsg = gdf.crs.to_epsg() or DEFAULT_EPSG
            results.append((sanitize_layer_name(shp.stem), gdf, epsg))
        for gdb in Path(tmpdir).rglob("*.gdb"):
            for layer in fiona.listlayers(str(gdb)):
                gdf = gpd.read_file(gdb, layer=layer)
                epsg = gdf.crs.to_epsg() or DEFAULT_EPSG
                results.append((sanitize_layer_name(layer), gdf, epsg))
    return results
```

## src/stp/fetch/geojson.py

```python
"""GeoJSON direct download helper."""

from io import BytesIO
from pathlib import Path
from typing import List, Tuple
import logging

import geopandas as gpd
from fiona.errors import DriverError, FionaValueError

from .. import http_client
from ..storage.file_storage import sanitize_layer_name
from ..settings import DEFAULT_EPSG

logger = logging.getLogger(__name__)


def fetch_geojson_direct(url: str) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Download and parse a GeoJSON URL."""
    data = http_client.fetch_bytes(url)
    try:
        gdf = gpd.read_file(BytesIO(data))
    except (FionaValueError, DriverError) as err:
        logger.warning("GeoJSON read failed for %s: %s", url, err)
        return []
    gdf.set_crs(epsg=DEFAULT_EPSG, inplace=True)
    layer_name = sanitize_layer_name(Path(url).stem)
    return [(layer_name, gdf, DEFAULT_EPSG)]
```

## src/stp/fetch/gpkg.py

```python
"""Fetch layers from a GeoPackage."""

from __future__ import annotations

from pathlib import Path
from typing import List, Tuple
from tempfile import TemporaryDirectory

import fiona
import geopandas as gpd

from .. import http_client
from ..storage.file_storage import sanitize_layer_name
from ..settings import DEFAULT_EPSG

__all__ = ["fetch_gpkg_layers"]


def fetch_gpkg_layers(
    path_or_url: str,
) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Load all layers from a GeoPackage file or URL."""
    tmpdir: TemporaryDirectory | None = None
    gpkg_path = Path(path_or_url)
    if path_or_url.startswith("http"):
        tmpdir = TemporaryDirectory()
        gpkg_path = Path(tmpdir.name) / "data.gpkg"
        gpkg_path.write_bytes(http_client.fetch_bytes(path_or_url))
    try:
        results: List[Tuple[str, gpd.GeoDataFrame, int]] = []
        for layer in fiona.listlayers(str(gpkg_path)):
            gdf = gpd.read_file(gpkg_path, layer=layer)
            epsg = gdf.crs.to_epsg() or DEFAULT_EPSG
            results.append((sanitize_layer_name(layer), gdf, epsg))
        return results
    finally:
        if tmpdir is not None:
            tmpdir.cleanup()
```

## src/stp/fetch/lookup.py

```python
"""
Fetcher utilization
"""
# src/stp/scripts/download_utils.py

from stp.fetch.socrata import dispatch_socrata_table
from stp.fetch.arcgis import fetch_arcgis_table, fetch_arcgis_vector
from stp.fetch.csv import fetch_csv_direct
from stp.fetch.geojson import fetch_geojson_direct
from stp.fetch.gdb import fetch_gdb_or_zip
from stp.fetch.gpkg import fetch_gpkg_layers

FETCHERS = {
    ("socrata", "csv"):   dispatch_socrata_table,
    ("socrata", "json"):  dispatch_socrata_table,
    ("socrata", "geojson"): dispatch_socrata_table,
    ("socrata", "shapefile"): dispatch_socrata_table,
    ("arcgis", "csv"):    fetch_arcgis_table,
    ("arcgis", "json"):   fetch_arcgis_table,
    ("arcgis", "geojson"): fetch_arcgis_vector,
    ("arcgis", "shapefile"): fetch_arcgis_vector,
    (None, "csv"):        fetch_csv_direct,
    (None, "geojson"):    fetch_geojson_direct,
    (None, "shapefile"):  fetch_gdb_or_zip,
    (None, "gpkg"):       fetch_gpkg_layers,
}
```

## src/stp/fetch/socrata.py

```python
"""Placeholder Socrata fetcher."""

from __future__ import annotations

from typing import List, Tuple, Optional

import geopandas as gpd

__all__ = ["dispatch_socrata_table"]


def dispatch_socrata_table(url: str, app_token: Optional[str] = None
                           ) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Temporary stub for Socrata dataset fetching."""
    raise NotImplementedError("Socrata fetcher not implemented")
```

## src/stp/fetch/_optional_deps.py

```python
# _optional_deps.py
"""
optional
"""
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    import geopandas as gpd
    import pandas as pd
else:
    try:
        import geopandas as gpd
        import pandas as pd
    except ImportError:  # pragma: no cover
        gpd = pd = None

GeoDataFrame = getattr(gpd, "GeoDataFrame", None)
DataFrame = getattr(pd, "DataFrame", None)
DATAFRAME_LIKE = "GeoDataFrame | DataFrame | None"
```

## src/stp/fetch/__init__.py

```python
"""Spatial data fetcher helpers."""

from .csv import fetch_csv_direct
from .geojson import fetch_geojson_direct
from .arcgis import fetch_arcgis_vector, fetch_arcgis_table
from .gdb import fetch_gdb_or_zip
from .gpkg import fetch_gpkg_layers
from .socrata import dispatch_socrata_table

__all__ = [
    "fetch_csv_direct",
    "fetch_geojson_direct",
    "fetch_arcgis_vector",
    "fetch_arcgis_table",
    "fetch_gdb_or_zip",
    "fetch_gpkg_layers",
    "dispatch_socrata_table",
]
```

## src/stp/process/clean/address.py

```python
"""Address-related cleaning routines."""

from typing import Optional

import geopandas as gpd
import pandas as pd


def clean_street_signs(
    gdf: gpd.GeoDataFrame,
    *,
    require_record_type: str = "Current",
    date_fields: Optional[list[str]] = None,
    int_fields: Optional[list[str]] = None,
    drop_suffixes: Optional[list[str]] = None,
    keep_fields: Optional[list[str]] = None,
) -> gpd.GeoDataFrame:
    """Clean street sign records and drop non-current entries."""
    df = gdf.copy()
    if date_fields is None:
        date_fields = [
            "order_completed_on_date",
            "sign_design_voided_on_date",
        ]
    if int_fields is None:
        int_fields = ["distance_from_intersection"]
    if drop_suffixes is None:
        drop_suffixes = [
            "on_street_suffix",
            "from_street_suffix",
            "to_street_suffix",
        ]
    if keep_fields is None:
        keep_fields = [
            "order_number",
            "record_type",
            "order_type",
            "borough",
            "on_street",
            "from_street",
            "side_of_street",
            "order_completed_on_date",
            "sign_code",
            "sign_description",
            "sign_size",
            "sign_location",
            "distance_from_intersection",
            "arrow_direction",
            "sheeting_type",
            "support",
            "to_street",
            "facing_direction",
            "sign_notes",
            "sign_design_voided_on_date",
        ]
    df = df[df["record_type"].str.strip().str.title() == require_record_type]
    for fld in date_fields:
        if fld in df:
            df[fld] = pd.to_datetime(df[fld], errors="coerce")
    for fld in int_fields:
        if fld in df:
            df[fld] = pd.to_numeric(df[fld], errors="coerce")
    df = df.drop(columns=[c for c in drop_suffixes if c in df.columns])
    final_cols = [c for c in keep_fields if c in df.columns] + ["geometry"]
    df = df[final_cols].copy()
    df["record_type"] = df["record_type"].str.strip().str.title()
    df["side_of_street"] = df["side_of_street"].str.strip().str.upper()
    df["arrow_direction"] = df["arrow_direction"].str.strip()
    df["sign_description"] = df["sign_description"].str.strip()
    return gpd.GeoDataFrame(df, geometry="geometry", crs=gdf.crs)
```

## src/stp/process/clean/trees.py

```python
"""Tree-related cleaning routines."""

from typing import Optional

import geopandas as gpd

MIN_DBH = 0.01


def clean_trees_basic(
    trees: gpd.GeoDataFrame,
    *,
    structure_field: str = "tpstructure",
    require_structure: str = "Full",
    id_field: str = "objectid",
    out_id: str = "TreeID",
) -> gpd.GeoDataFrame:
    """Return trees with full structure."""
    df = trees.loc[
        trees[structure_field] == require_structure, [id_field, "geometry"]
    ].copy()
    return df.rename(columns={id_field: out_id})


def clean_trees_advanced(
    trees: gpd.GeoDataFrame,
    planting_spaces: gpd.GeoDataFrame,
    *,
    condition_field: str = "tpcondition",
    drop_conditions: Optional[list[str]] = None,
    structure_field: str = "tpstructure",
    require_structure: str = "Full",
    dbh_field: str = "dbh",
    min_dbh: float = MIN_DBH,
    ps_key: str = "plantingspaceglobalid",
    ps_globalid: str = "globalid",
    ps_status_field: str = "psstatus",
    keep_ps_status: str = "Populated",
    ps_jur_field: str = "jurisdiction",
    exclude_jur: str = "Private",
    id_field: str = "objectid",
    out_id: str = "TreeID",
) -> gpd.GeoDataFrame:
    """Return cleaned trees joined to planting spaces."""
    if drop_conditions is None:
        drop_conditions = ["Unknown", "Dead"]
    mask = (
        ~trees[condition_field].isin(drop_conditions)
        & (trees[structure_field] == require_structure)
        & trees[dbh_field].gt(min_dbh)
    )
    df = trees.loc[mask].copy()
    df = df.merge(
        planting_spaces[[ps_globalid, ps_status_field, ps_jur_field]],
        left_on=ps_key,
        right_on=ps_globalid,
        how="inner",
    )
    ps_mask = (
        (df[ps_status_field] == keep_ps_status)
        & (df[ps_jur_field] != exclude_jur)
    )
    df = df.loc[ps_mask]
    df = df[[id_field, "geometry"]]
    return gpd.GeoDataFrame(df, geometry="geometry", crs=trees.crs).rename(
        columns={id_field: out_id}
    )


def canceled_work_orders(
    wo: gpd.GeoDataFrame,
    *,
    wo_type_field: str = "wotype",
    wo_cat_field: str = "wocategory",
    wo_status_field: str = "wostatus",
    allowed_types: Optional[list[str]] = None,
    allow_category: str = "Tree Planting",
    cancel_status: str = "Cancel",
    id_field: str = "objectid",
    out_id: str = "WOID",
) -> gpd.GeoDataFrame:
    """Filter work orders to cancelled planting jobs."""
    if allowed_types is None:
        allowed_types = [
            "Tree Plant-Park Tree",
            "Tree Plant-Street Tree",
            "Tree Plant-Street Tree Block",
        ]
    mask = (
        wo[wo_type_field].isin(allowed_types)
        & (wo[wo_cat_field] == allow_category)
        & (wo[wo_status_field] == cancel_status)
    )
    df = wo.loc[mask, [id_field, "geometry"]].copy()
    return df.rename(columns={id_field: out_id})


def clean_planting_spaces(
    ps: gpd.GeoDataFrame,
    *,
    status_field: str = "psstatus",
    keep_status: str = "Populated",
    jur_field: str = "jurisdiction",
    exclude_jur: str = "Private",
    id_field: str = "globalid",
    out_id: str = "PSID",
) -> gpd.GeoDataFrame:
    """Return populated planting spaces excluding private sites."""
    mask = ps[status_field] == keep_status
    if exclude_jur:
        mask &= ps[jur_field] != exclude_jur
    df = ps.loc[mask, [id_field, "geometry"]].copy()
    return df.rename(columns={id_field: out_id})
```

## src/stp/process/clean/__init__.py

```python

```

## src/stp/process/custom_ops.py

```python
"""Custom operations for STP pipeline (special/complex functions)."""

import logging

import geopandas as gpd
from pygeoops import centerline
# pip install pygeoops (medial axis for polygon centerlines)

logging.basicConfig(level=logging.INFO)


def collapse_to_centerline(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
    """Collapse polygons to centerlines (like CollapseHydroPolygon).

    Extracts medial axis (midpoints between edges).
    """
    centerlines = []
    for geom in gdf.geometry:
        if geom.is_valid and not geom.is_empty:
            cl = centerline(geom)
            if cl:
                centerlines.append(cl)
    collapsed = gpd.GeoDataFrame(geometry=centerlines, crs=gdf.crs)
    logging.info("Collapsed to centerlines")
    return collapsed
```

## src/stp/process/data_cleaning.py

```python
"""High-level cleaning dispatch functions."""

from .clean.trees import (
    clean_trees_basic,
    clean_trees_advanced,
    canceled_work_orders,
    clean_planting_spaces,
)
from .clean.address import clean_street_signs

__all__ = [
    "clean_trees_basic",
    "clean_trees_advanced",
    "canceled_work_orders",
    "clean_planting_spaces",
    "clean_street_signs",
]
```

## src/stp/process/field_ops.py

```python
"""Field operations for STP pipeline (attribute tweaks)."""

import logging

import geopandas as gpd

logging.basicConfig(level=logging.INFO)


def calculate_unique_blocks(
    gdf: gpd.GeoDataFrame,
    borough_field: str = 'Borough',
    block_field: str = 'Block',
    output_field: str = 'Unique_Blocks'
) -> gpd.GeoDataFrame:
    """Calculate unique blocks (concat Borough + Block, like CalculateField)."""
    gdf[output_field] = gdf[borough_field].astype(str) + gdf[block_field].astype(str)
    logging.info("Calculated %s", output_field)
    return gdf


def add_unique_parts(
    gdf: gpd.GeoDataFrame,
    unique_field: str = 'Unique_Blocks',
    output_field: str = 'Unique_Block_Parts'
) -> gpd.GeoDataFrame:
    """Add unique parts field(Unique_Blocks + index,
      like AddField + CalculateField)."""
    gdf[output_field] = gdf[unique_field].astype(str) + gdf.index.astype(str)
    logging.info("Added %s", output_field)
    return gdf


def delete_fields(
        gdf: gpd.GeoDataFrame, fields_to_drop: list
        ) -> gpd.GeoDataFrame:
    """Delete fields (like DeleteField). Drops specified columns."""
    gdf = gdf.drop(columns=fields_to_drop, errors='ignore')
    logging.info("Deleted fields")
    return gdf
```

## src/stp/process/geometry_ops.py

```python
"""Geometry operations for STP pipeline (shape-changing functions)."""

import logging

import geopandas as gpd

logging.basicConfig(level=logging.INFO)


def dissolve_gdf(gdf: gpd.GeoDataFrame, by_field: str, single_part: bool = True
                 ) -> gpd.GeoDataFrame:
    """Dissolve by field (like PairwiseDissolve).

    Groups features with same value in by_field, merging geometries.
    """
    dissolved = gdf.dissolve(by=by_field)
    if single_part:
        # Multipart to singlepart
        dissolved = dissolved.explode(ignore_index=True)
    logging.info("Dissolved by %s", by_field)
    return dissolved


def erase_gdf(input_gdf: gpd.GeoDataFrame, erase_gdf: gpd.GeoDataFrame
              ) -> gpd.GeoDataFrame:
    """Erase input by erase geometry (like PairwiseErase).

    Removes parts of input that overlap erase.
    """
    # unary_union combines erase shapes into one
    erased_geom = input_gdf.difference(erase_gdf.unary_union)
    erased = gpd.GeoDataFrame(
        geometry=erased_geom, crs=input_gdf.crs
        ).explode(ignore_index=True)
    logging.info("Erased geometries")
    return erased


def buffer_gdf(gdf: gpd.GeoDataFrame, distance: float
               ) -> gpd.GeoDataFrame:
    """Buffer geometries (like PairwiseBuffer).

    Adds a 'halo' around shapes.
    """
    buffered = gdf.buffer(distance)
    buffered_gdf = gpd.GeoDataFrame(geometry=buffered, crs=gdf.crs)
    logging.info("Buffered by %s", distance)
    return buffered_gdf


def intersect_gdf(input_gdf: gpd.GeoDataFrame, intersect_gdf: gpd.GeoDataFrame
                  ) -> gpd.GeoDataFrame:
    """Intersect (like PairwiseIntersect).

    Keeps overlapping parts, combines attributes.
    """
    intersected = gpd.overlay(input_gdf, intersect_gdf, how='intersection')
    logging.info("Intersected geometries")
    return intersected


def repair_geometry(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
    """Repair invalid geometries (like RepairGeometry).

    Fixes issues like self-intersects or bad rings.
    """
    gdf['geometry'] = gdf.make_valid()
    logging.info("Repaired geometries")
    return gdf
```

## src/stp/process/table.py

```python
"""Backward-compatible passthroughs for inventory and metadata helpers."""

from __future__ import annotations

from ..record.db import record as record_layer_metadata_db
from ..record.csv import record as record_layer_metadata_csv
from ..fetch.gpkg import (
    from_gpkg as build_fields_inventory_gpkg,
)
from ..fetch.postgis import (
    from_postgis as build_fields_inventory_postgis,
)
from ..fetch.export import to_csv as write_inventory

__all__ = [
    "record_layer_metadata_db",
    "record_layer_metadata_csv",
    "build_fields_inventory_gpkg",
    "build_fields_inventory_postgis",
    "write_inventory",
]
```

## src/stp/record/csv.py

```python
"""CSV metadata recording functions."""

from __future__ import annotations

import csv
import logging
from datetime import datetime
from pathlib import Path

__all__ = ["record"]


def record(
        csv_path: Path,
        layer_id: str,
        url: str,
        source_epsg: int,
        service_wkid: int | None = None) -> None:
    """Append a row to ``layers_inventory.csv``.

    Parameters
    ----------
    csv_path:
        Destination CSV path.
    layer_id:
        Identifier for the layer being recorded.
    url:
        Source URL of the dataset.
    source_epsg:
        EPSG code of the dataset.
    service_wkid:
        Optional WKID from an ArcGIS service.
    """
    logger = logging.getLogger(__name__)
    write_header = not csv_path.exists()
    try:
        csv_path.parent.mkdir(parents=True, exist_ok=True)
        with csv_path.open("a", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            if write_header:
                writer.writerow([
                    "layer_id",
                    "source_url",
                    "source_epsg",
                    "service_wkid",
                    "downloaded_at",
                ])
            writer.writerow([
                layer_id,
                url,
                source_epsg,
                service_wkid if service_wkid is not None else "",
                datetime.utcnow().isoformat(),
            ])
    except Exception as exc:  # pragma: no cover - log and continue
        logger.error("Failed to record metadata CSV %s: %s", csv_path, exc)
```

## src/stp/record/db.py

```python
"""Database metadata recording functions."""

from __future__ import annotations

import logging
from sqlalchemy.engine import Engine
from sqlalchemy import text

__all__ = ["record"]


def record(
        engine: Engine,
        layer_id: str,
        url: str,
        source_epsg: int,
        service_wkid: int | None = None) -> None:
    """Insert a row into the ``layers_inventory`` table.

    Parameters
    ----------
    engine:
        SQLAlchemy database engine.
    layer_id:
        Identifier for the layer being recorded.
    url:
        Source URL of the dataset.
    source_epsg:
        EPSG code of the dataset.
    service_wkid:
        Optional WKID from an ArcGIS service.
    """
    if engine is None:
        return

    logger = logging.getLogger(__name__)
    stmt = text(
        """
        INSERT INTO layers_inventory (
            layer_id, source_url, source_epsg, service_wkid, downloaded_at
        ) VALUES (
            :layer_id, :url, :epsg, :service_wkid, NOW()
        )
        ON CONFLICT (layer_id) DO NOTHING
        """
    )
    try:
        engine.execute(
            stmt,
            {
                "layer_id": layer_id,
                "url": url,
                "epsg": source_epsg,
                "service_wkid": service_wkid,
            },
        )
    except Exception as exc:  # pragma: no cover - log and continue
        logger.error("Failed to record metadata for %s: %s", layer_id, exc)
```

## src/stp/record/export.py

```python
"""Export inventory DataFrames."""

from __future__ import annotations

from pathlib import Path
import pandas as pd

__all__ = ["to_csv"]


def to_csv(df: pd.DataFrame, out_csv: Path, show_path: bool = True) -> None:
    """Write *df* to *out_csv* and optionally print the path."""
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(out_csv, index=False)
    if show_path:
        print(f"Schema inventory written to {out_csv}")
```

## src/stp/record/gpkg.py

```python
"""Extract field inventory from a GeoPackage."""

from __future__ import annotations

from pathlib import Path
from typing import List, Dict

import fiona
import pandas as pd

__all__ = ["from_gpkg"]


def from_gpkg(gpkg_path: Path) -> pd.DataFrame:
    """Return field inventory for all layers in *gpkg_path*.

    The returned ``DataFrame`` has columns ``layer_name``, ``field_name`` and
    ``field_type``.
    """
    rows: List[Dict[str, str]] = []
    for layer in fiona.listlayers(str(gpkg_path)):
        with fiona.open(str(gpkg_path), layer=layer) as src:
            for field, ftype in src.schema["properties"].items():
                rows.append(
                    {
                        "layer_name": layer,
                        "field_name": field,
                        "field_type": ftype,
                    }
                )
    return pd.DataFrame(rows)
```

## src/stp/record/postgis.py

```python
"""Extract field inventory from a PostGIS database."""

from __future__ import annotations

from sqlalchemy import text
from sqlalchemy.engine import Engine
import pandas as pd

__all__ = ["from_postgis"]


def from_postgis(engine: Engine, schema: str = "public") -> pd.DataFrame:
    """Return field inventory for all tables in a PostGIS schema."""
    sql = text(
        """
        SELECT table_name AS layer_name,
               column_name AS field_name,
               data_type AS field_type
        FROM information_schema.columns
        WHERE table_schema = :schema
        ORDER BY table_name, ordinal_position
        """
    )
    return pd.read_sql(sql, engine, params={"schema": schema})
```

## src/stp/record/__init__.py

```python
"""Schema inventory helpers."""
```

## src/stp/scaffold.md

````markdown
# Scaffold.md: Tree Planting Analysis Pipeline Overview

This document outlines the high-level workflow for the STP (Spatial Tabular Pipeline) GIS project, originally built in ArcPy/ModelBuilder and now converted to pure Python (using GeoPandas for ops like buffers/unions/clips). The focus is NYC tree planting analysis, but it's designed for flexibility (e.g., toggle steps for other cities via config edits). We use a unified YAML config to drive the pipeline dynamically, with Docker for automatic PostGIS setup (spatial DB storage for efficient queries/filters).

## 1. Define Configuration
Configuration is centralized for ease‚Äîedit files to customize sources, ops, filters, and params without changing code. This replaces a "locked" pipeline with variable/user-defined flows (e.g., nest ops in YAML for reordering/toggling).

- **root/config/workflow.yaml**: Main unified config (combines sources, prep ops, filters, and final steps).
  - Data sources from open data portals (e.g., Socrata/ArcGIS REST).
  - Includes URL, type/format, schema, filter parameters (e.g., Socrata $where queries).
  - Nested ops/steps for processing (e.g., copy ‚Üí select ‚Üí buffer), with descriptions, enabled toggles, inputs/outputs.
  - Users can fill out or override (e.g., add local paths for offline use).
  - For reusing a feature class multiple times (e.g., curb_cut in different filters): Reference the same output_layer in steps‚Äîno duplication needed. If mutable (changes needed without overwriting), nest a 'copy' op first. If non-mutable download, use local_path fallback in sources section.
  - Example snippet (from our merged version):
    ```yaml
    data:  # Top-level for all datasets (renamed from 'data_id' for clarity)
        trees:
            sources:  # Fetch details
                type: "json"
                source_type: "socrata"
                format: "json"
                url: "https://data.cityofnewyork.us/resource/hn5i-inap.json"
                schema: "political"
                filter: "SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')"
                to_layer: "trees_raw"
                enabled: true
            prep_ops:  # Nested processing
                description: "Existing tree locations to avoid"
                enabled: true
                steps:
                    - op: copy
                    input: "trees_raw"  # References the source's to_layer
                    output: "trees_copy"
                    - op: xy_to_point
                    output: "trees_1"
                    - op: buffer
                    distance: 25
                    output: "trees_ready"

        nyzd:  # Another dataset bundle
            sources:
            # ... (URL/type/filter)
            prep_ops:
            # ... (steps like copy/select)
    ```

- **root/config/defaults.yaml**: Stores default parameters for tools/ops (e.g., EPSG codes, limits). Optional but useful for globals; can be loaded into pipeline.py and overridden in workflow.yaml if needed.
  - Example:
    ```yaml
    epsg:
      default: 4326
      nysp: 2263
    limits:
      socrata: 50000
      arcgis_default_max_records: 1000
    validation:
      layer_name_max_length: 60
      min_dbh: 0.01
    ```

- **root/config/user.yaml**: User-specific overrides (e.g., API keys, batch params, DB creds). Loads defaults.yaml if none specified. With variable pipelines, this is key for personalization (e.g., change Socrata batch size).
  - Example for DB/Docker:
    ```yaml
    db:
      user: 'admin'
      pass: 'admin'
      host: 'localhost'
      port: 5432
      db_name: 'tree_pipeline'
    api:
      socrata_key: 'your_key_here'  # If needed for throttled APIs
      batch_size: 50000  # Override defaults never above 50000, if above revert to 50,000
    ```

## 2. Download Sources & Create Mutable DB 
Primary goal: Fetch and prep NYC-relevant data for tree planting (e.g., trees, hydrants, sidewalks) from open sources. Variable for other cities (edit workflow.yaml URLs/filters). Downloads via fetchers/ (Socrata/ArcGIS REST handlers in STP tools), with API keys/batch params from user.yaml (e.g., batch to avoid limits on big datasets like census blocks).

- Sources located in workflow.yaml 'sources' section (merged from old sources.json).
- Sources can be from anywhere, but default to Socrata and ArcGIS REST.
  - Requires API key/parameters (e.g., batch size/combining for large fetches) from user.yaml.
- Storage: Use Docker to automatically install/setup a DB with PostgreSQL/PostGIS for spatial efficiency (queries/filters on layers like zoning districts).
  - Docker auto-creates the environment (isolated, portable‚Äîrun `docker compose up -d` from docker-compose.yml).
  - Load creds from user.yaml (e.g., user: 'admin', pass: 'admin', host: 'localhost', port: 5432).
  - Connect: `engine = create_engine('postgresql://admin:admin@localhost:5432/tree_pipeline')`.
  - Load data: After download, `gdf.to_postgis('trees_raw', engine, if_exists='replace')` (GeoPandas handles conversion to spatial tables).
  - Why PostGIS? Faster for ops (e.g., SQL filters during select) than files; fallback to GPKG if needed.
- Prep: Run nested steps from workflow.yaml 'prep_ops' (e.g., copy ‚Üí xy_to_point ‚Üí buffer). Outputs saved as new tables/layers.
- Final clip/big scripts: Handled in workflow.yaml sections (nested steps for union/clip/points generation, then nostanding/rank/curb).

## 3. Initiate Pipeline
    - 


5. Preliminary Operations
   3.1 ExportFeatures(Online_NYZD ‚Üí NYZD)
   3.2 PairwiseBuffer(NYZD ‚Üí NYZD_Buffer, 20 ft)
   3.3 ExportFeatures(BK_Vaults ‚Üí BK_Vaults_2)
   3.4 PairwiseBuffer(BK_Vaults_2 ‚Üí BK_Vaults_Buffer, 20 ft)
   3.5 ExportFeatures(STP_Apps_Vaults ‚Üí DOT_Vault_ExportFeatures)
   3.6 PairwiseBuffer(DOT_Vault_ExportFeatures ‚Üí DOT_Vault_Buffer, 20 ft)
   3.7 ExportFeatures(ConEd_Transformer_Vault ‚Üí ConEd_Transformer_Vault)
   3.8 PairwiseBuffer(ConEd_Transformer_Vault ‚Üí ConEd_Transformer_Buffer, 20 ft)
   3.9 ExportFeatures(DEP_GI_Assets ‚Üí DEP_GI_Assets)
   3.10 PairwiseBuffer(DEP_GI_Assets ‚Üí DEP_GI_Assets_Buffer, 20 ft)

4. Copy & select raw features
   4.1 CopyFeatures(SIDEWALK ‚Üí SIDEWALK_2)
   4.2 CopyFeatures(HVI_CensusTracts_v2013 ‚Üí HVI_CensusTracts_v2013_Copy)
   4.3 CopyFeatures(Curb_Cuts_Intersections_2 ‚Üí CURB_CUT_CopyFeatures)
   4.4 Select(CURB_CUT_CopyFeatures ‚Üí Curb_Cuts_Intersections, where SUB_FEATURE_CODE=222700)
   4.5 PairwiseBuffer(Curb_Cuts_Intersections ‚Üí Curb_Cuts_Intersection_Buffer, 30 ft)
   4.6 CopyFeatures(Subway_Lines ‚Üí Subway_Lines_2)
   4.7 PairwiseBuffer(Subway_Lines_2 ‚Üí Subway_Lines_Buffer, 80 ft)

5. Convert CSV ‚Üí points
   5.1 ExportTable(Workorders.csv ‚Üí Workorders)
   5.2 XYTableToPoint(Workorders ‚Üí Workorders_XYTableToPoint)
   5.3 PairwiseBuffer(Workorders_XYTableToPoint ‚Üí WorkOrders_Buffer, 25 ft)
   5.4 ExportTable(TreeandSite.csv ‚Üí TreenSite)
   5.5 XYTableToPoint(TreenSite ‚Üí TreenSite_XYTableToPoint)
   5.6 PairwiseBuffer(TreenSite_XYTableToPoint ‚Üí TreeAndSite_Buffer, 25 ft)

6. Clean & prep shrub layer
   6.1 CopyFeatures(Grass_Shrub_ExportFeatures ‚Üí Grass_Shrub)
   6.2 DeleteField(Grass_Shrub, ["Id", "gridcode"])
   6.3 AddField("Pit_Type", TEXT, length=10)
   6.4 CalculateField(Pit_Type = "EP/LP")

7. Clean & prep HVI tracts
   7.1 CopyFeatures(FHNR_Datasets_HVI_CensusTracts ‚Üí HVI_CensusTracts_v2013_CopyFeatures)
   7.2 DeleteField(HVI_CensusTracts_v2013_CopyFeatures, [...lots of fields...])
   7.3 AlterField("QUINTILES" ‚Üí "HVI_CT_2013")

8. Union political & HVI boundaries
   8.1 Union([
         Political_Boundary,
         HVI_CensusTracts_v2013_CopyFeatures_2,
         NYCDOHMH_HVI_CensusTracts_2018_Clip,
         NYCDOHMH_HVI_CensusTracts_2023,
         NYCDOHMH_HVI_NeighborhoodTabulationAreas_2018,
         NYCDOHMH_HVI_NeighborhoodTabulationAreas_2023,
         NYCDCP_Borough_Boundaries_Water_Included,
         NYCDCP_Borough_Boundaries
       ] ‚Üí Political_Boundary_Final)
   8.2 DeleteField(Political_Boundary_Final, [all FID_* fields])

9. Further buffers & selections
   9.1 PairwiseBuffer(Curb_Cuts_Intersections ‚Üí Curb_Cuts_Intersection_20ft, 20 ft)
   9.2 Select(Street_Centerline WHERE L_LOW_HN <> '' ‚Üí Street_Centerline_Select)
   9.3 SimplifyLine(Street_Centerline_Select ‚Üí Street_Centerli_SimplifyLine, POINT_REMOVE, 1 ft)
   9.4 FeatureVerticesToPoints(Street_Centerli_SimplifyLine ‚Üí Street_Vertices_Points)
   9.5 PairwiseBuffer(Street_Vertices_Points ‚Üí Street_Vertice_Buffer, 40 ft)

10. Hydrant proximity & cleanup
   10.1 Near(DEP_Hydrants ‚Üí Sidewalk_Pluto, radius=10 ft, location)
   10.2 MoveStreetSigns(Hydrants_Near ‚Üí Hydrants_Corrected)
   10.3 PairwiseBuffer(Hydrants_Corrected ‚Üí DEP_Hydrants_PairwiseBuffer, 3 ft)

11. Driveway curb-cut processing
    11.1 Select(CURB_CUT_CopyFeatures WHERE SUB_FEATURE_CODE=222600 ‚Üí Curb_Cuts_Driveways)
    11.2 CurbCuts(input=Curb_Cuts_Driveways, extension=7, buffer=15 ‚Üí Curb_Cuts_Driveways_Buffer)

12. Vault union & cleanup
    12.1 Union([BK_Vaults_Buffer, DOT_Vault_Buffer] ‚Üí Vaults)
    12.2 DeleteField(Vaults, [a long list of original fields])
    12.3 AddField("Vaults", LONG)
    12.4 CalculateField(Vaults = 1)
    12.5 (Placeholder) Union(‚Ä¶ ‚Üí Output_Feature_Class)

13. Final buffer
    13.1 PairwiseBuffer(Curb_Cuts_Intersections ‚Üí Curb_Cuts_Intersection_10ft, 10 ft)
````

## src/stp/scrap.md

```markdown
# To Use / How To Use

## No Planting Areas / Locations to clip sidewalk

- NYZD  
  - Description: Zoning districts where planting is prohibited  
  - Filters: `"ZONE_DIST" IN ('M1','M2','M3','IG','IH')`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `nyzd_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `nyzd_ready`

- DEP_GI_Assets  
  - Description: Green infrastructure assets to avoid  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `dep_gi_assets_copy`  
    - Operation b: Buffer  
      - Params: distance = 20 Feet  
      - Output: `dep_gi_assets_ready`

- Sidewalk  
  - Description: Raw sidewalk polygons, split for different logic  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `sidewalk_copy`  
    - Operation b: Polygon‚ÜíPolyline  
      - Params: input = `sidewalk_copy`  
      - Output: `sidewalk_1`  
    - Operation c: Split immutable/mutable  
      - Params: input = `sidewalk_1`  
      - Outputs: `sidewalk_immutable_ready`, `sidewalk_mutable_ready`

- Curb_Cuts  
  - Description: Sidewalk curb-cut intersections  
  - Filters: `SUB_FEATURE_CODE = 222700`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `curb_cuts_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `curb_cuts_1`  
    - Operation c: Buffer  
      - Params: input = `curb_cuts_1`, distance = 30 Feet  
      - Output: `curb_cuts_ready`

- Subway_Lines  
  - Description: Subway buffers to exclude planting near tracks  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `subway_lines_copy`  
    - Operation b: Buffer  
      - Params: distance = 80 Feet  
      - Output: `subway_lines_ready`

- Workorders  
  - Description: Active DOT work orders  
  - Filters: `STATUS <> 'Cancelled'`  
  - Operations  
    - Operation a: CopyFeatures (ExportTable/Select)  
      - Output: `workorders_copy`  
    - Operation b: XYTableToPoint  
      - Params: input = `workorders_copy`  
      - Output: `workorders_1`  
    - Operation c: Buffer  
      - Params: input = `workorders_1`, distance = 25 Feet  
      - Output: `workorders_ready`

- TreeandSite  
  - Description: Existing tree locations to avoid  
  - Filters: `SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')`  
  - Operations  
    - Operation a: CopyFeatures (ExportTable/Select)  
      - Output: `treeandsite_copy`  
    - Operation b: XYTableToPoint  
      - Params: input = `treeandsite_copy`  
      - Output: `treeandsite_1`  
    - Operation c: Buffer  
      - Params: input = `treeandsite_1`, distance = 25 Feet  
      - Output: `treeandsite_ready`

- Grass_Shrub  
  - Description: 2017 land-use raster converted to shrub/grass polygons  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures (Copy Raster)  
      - Output: `grass_shrub_copy`  
    - Operation b: RasterToPolygon_conversion  
      - Params: simplify = NO_SIMPLIFY  
      - Output: `grass_shrub_1`  
    - Operation c: DeleteField  
      - Params: fields = `["gridcode","Id"]`  
      - Output: `grass_shrub_ready`

- Political_Boundary  
  - Description: Union of all political boundaries for spatial join  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `political_boundary_copy`  
    - Operation b: Union_analysis  
      - Params: inputs = borough, community boards, council, senate, assembly, NTA, tracts  
      - Output: `political_boundary_1`  
    - Operation c: DeleteField  
      - Params: fields = all `FID_*`  
      - Output: `political_boundary_ready`

- Street_Centerline  
  - Description: Simplify street geometry and buffer vertices  
  - Filters: `L_LOW_HN IS NOT NULL`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `street_centerline_copy`  
    - Operation b: SimplifyLine_cartography  
      - Params: method = POINT_REMOVE, tolerance = 1 Feet  
      - Output: `street_centerline_1`  
    - Operation c: FeatureVerticesToPoints  
      - Params: input = `street_centerline_1`  
      - Output: `street_centerline_2`  
    - Operation d: Buffer  
      - Params: input = `street_centerline_2`, distance = 40 Feet  
      - Output: `street_centerline_ready`

- DEP_Hydrants  
  - Description: Hydrant proximity adjustments  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `dep_hydrants_copy`  
    - Operation b: GenerateNearTable  
      - Params: distance = 10 Feet  
      - Output: `dep_hydrants_1`  
    - Operation c: MoveStreetSigns (custom)  
      - Params: as defined in script  
      - Output: `dep_hydrants_2`  
    - Operation d: Buffer  
      - Params: input = `dep_hydrants_2`, distance = 3 Feet  
      - Output: `dep_hydrants_ready`

- Curb_Cuts_Driveways  
  - Description: Driveway curb cuts to exclude planting  
  - Filters: `SUB_FEATURE_CODE = 222600`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `curb_cuts_driveways_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `curb_cuts_driveways_1`  
    - Operation c: Buffer  
      - Params: input = `curb_cuts_driveways_1`, distance = 15 Feet  
      - Output: `curb_cuts_driveways_ready`

## Final first step

- Final Clip  
  - Description: Compute allowable planting points within clipped sidewalk  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures (non-mutable backup)  
      - Params: input = `sidewalk_mutable_ready`  
      - Output: `sidewalk_mutable_backup`  
    - Operation b: Union no-plant zones  
      - Params: inputs = nyzd_ready, dep_gi_assets_ready, curb_cuts_ready, subway_lines_ready, workorders_ready, treeandsite_ready, grass_shrub_ready  
      - Output: `no_plant_zones_union`  
    - Operation c: Clip_analysis  
      - Params: input = `sidewalk_mutable_ready`, clip_features = `no_plant_zones_union`  
      - Output: `sidewalk_availability_ready`  
    - Operation d: Create planting points  
      - Params: method = CreateFishnet or custom  
      - Output: `plant_pts_1`  
    - Operation e: SpatialJoin  
      - Params: target = `plant_pts_1`, join_features = `sidewalk_availability_ready`  
      - Output: `plant_pts_ready`
```

## src/stp/storage/file_storage.py

```python
"""File-based GeoDataFrame helpers."""

from pathlib import Path

import geopandas as gpd
import pandas as pd

__all__ = [
    "get_geopackage_path",
    "sanitize_layer_name",
    "export_spatial_layer",
    "reproject_all_layers",
]

LAYER_NAME_MAX_LENGTH = 60


def get_geopackage_path(
    output_dir: Path, filename: str = "project_data.gpkg"
) -> Path:
    """Return a fresh GeoPackage path under *output_dir*."""
    gpkg = Path(output_dir) / filename
    if gpkg.exists():
        try:
            gpkg.unlink()
        except PermissionError as err:
            print(f"\u26a0\ufe0f Could not delete '{gpkg}': {err}")
    return gpkg


def sanitize_layer_name(name: str) -> str:
    """Return *name* cleaned for file or database layers."""
    safe = "".join(ch if (ch.isalnum() or ch == "_") else "_" for ch in name)
    if safe and safe[0].isdigit():
        safe = "_" + safe
    return safe[:LAYER_NAME_MAX_LENGTH]


def export_spatial_layer(gdf: gpd.GeoDataFrame, layer_name: str,
                         gpkg_path: Path) -> None:
    """Write ``gdf`` to ``gpkg_path`` under ``layer_name``."""
    gdf.to_file(gpkg_path, layer=layer_name, driver="GPKG")


def reproject_all_layers(
    gpkg_path: Path, metadata_csv: Path, target_epsg: int
) -> None:
    """Reproject each layer in the GeoPackage in place."""
    meta = pd.read_csv(metadata_csv)
    for _, row in meta.iterrows():
        layer_name = row["layer_id"]
        source_epsg = int(row["source_epsg"])
        raw_wkid = row.get("service_wkid")
        try:
            service_wkid = int(raw_wkid) if raw_wkid not in (None, "") else ""
        except ValueError:
            service_wkid = ""
        gdf = gpd.read_file(gpkg_path, layer=layer_name)
        if gdf.crs is None:
            gdf = gdf.set_crs(epsg=source_epsg, allow_override=True)
        else:
            gdf = gdf.to_crs(epsg=source_epsg)
        gdf = gdf.to_crs(epsg=target_epsg)
        gdf.to_file(gpkg_path, layer=layer_name, driver="GPKG")
        if service_wkid:
            print(
                f"Reprojected '{layer_name}': {service_wkid} ‚Üí {target_epsg}"
            )
        else:
            print(
                f"Reprojected '{layer_name}': {source_epsg} ‚Üí {target_epsg}"
            )
```

## src/stp/storage/__init__.py

```python

```

## src/stp/stp.md

`````markdown
# STP ‚Äì Spatial / Tabular Pipeline

`src/stp` is a lightweight toolkit for fetching raw spatial / tabular data,
cleaning it, and persisting it to GeoPackage or PostGIS.  
Everything is pure-Python, built on GeoPandas, Shapely and SQLAlchemy.

---

## Folder structure

```
![alt text](image.png)
stp/
‚îÇ
‚îú‚îÄ‚îÄ cleaning/        ‚Üê cleaning steps for specific datasets
‚îÇ   ‚îú‚îÄ‚îÄ address.py
‚îÇ   ‚îú‚îÄ‚îÄ trees.py
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îú‚îÄ‚îÄ fetchers/        ‚Üê source-specific download helpers
‚îÇ   ‚îú‚îÄ‚îÄ arcgis.py         # ArcGIS REST ‚Üí GeoJSON/Parquet
‚îÇ   ‚îú‚îÄ‚îÄ csv.py            # Plain CSV files (HTTP/local)
‚îÇ   ‚îú‚îÄ‚îÄ gdb.py            # FileGDB via ogr2ogr
‚îÇ   ‚îú‚îÄ‚îÄ geojson.py        # Arbitrary GeoJSON endpoints
‚îÇ   ‚îú‚îÄ‚îÄ gpkg.py           # Remote GeoPackage layers
‚îÇ   ‚îú‚îÄ‚îÄ socrata.py        # Socrata Open Data API
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îú‚îÄ‚îÄ inventory/       ‚Üê schema inspection & export utilities
‚îÇ   ‚îú‚îÄ‚îÄ export.py         # dump layer ‚Üí CSV/Markdown schema
‚îÇ   ‚îú‚îÄ‚îÄ gpkg.py           # field inventory for GeoPackage
‚îÇ   ‚îú‚îÄ‚îÄ postgis.py        # field inventory for PostGIS
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îú‚îÄ‚îÄ metadata/        ‚Üê read / write layer-level metadata
‚îÇ   ‚îú‚îÄ‚îÄ csv.py            # side-car CSV metadata files
‚îÇ   ‚îú‚îÄ‚îÄ db.py             # PostGIS / SQLite layer comments
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îú‚îÄ‚îÄ scripts/         ‚Üê one-off CLI entry points
‚îÇ   ‚îî‚îÄ‚îÄ download_utils.py # `python -m stp.scripts.download_utils`
‚îÇ
‚îú‚îÄ‚îÄ storage/         ‚Üê persistence back-ends
‚îÇ   ‚îú‚îÄ‚îÄ db\_storage.py     # PostGIS via SQLAlchemy
‚îÇ   ‚îú‚îÄ‚îÄ file\_storage.py   # GeoPackage / Shapefile
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îî‚îÄ‚îÄ (root modules)   ‚Üê orchestration & shared helpers
‚îú‚îÄ‚îÄ config\_loader.py
‚îú‚îÄ‚îÄ data\_cleaning.py
‚îú‚îÄ‚îÄ download.py
‚îú‚îÄ‚îÄ fields\_inventory.py
‚îú‚îÄ‚îÄ http\_client.py
‚îú‚îÄ‚îÄ settings.py
‚îú‚îÄ‚îÄ table.py
‚îî‚îÄ‚îÄ **init**.py

```

---

## Top-level module cheat-sheet

| Module | Purpose |
|--------|---------|
| **config_loader.py** | Read YAML/ENV configuration & expose `get_setting`, `get_constant`. |
| **settings.py** | Hard-coded fall-backs (NYSP EPSG 2263, default filenames, etc.). |
| **download.py** | ‚ÄúOrchestrator‚Äù ‚Äì loops through every fetcher listed in config and drops raw files into `Data/raw/`. |
| **http_client.py** | Thin `requests.Session` wrapper with retry & back-off. |
| **data_cleaning.py** | Pipeline runner ‚Äì re-projects, trims fields, fixes datatypes using functions from `cleaning/`. |
| **fields_inventory.py** | Generates a schema report (dtype, null %, sample) for any GeoDataFrame or DB layer. |
| **table.py** | Helpers to convert between CSV ‚Üî GeoJSON ‚Üî GeoPackage with consistent field ordering. |

---

## How the pieces fit

```

fetchers/        ‚Üí  Data/raw/\*.geojson / .csv
‚îÇ
‚ñº
cleaning/         GeoDataFrame in EPSG:2263
‚îÇ
‚ñº
storage/          PostGIS (db\_storage)  or  GeoPackage (file\_storage)
‚îÇ
‚ñº
inventory/        Optional schema dump / metadata write-back

````

---

### Quickstart

```bash
# 1) install deps
pip install -r requirements.txt   # add psycopg2-binary for PostGIS

# 2) pull all configured layers
python -m stp.download

# 3) clean & normalise
python -m stp.data_cleaning

# 4) inspect schema (optional)
python -m stp.fields_inventory Data/clean/addresses.gpkg
````

That‚Äôs it!
Drop this `README_stp.md` into `src/stp/` to give new contributors an instant
map of the toolkit.
`````

## src/stp/zip.py

```python
import os
import zipfile

def main():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    root_folder_name = os.path.basename(script_dir)
    zip_path = os.path.join(script_dir, 'archived_py_files.zip')
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(script_dir):
            for file in files:
                if file.endswith('.py'):
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(root, script_dir)
                    path_parts = [root_folder_name] if rel_path == '.' else [root_folder_name] + rel_path.split(os.sep)
                    prefix = '_'.join(path_parts) + '_'
                    new_name = prefix + file
                    with open(full_path, 'rb') as f:
                        content = f.read()
                    zipf.writestr(new_name, content)

if __name__ == '__main__':
    main()
```

## src/stp/__init__.py

```python

```

## src/temp/download_data.py

```python
"""
download_data.py

Main entry point for fetching spatial and tabular datasets based on a
configuration file and source registry. Supports Socrata, ArcGIS, and
direct URLs (CSV, GeoJSON, Shapefile, GPKG). Records metadata and
optionally reprojects in a GeoPackage or loads into PostGIS.
"""

import json
import logging
from pathlib import Path

# use get_setting (aliased to 'get') so both settings.yaml overrides and
# defaults.yaml fallbacks work the same way
from stp.config_loader import get_setting as get, get_constant

from stp.fetch import fetch_arcgis_vector

from stp.storage.file_storage import (
    get_geopackage_path,
    get_postgis_engine,
    reproject_all_layers,
    sanitize_layer_name,
    export_spatial_layer,
)
from stp.process.table import (
    record_layer_metadata_csv,
    record_layer_metadata_db,
)
from stp.fetch.lookup import FETCHERS

logger = logging.getLogger(__name__)


def setup_destinations():
    """Read config settings and prepare output destinations."""
    socrata_token = get("socrata.app_token")
    db_cfg = get("db", {})

    if db_cfg.get("enabled", False):
        db_engine = get_postgis_engine(db_cfg)
    else:
        db_engine = None

    output_epsg = get("data.output_epsg", get_constant("nysp_epsg"))
    out_shp_dir = Path(get("data.output_shapefile"))
    out_tbl_dir = Path(get("data.output_tables"))
    out_shp_dir.mkdir(parents=True, exist_ok=True)
    out_tbl_dir.mkdir(parents=True, exist_ok=True)

    if not db_engine:
        metadata_csv = out_tbl_dir / get_constant(
            "data_inventory_filename"
        )
        gpkg = get_geopackage_path(out_shp_dir)
        if metadata_csv.exists():
            try:
                metadata_csv.unlink()
            except OSError as e:
                logger.warning(
                    "Could not delete existing CSV '%s': %s", metadata_csv, e
                )
    else:
        metadata_csv = None
        gpkg = None

    return socrata_token, db_engine, gpkg, metadata_csv, output_epsg


def load_layer_list():
    """Load the list of layers to fetch from the JSON registry."""
    data_sources = Path("config") / "sources.json"
    with open(data_sources, encoding="utf-8") as f:
        return json.load(f)


def process_layer(
    layer, idx, total, socrata_token, db_engine, gpkg, metadata_csv
):
    """Fetch, record metadata, and store one layer."""
    layer_id = layer["id"]
    url = layer["url"]
    stype = layer.get("source_type")
    fmt = layer.get("format", "").lower()
    helper_fn = FETCHERS.get((stype, fmt))

    logger.info(
        "[%d/%d] %s (source_type=%s, format=%s)",
        idx,
        total,
        layer_id,
        stype,
        fmt,
    )

    if stype == "arcgis":
        raw = fetch_arcgis_vector(url)
        results = [
            (layer_id, gdf, src_epsg, wkid)
            for (_, gdf, src_epsg, wkid) in raw
        ]
    elif stype == "socrata":
        raw = helper_fn(url, app_token=socrata_token)
        results = [
            (layer_id, gdf, src_epsg, None)
            for (_, gdf, src_epsg) in raw
        ]
    else:
        raw = helper_fn(url)
        results = [
            (layer_id, gdf, src_epsg, None)
            for (_, gdf, src_epsg) in raw
        ]

    for raw_name, gdf, source_epsg, service_wkid in results:
        clean_name = sanitize_layer_name(raw_name)

        if db_engine:
            record_layer_metadata_db(
                db_engine,
                clean_name,
                url,
                source_epsg,
                service_wkid,
            )
            gdf.to_postgis(
                clean_name,
                db_engine,
                if_exists="replace",
                index=False,
            )
        else:
            record_layer_metadata_csv(
                metadata_csv,
                clean_name,
                url,
                source_epsg,
                service_wkid,
            )
            export_spatial_layer(gdf, clean_name, gpkg)


def finalize(gpkg, metadata_csv, output_epsg):
    """Reproject all layers in the GeoPackage to the target EPSG."""
    if gpkg and metadata_csv:
        reproject_all_layers(gpkg, metadata_csv, target_epsg=output_epsg)


def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
    )
    socrata_token, db_engine, gpkg, metadata_csv, output_epsg = (
        setup_destinations()
    )
    layers = load_layer_list()
    total = len(layers)
    for idx, layer in enumerate(layers, start=1):
        process_layer(
            layer,
            idx,
            total,
            socrata_token,
            db_engine,
            gpkg,
            metadata_csv,
        )
    finalize(gpkg, metadata_csv, output_epsg)


if __name__ == "__main__":
    main()
```

## src/temp/political_boundary.py

```python
"""
tempp
"""
from pathlib import Path
import geopandas as gpd
from sqlalchemy import create_engine
from shapely.ops import unary_union
from stp.config_loader import get_setting, get_constant
from stp.config_loader import get_setting, get_constant

# 1) Paths and config
base_dir = Path.cwd()
db_cfg = get_setting("db", {})
output_epsg = get_setting("data.output_epsg", get_constant("nysp_epsg", 2263))
output_dir = Path(get_setting("data.output_shapefile", "Data/shapefiles"))
output_dir.mkdir(parents=True, exist_ok=True)

# 2) Setup storage mode
engine = None
if db_cfg.get("enabled"):
    conn_url = (
        f"{db_cfg['driver']}://{db_cfg['user']}:{db_cfg['password']}@"
        f"{db_cfg['host']}:{db_cfg['port']}/{db_cfg['database']}"
    )
    engine = create_engine(conn_url)
else:
    gpkg_path = output_dir / get_constant(
        "default_gpkg_name", "project_data.gpkg"
    )
    if gpkg_path.exists():
        gpkg_path.unlink()

# 3) Define layers to process
layer_ids = [
    "borough", "community_districts", "city_council_districts",
    "us_congressional_districts", "state_senate_districts",
    "state_assembly_districts", "community_district_tabulation_areas",
    "neighborhood_tabulation_areas", "census_tracts", "census_blocks",
    "zoning_districts", "commercial_districts", "special_purpose_districts"
]

# 4) Load each layer into GeoDataFrames
gdfs = []
for layer in layer_ids:
    if engine:
        # Read from PostGIS
        gdf = gpd.read_postgis(
            f"SELECT * FROM {layer}", engine, geom_col="geometry"
        )
        gdf = gpd.read_postgis(
            f"SELECT * FROM {layer}", engine, geom_col="geometry"
        )
        gdf.set_crs(epsg=output_epsg, inplace=True)
    else:
        # Read from GeoPackage
        gdf = gpd.read_file(gpkg_path, layer=layer)
    gdfs.append(gdf)

# 5) Union all boundaries
all_union = unary_union([gdf.unary_union for gdf in gdfs])
result_gdf = gpd.GeoDataFrame([{"geometry": all_union}], crs=output_epsg)

# 6) Persist the result
if engine:
    result_gdf.to_postgis(
        "political_boundaries", engine, if_exists="replace", index=False
    )
else:
    result_gdf.to_file(
        gpkg_path, layer="political_boundaries", driver="GPKG"
    )
print("‚úÖ political_boundaries created")
```

## stp_repo.md

```````markdown
This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-07-11 12:23:18

# File Summary

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
   a. A header with the file path (## File: path/to/file)
   b. The full contents of the file in a code block

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Information:

For more information about Repomix, visit: https://github.com/andersonby/python-repomix


# Repository Structure

```
.codexignore
.gitignore
AGENTS.md
config
  defaults.yaml
  sources.json
  user.example.yaml
  user.yaml
  worfklow.yaml
data
  gpkg
    _gpkg.md
  table
    _table.md
docker.yaml
docs
  todo.md
License
pyproject.toml
README.md
requirements.txt
requirements_dev.txt
scrap
  01.md
  02.json
  03.md
  tokens.py
  zip.py
secrets
  secrets.yaml
src
  arcpy
    arcpy converted
      curb.py
      nostandingy.py
      rank.py
    arcpy depracated
      arcpy_First Step.py
      arcpy_nostanding.py
      arcpy_rank_dominant_working.py
      arcpy_Second_Step_Alternative.py
  stp
    cli
      stp_pipeline.py
    core
      config.py
      http.py
      settings.py
    fetch
      arcgis.py
      csv.py
      download.py
      gdb.py
      geojson.py
      gpkg.py
      lookup.py
      socrata.py
      __init__.py
    process
      clean
        address.py
        trees.py
        __init__.py
      custom_ops.py
      data_cleaning.py
      fields_inventory.py
      field_ops.py
      geometry_ops.py
      table.py
    record
      csv.py
      db.py
      export.py
      gpkg.py
      postgis.py
      __init__.py
    scaffold.md
    scrap.md
    storage
      db_storage.py
      file_storage.py
      __init__.py
    stp.md
    zip.py
    __init__.py
  temp
    download_data.py
    political_boundary.py
stp_repo.md
tests
  conftest.py
  test_address.py
  test_config_loader.py
  test_csv.py
  test_db_storage.py
  test_download.py
  test_file_storage.py
  test_geojson.py
  test_http_client.py
  test_trees.py
```

# Repository Files


## .codexignore

```text
# ignore VCS
.git/
# ignore virtual environments
.venv/
env/
# ignore data blobs
Data/
docs/
# ignore miscellaneous
*.tar
*.zip
AGENTS.md
hello.txt
scratch.py
```

## .gitignore

```text
# -------------------------------
# üêç Python build artifacts
# -------------------------------
__pycache__/
*.py[cod]
.venv/
.env/
.pytest_cache
*.log
/scrap
src/arcpy
data/
# -------------------------------
# üß™ Project-specific junk & output
# -------------------------------
*.tar
*.zip
project.zip
config/user.yaml
tokens.py
# -------------------------------
# üßæ Secret or local-only folders
# -------------------------------
secrets/

# -------------------------------
# üìä Data outputs (selectively ignored)
# -------------------------------
Data/table/*
Data/gpkg/*

# Keep placeholder files and readmes
!Data/table/
!Data/gpkg/
!Data/table/table.md
!Data/gpkg/gpkg.md

# -------------------------------
# üîß Developer tools/utilities
# -------------------------------
zip.py
project.zip
```

## AGENTS.md

````markdown
# AGENTS.md

## ‚úèÔ∏è Code Style Guide

This project enforces basic Python style conventions across `src/stp/`.

---

### üîπ Formatting Rules

- **Indentation**: 4 spaces  
- **Line length**: max 79 characters  
- **No trailing whitespace**  
- **Blank lines**:  
  - 2 between top-level functions and classes  
  - 1 between method definitions inside classes  

---

### üîπ Import Order

1. **Standard library**
2. **Third-party**
3. **Local modules** (`src/stp/...`)

Use `isort` or arrange manually to match.

---

### üîπ Docstrings

- Required on all **public functions** and **classes**
- Use triple double quotes (`"""docstring"""`)
- Be concise and descriptive

---

### üîπ Comments

- Use inline `#` comments sparingly ‚Äî only for **non-obvious logic**
- Avoid restating what the code already expresses clearly

---

### üîπ Enforcement

You can check for style issues using:

```bash
flake8 src/stp/ --max-line-length=79
pylint src/stp/
```

---

### ‚ùå Exclusions

These folders are not checked by linters:

- `.venv/`
- `Data/`
- `config/`
- `tests/gis_*`

---

This file focuses only on style. Testing, CI, and agents are optional layers you can add later.
````

## config/defaults.yaml

```yaml
epsg:
  default: 4326
  nysp: 2263

limits:
  socrata: 50000
  arcgis_default_max_records: 1000

validation:
  layer_name_max_length: 60
  min_dbh: 0.01
```

## config/sources.json

```json
[
    {
      "id": "trees",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/hn5i-inap.json",
      "schema": "political"
    },
    {
      "id": "work_orders",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/bdjm-n7q4.json",
      "schema": "political"
    },
    {
      "id": "planting_spaces",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/82zj-84is.json",
      "schema": "political"
    },
    {
      "id": "street_sign",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/qt6m-xctn.json",
      "schema": "political"
    },
    {
      "id": "hydrants",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/5bgh-vtsn.json",
      "schema": "political"
    },
    {
      "id": "green_infrastructure",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/df32-vzax.json",
      "schema": "political"
    },
    {
      "id": "subway_lines",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services.arcgis.com/ue9rwulIoeLEI9bj/arcgis/rest/services/NYC_SubwayLines/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "borough",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "community_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Community_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "council_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_City_Council_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "congressional_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/ArcGIS/rest/services/NYC_Congressional_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "senate_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_State_Senate_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "assembly_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_State_Assembly_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "community_tabulations",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Community_District_Tabulation_Areas_2020/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "neighborhood_tabulations",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Neighborhood_Tabulation_Areas_2020/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "census_tracts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Tracts_for_2020_US_Census/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "census_blocks",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Blocks_for_2020_US_Census/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "zoning_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyzd/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "commercial_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyco/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "special_purpose_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nysp/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "pluto",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/MAPPLUTO/FeatureServer/0",
      "schema": "land_use"
    },
    {
      "id": "street_center",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/DCM_Street_Center_Line/FeatureServer/0",
      "schema": "infrastructure"
    },
    {
      "id": "curb",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Curb_2022/FeatureServer/4",
      "schema": "infrastructure"
    },
    {
      "id": "curb_cut",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/ArcGIS/rest/services/Curb_Cut_2022/FeatureServer/5",
      "schema": "infrastructure"
    },
    {
      "id": "sidewalk",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Sidewalk_2022/FeatureServer/22",
      "schema": "infrastructure"
    }
  ]
```

## config/user.example.yaml

```yaml
epsg:
  default: 4326
  nysp: 2263

limits:
  socrata: 50000
  arcgis_default_max_records: 1000

validation:
  layer_name_max_length: 60
  min_dbh: 0.01
```

## config/user.yaml

```yaml
epsg:
  default: 4326
  nysp: 2263

limits:
  socrata: 50000
  arcgis_default_max_records: 1000

validation:
  layer_name_max_length: 60
  min_dbh: 0.01

paths:
  config:   "Config/config.yaml"
  constants: "Config/constants.yaml"
  output:
    shapefiles: "Data/shapefiles"
    tables:     "Data/tables"

filenames:
  data_inventory:  "data_inventory.csv"
  default_gpkg:    "project_data.gpkg"
```

## config/worfklow.yaml

```yaml
# Unified NYC Tree Planting Workflow Config (combined from your 01.md ops, 02.json sources, 03.md filters)
# Edit for toggles, other cities, or reordering. Pipeline reads this to run dynamically.

sources:  # From 02.json (URLs/types/schema) + 03.md (filters/simple types)
  trees:
    type: "json"  # From 03.md
    source_type: "socrata"  # From 02.json
    format: "json"
    url: "https://data.cityofnewyork.us/resource/hn5i-inap.json"
    schema: "political"
    filter: "SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')"  # From 03.md
    to_layer: "trees_raw"  # Initial save in GPKG
    enabled: true

  work_orders:
    type: "json"
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/bdjm-n7q4.json"
    schema: "political"
    filter: "STATUS <> 'Cancelled'"
    to_layer: "work_orders_raw"
    enabled: true

  planting_spaces:
    type: "json"  # Assumed from 03.md pattern (not listed, but in scaffold)
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/82zj-84is.json"
    schema: "political"
    filter: null  # None specified
    to_layer: "planting_spaces_raw"
    enabled: true

  street_sign:
    type: "json"
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/nfid-uabd.json"  # Updated from search (old qt6m-xctn invalid)
    schema: "political"
    filter: null  # Or "$where=sign_desc like '%NO STANDING%' OR sign_desc like '%NO PARKING%'" if pre-filtering; done in nostanding.py otherwise
    to_layer: "street_sign_raw"
    enabled: true

  hydrants:
    type: "json"
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/5bgh-vtsn.json"
    schema: "political"
    filter: null
    to_layer: "hydrants_raw"
    enabled: true

  green_infrastructure:
    type: "json"
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/df32-vzax.json"  # Confirmed current
    schema: "political"
    filter: null
    to_layer: "green_infrastructure_raw"
    enabled: true

  subway_lines:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services.arcgis.com/ue9rwulIoeLEI9bj/arcgis/rest/services/NYC_SubwayLines/FeatureServer/0"
    schema: "political"
    filter: null
    to_layer: "subway_lines_raw"
    enabled: true

  # ... (Adding the rest from 02.json/03.md similarly; I shortened for space, but include all in your file)
  borough:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary/FeatureServer/0"
    schema: "political"
    filter: null
    to_layer: "borough_raw"
    enabled: true

  # community_districts, council_districts, etc. - follow pattern above

  census_blocks:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Blocks_for_2020_US_Census/FeatureServer/0"
    schema: "political"
    filter: null
    to_layer: "census_blocks_raw"
    enabled: true

  zoning_districts:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyzd/FeatureServer/0"
    schema: "zoning"
    filter: null  # Filter in ops
    to_layer: "zoning_districts_raw"
    enabled: true

  # commercial_districts, special_purpose_districts, pluto, street_center, curb, curb_cut, sidewalk - similar

  sidewalk:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Sidewalk_2022/FeatureServer/22"
    schema: "infrastructure"
    filter: null
    to_layer: "sidewalk_raw"
    enabled: true

  grass_shrub:  # Added from search (2017 land cover raster for Grass_Shrub ops)
    type: "raster"  # Special handling (download GeoTIFF, convert to poly)
    source_type: "socrata"
    format: "geotiff"  # Export from Socrata
    url: "https://data.cityofnewyork.us/api/geospatial/he6d-2qns?method=export&format=GeoTIFF"  # Or Shapefile if preferred
    schema: "land_use"
    filter: null
    to_layer: "grass_shrub_raw"  # Save as raster or convert early
    enabled: true

prep_ops:  # From 01.md - ops per layer (description, filters if not in sources, steps)
  nyzd:  # Maps to zoning_districts source
    description: "Zoning districts where planting is prohibited"
    filter: "ZONE_DIST IN ('M1','M2','M3','IG','IH')"  # Applied in select step
    enabled: true
    steps:
      - op: copy
        input: "zoning_districts_raw"
        output: "nyzd_copy"
      - op: select
        query: "ZONE_DIST IN ('M1','M2','M3','IG','IH')"
        output: "nyzd_ready"

  dep_gi_assets:  # Maps to green_infrastructure
    description: "Green infrastructure assets to avoid"
    filter: null
    enabled: true
    steps:
      - op: copy
        input: "green_infrastructure_raw"
        output: "dep_gi_assets_copy"
      - op: buffer
        distance: 20  # Feet
        output: "dep_gi_assets_ready"

  sidewalk:
    description: "Raw sidewalk polygons, split for different logic"
    filter: null
    enabled: true
    steps:
      - op: copy
        input: "sidewalk_raw"
        output: "sidewalk_copy"
      - op: polygon_to_polyline
        input: "sidewalk_copy"
        output: "sidewalk_1"
      - op: split_immutable_mutable  # Custom? (Assume function for splitting)
        input: "sidewalk_1"
        outputs: ["sidewalk_immutable_ready", "sidewalk_mutable_ready"]

  curb_cuts:  # Maps to curb_cut (for intersections)
    description: "Sidewalk curb-cut intersections"
    filter: "SUB_FEATURE_CODE = 222700"
    enabled: true
    steps:
      - op: copy
        input: "curb_cut_raw"
        output: "curb_cuts_copy"
      - op: select
        query: "SUB_FEATURE_CODE = 222700"
        output: "curb_cuts_1"
      - op: buffer
        distance: 30
        output: "curb_cuts_ready"

  # curb_cuts_driveways: Separate group for driveway filter (same source as curb_cuts, but different filter/ops)
  curb_cuts_driveways:
    description: "Driveway curb cuts to exclude planting"
    filter: "SUB_FEATURE_CODE = 222600"
    enabled: true
    steps:
      - op: copy
        input: "curb_cut_raw"  # Reuse source
        output: "curb_cuts_driveways_copy"
      - op: select
        query: "SUB_FEATURE_CODE = 222600"
        output: "curb_cuts_driveways_1"
      - op: buffer
        distance: 15
        output: "curb_cuts_driveways_ready"

  subway_lines:
    description: "Subway buffers to exclude planting near tracks"
    filter: null
    enabled: true
    steps:
      - op: copy
        input: "subway_lines_raw"
        output: "subway_lines_copy"
      - op: buffer
        distance: 80
        output: "subway_lines_ready"

  workorders:
    description: "Active DOT work orders"
    filter: "STATUS <> 'Cancelled'"  # Already in sources, but repeated for clarity
    enabled: true
    steps:
      - op: copy  # Or export_table + select
        input: "work_orders_raw"
        output: "workorders_copy"
      - op: xy_to_point
        input: "workorders_copy"
        output: "workorders_1"
      - op: buffer
        distance: 25
        output: "workorders_ready"

  treeandsite:  # Maps to trees (or planting_spaces? Assumed trees based on filter)
    description: "Existing tree locations to avoid"
    filter: "SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')"
    enabled: true
    steps:
      - op: copy  # Export_table + select
        input: "trees_raw"
        output: "treeandsite_copy"
      - op: xy_to_point
        input: "treeandsite_copy"
        output: "treeandsite_1"
      - op: buffer
        distance: 25
        output: "treeandsite_ready"

  grass_shrub:
    description: "2017 land-use raster converted to shrub/grass polygons"
    filter: null
    enabled: true
    steps:
      - op: copy  # Copy raster
        input: "grass_shrub_raw"
        output: "grass_shrub_copy"
      - op: raster_to_polygon
        simplify: false
        output: "grass_shrub_1"
      - op: delete_field
        fields: ["gridcode", "Id"]
        output: "grass_shrub_ready"

  political_boundary:  # Composite (no direct source; unions others)
    description: "Union of all political boundaries for spatial join"
    filter: null
    enabled: true
    steps:
      - op: copy  # Or direct union
        input: null  # No single input
        output: "political_boundary_copy"
      - op: union
        inputs: ["borough_raw", "community_districts_raw", "council_districts_raw", "congressional_districts_raw", "senate_districts_raw", "assembly_districts_raw", "community_tabulations_raw", "neighborhood_tabulations_raw", "census_tracts_raw"]  # From scaffold
        output: "political_boundary_1"
      - op: delete_field
        fields: ["FID_*"]  # All FID_ fields
        output: "political_boundary_ready"

  street_centerline:  # Maps to street_center
    description: "Simplify street geometry and buffer vertices"
    filter: "L_LOW_HN IS NOT NULL"
    enabled: true
    steps:
      - op: copy
        input: "street_center_raw"
        output: "street_centerline_copy"
      - op: simplify_line
        method: "POINT_REMOVE"
        tolerance: 1  # Feet
        output: "street_centerline_1"
      - op: vertices_to_points
        input: "street_centerline_1"
        output: "street_centerline_2"
      - op: buffer
        distance: 40
        output: "street_centerline_ready"

  dep_hydrants:  # Maps to hydrants
    description: "Hydrant proximity adjustments"
    filter: null
    enabled: true
    steps:
      - op: copy
        input: "hydrants_raw"
        output: "dep_hydrants_copy"
      - op: generate_near_table
        distance: 10
        output: "dep_hydrants_1"
      - op: move_street_signs  # Custom script/tool
        input: "dep_hydrants_1"
        output: "dep_hydrants_2"
      - op: buffer
        distance: 3
        output: "dep_hydrants_ready"

final_clip:  # From 01.md end - separate section for pipeline end
  description: "Compute allowable planting points within clipped sidewalk"
  filter: null
  enabled: true
  steps:
    - op: copy
      input: "sidewalk_mutable_ready"
      output: "sidewalk_mutable_backup"
    - op: union
      inputs: ["nyzd_ready", "dep_gi_assets_ready", "curb_cuts_ready", "subway_lines_ready", "workorders_ready", "treeandsite_ready", "grass_shrub_ready"]
      output: "no_plant_zones_union"
    - op: clip
      input: "sidewalk_mutable_ready"
      clip_with: "no_plant_zones_union"
      output: "sidewalk_availability_ready"
    - op: create_points  # Custom: Fishnet or generate along lines
      input: "sidewalk_availability_ready"
      method: "fishnet"  # Or your rank_dominant
      output: "plant_pts_1"
    - op: spatial_join
      target: "plant_pts_1"
      join_features: "sidewalk_availability_ready"  # Plus political_boundary_ready?
      output: "plant_pts_ready"

big_scripts:  # Optional: Tie in nostanding/rank/curb (run after prep)
  nostanding:
    enabled: true
    input_csv: "street_sign_raw"  # Or path
    sw_layer: "sidewalk_mutable_ready"
    out_layer: "no_standing_ready"

  # rank_dominant, curb - as before
```

## data/gpkg/_gpkg.md

```markdown
# Data

## Data sources will be generated in this folder. Will be included in the gitignore because it doesn't need to be on github.
```

## data/table/_table.md

```markdown
# Data

## Helping data tables will be generated in this folder. Will be included in the gitignore because it doesn't need to be on github.
```

## docs/todo.md

```markdown
# TO DO 
## 7/6/2025
- [ ] I removed some items in user.example.yaml, so I need to make sure those removed variables aren't used elsewhere. 
- [ ] Scafffold a main.py within stp. Needs to follow the logic of the primary scope in README
- [ ] I need to find a way to utilize .env corerctly. Perhaps the best thing is to run a script on open, to check if .env is there, else create folder, create yaml with default inputs. 
- [ ] List out the steps of the project within the stp readme. 
- [ ] Parametize the pipeline for users. If they need to change when union happens in the model, figure it out.
- [ ] Create gui, for user to input parameters, select operations, input local features if they have it.
```

## License

```text
All Rights Reserved.

Copyright (c) 2025 Shawn Ganz.

This repository and its contents may not be used, copied, or distributed in any form without explicit written permission from the owner.

Unauthorized use, reproduction, or distribution is strictly prohibited.
```

## pyproject.toml

```text
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "stp"
version = "0.1.0"
description = "Street Tree Planting Analysis tools"
authors = [ { name="Shawn Ganz", email="ganz.shawn@gmail.com" } ]
readme = "README.md"
license = { file="LICENSE" }
requires-python = ">=3.10"
dependencies = [
    "geopandas>=0.14.0",
    "pandas>=2.0",
    "shapely>=2.0",
    "sqlalchemy>=2.0",
    "requests>=2.0",
    "pyproj",
    "psycopg2-binary",
]

[project.scripts]  # CLI entry points!
stp-download = "stp.download:main"  # Add a main() function to download.py if needed
stp-clean = "stp.data_cleaning:main"  # Same for others
stp-inventory = "stp.fields_inventory:main"
```

## README.md

````markdown
# Street Tree Planting Analysis (STP)

Scripts and tools for downloading, processing, and analyzing New York City street tree and sidewalk data.

---

## Table of Contents

* [Installation](#installation)
* [Environment Setup](#environment-setup)
* [Configuration](#configuration)
* [Folder Structure](#folder-structure)
* [Contributing](#contributing)
* [License](#license)

---

## Installation

1. **Clone the repository:**

   ```bash
   git clone https://github.com/shawnganznyc/Street_Tree_Planting_Analysis.git
   cd Street_Tree_Planting_Analysis
   ```

2. **Install dependencies**
pip install -r requirements.txt

## Configuration

1. **Edit configuration files:**

   * `config/defaults.yaml` ‚Äî project-wide constants (safe to commit)
   * `config/user.yaml` ‚Äî user-specific secrets/overrides (do **NOT** commit)

2. **On first use:**

   * Copy `config/user.example.yaml` to `config/user.yaml` and fill in any required secrets (e.g. Socrata app token).
   * Ensure `config/user.yaml` is listed in `.gitignore`.

---

## Folder Structure

```
config/      - Configuration files (defaults, user secrets)
src/stp/     - Main Python package (cleaning, fetchers, storage, etc.)
Data/        - Downloaded/generated data (not tracked in version control)
tests/       - Unit and integration tests
```

---

## Scope

1. Read user parameters
2. Download files
3. Create GIS assets if specified/possible
4. Clean
   * Read user settings per item to figure out what fields to keep. 
5. Manipulate
   * Shapes undergo geoprocessing functions
      * User specified options
         * ex: Buffer, 20 ft. 
   * Pipeline functions
      * union
      * clip
      * merge
      * spatial join
      * custom operations
6. Combine
7. Output
   * Number of potential trees in the area. 

# Primary Scope

1. Read established parameters
2. Download files from arcgis/nyc opendata
3. Convert json into geojson point/polygons
4. Clean files of unecessary fields
5. Apply buffers, filters, custom scripts. 
6. Create a sidewalk polyline using polygon buffer
   - Copy non mutable sidewalk polyline
7. Merge all polygons where plantings cannot occur. 
8. Clip do not plant locations with sidewalk polyline. 
9. Use non mutable sidewalk polyline to use for traffic signs / parking rules
   - Build no parking zones
      - Classify no parking into bins: 
         - No Parking
         - No Standing
            - No Standing Taxi
            - No Standing Truck Loading
            - No Standing (something else, forgot)
   - Build sidewalk vehicle parking times into each sidewalk 
   - Build mta no bus locations/rules
10. Generate potential planting locations
11. Join potential planting location with sidewalk information
12. Done.

# Current Goal
- Functioning python pipeline
   - Pipeline already developed on arcpy.
- User parameter control
   - Allow user to manipulate variables/parameters. 
      - Atm the original project use parameters and variables are somewhat set. 
- Automatic pipeline based on geography
   - All inputs need to be faithful to the pipeline, otherwise errors will occur
   - Template settings for all major cities? 

## Contributing

Pull requests and issues are welcome. Please open an issue to discuss major changes before submitting a PR.

---

## License

This project is open-source. See `LICENSE` for details.

---

**Tip:**
All user secrets and API keys go in `config/user.yaml` (never committed).
Project settings and defaults live in `config/defaults.yaml`.

---

Let me know if you want any sections expanded or customized for your onboarding workflow.
````

## requirements.txt

```text
fiona==1.10.1
geopandas==1.1.1
pandas==2.3.0
python-dotenv==1.1.1
PyYAML==6.0.2
PyYAML==6.0.2
Requests==2.32.4
Shapely==2.1.1
SQLAlchemy==2.0.41
```

## scrap/01.md

```markdown
- NYZD  
  - Description: Zoning districts where planting is prohibited  
  - Filters: `"ZONE_DIST" IN ('M1','M2','M3','IG','IH')`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `nyzd_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `nyzd_ready`

- DEP_GI_Assets  
  - Description: Green infrastructure assets to avoid  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `dep_gi_assets_copy`  
    - Operation b: Buffer  
      - Params: distance = 20 Feet  
      - Output: `dep_gi_assets_ready`

- Sidewalk  
  - Description: Raw sidewalk polygons, split for different logic  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `sidewalk_copy`  
    - Operation b: Polygon‚ÜíPolyline  
      - Params: input = `sidewalk_copy`  
      - Output: `sidewalk_1`  
    - Operation c: Split immutable/mutable  
      - Params: input = `sidewalk_1`  
      - Outputs: `sidewalk_immutable_ready`, `sidewalk_mutable_ready`

- Curb_Cuts  
  - Description: Sidewalk curb-cut intersections  
  - Filters: `SUB_FEATURE_CODE = 222700`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `curb_cuts_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `curb_cuts_1`  
    - Operation c: Buffer  
      - Params: input = `curb_cuts_1`, distance = 30 Feet  
      - Output: `curb_cuts_ready`

- Subway_Lines  
  - Description: Subway buffers to exclude planting near tracks  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `subway_lines_copy`  
    - Operation b: Buffer  
      - Params: distance = 80 Feet  
      - Output: `subway_lines_ready`

- Workorders  
  - Description: Active DOT work orders  
  - Filters: `STATUS <> 'Cancelled'`  
  - Operations  
    - Operation a: CopyFeatures (ExportTable/Select)  
      - Output: `workorders_copy`  
    - Operation b: XYTableToPoint  
      - Params: input = `workorders_copy`  
      - Output: `workorders_1`  
    - Operation c: Buffer  
      - Params: input = `workorders_1`, distance = 25 Feet  
      - Output: `workorders_ready`

- TreeandSite  
  - Description: Existing tree locations to avoid  
  - Filters: `SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')`  
  - Operations  
    - Operation a: CopyFeatures (ExportTable/Select)  
      - Output: `treeandsite_copy`  
    - Operation b: XYTableToPoint  
      - Params: input = `treeandsite_copy`  
      - Output: `treeandsite_1`  
    - Operation c: Buffer  
      - Params: input = `treeandsite_1`, distance = 25 Feet  
      - Output: `treeandsite_ready`

- Grass_Shrub  
  - Description: 2017 land-use raster converted to shrub/grass polygons  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures (Copy Raster)  
      - Output: `grass_shrub_copy`  
    - Operation b: RasterToPolygon_conversion  
      - Params: simplify = NO_SIMPLIFY  
      - Output: `grass_shrub_1`  
    - Operation c: DeleteField  
      - Params: fields = `["gridcode","Id"]`  
      - Output: `grass_shrub_ready`

- Political_Boundary  
  - Description: Union of all political boundaries for spatial join  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `political_boundary_copy`  
    - Operation b: Union_analysis  
      - Params: inputs = borough, community boards, council, senate, assembly, NTA, tracts  
      - Output: `political_boundary_1`  
    - Operation c: DeleteField  
      - Params: fields = all `FID_*`  
      - Output: `political_boundary_ready`

- Street_Centerline  
  - Description: Simplify street geometry and buffer vertices  
  - Filters: `L_LOW_HN IS NOT NULL`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `street_centerline_copy`  
    - Operation b: SimplifyLine_cartography  
      - Params: method = POINT_REMOVE, tolerance = 1 Feet  
      - Output: `street_centerline_1`  
    - Operation c: FeatureVerticesToPoints  
      - Params: input = `street_centerline_1`  
      - Output: `street_centerline_2`  
    - Operation d: Buffer  
      - Params: input = `street_centerline_2`, distance = 40 Feet  
      - Output: `street_centerline_ready`

- DEP_Hydrants  
  - Description: Hydrant proximity adjustments  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `dep_hydrants_copy`  
    - Operation b: GenerateNearTable  
      - Params: distance = 10 Feet  
      - Output: `dep_hydrants_1`  
    - Operation c: MoveStreetSigns (custom)  
      - Params: as defined in script  
      - Output: `dep_hydrants_2`  
    - Operation d: Buffer  
      - Params: input = `dep_hydrants_2`, distance = 3 Feet  
      - Output: `dep_hydrants_ready`

- Curb_Cuts_Driveways  
  - Description: Driveway curb cuts to exclude planting  
  - Filters: `SUB_FEATURE_CODE = 222600`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `curb_cuts_driveways_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `curb_cuts_driveways_1`  
    - Operation c: Buffer  
      - Params: input = `curb_cuts_driveways_1`, distance = 15 Feet  
      - Output: `curb_cuts_driveways_ready`

## Final first step

- Final Clip  
  - Description: Compute allowable planting points within clipped sidewalk  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures (non-mutable backup)  
      - Params: input = `sidewalk_mutable_ready`  
      - Output: `sidewalk_mutable_backup`  
    - Operation b: Union no-plant zones  
      - Params: inputs = nyzd_ready, dep_gi_assets_ready, curb_cuts_ready, subway_lines_ready, workorders_ready, treeandsite_ready, grass_shrub_ready  
      - Output: `no_plant_zones_union`  
    - Operation c: Clip_analysis  
      - Params: input = `sidewalk_mutable_ready`, clip_features = `no_plant_zones_union`  
      - Output: `sidewalk_availability_ready`  
    - Operation d: Create planting points  
      - Params: method = CreateFishnet or custom  
      - Output: `plant_pts_1`  
    - Operation e: SpatialJoin  
      - Params: target = `plant_pts_1`, join_features = `sidewalk_availability_ready`  
      - Output: `plant_pts_ready`
```

## scrap/02.json

```json
[
    {
      "id": "trees",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/hn5i-inap.json",
      "schema": "political"
    },
    {
      "id": "work_orders",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/bdjm-n7q4.json",
      "schema": "political"
    },
    {
      "id": "planting_spaces",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/82zj-84is.json",
      "schema": "political"
    },
    {
      "id": "street_sign",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/qt6m-xctn.json",
      "schema": "political"
    },
    {
      "id": "hydrants",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/5bgh-vtsn.json",
      "schema": "political"
    },
    {
      "id": "green_infrastructure",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/df32-vzax.json",
      "schema": "political"
    },
    {
      "id": "subway_lines",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services.arcgis.com/ue9rwulIoeLEI9bj/arcgis/rest/services/NYC_SubwayLines/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "borough",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "community_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Community_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "council_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_City_Council_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "congressional_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/ArcGIS/rest/services/NYC_Congressional_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "senate_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_State_Senate_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "assembly_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_State_Assembly_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "community_tabulations",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Community_District_Tabulation_Areas_2020/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "neighborhood_tabulations",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Neighborhood_Tabulation_Areas_2020/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "census_tracts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Tracts_for_2020_US_Census/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "census_blocks",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Blocks_for_2020_US_Census/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "zoning_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyzd/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "commercial_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyco/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "special_purpose_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nysp/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "pluto",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/MAPPLUTO/FeatureServer/0",
      "schema": "land_use"
    },
    {
      "id": "street_center",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/DCM_Street_Center_Line/FeatureServer/0",
      "schema": "infrastructure"
    },
    {
      "id": "curb",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Curb_2022/FeatureServer/4",
      "schema": "infrastructure"
    },
    {
      "id": "curb_cut",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/ArcGIS/rest/services/Curb_Cut_2022/FeatureServer/5",
      "schema": "infrastructure"
    },
    {
      "id": "sidewalk",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Sidewalk_2022/FeatureServer/22",
      "schema": "infrastructure"
    }
  ]
```

## scrap/03.md

```markdown
2.1 Define source paths & queries:
   - All this information can be found within root/config/sources.json
    - trees
        - type: json
        - filter: `SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')` 

    - work_orders
        - type: json
        - filter: `STATUS <> 'Cancelled'`

    - street_sign
        - type: json
        - filter: *Need help*

    - hydrants
        - type: json
        - filter: None

    - green_infrastructure
        - type: json

    - subway_lines
        - shapefile

    - borough
        - shapefile

    - community_districts
        - shapefile

    - council_districts
        - shapefile

    - congressional_districts
        - shapefile

    - senate_districts
        - shapefile

    - assembly_districts
        - shapefile

    - community_tabulations
        - shapefile

    - neighborhood_tabulations
        - shapefile

    - census_tracts
        - shapefile

    - census_blocks
        - shapefile

    - zoning_districts
        - shapefile

    - commercial_districts
        - shapefile

    - special_purpose_districts
        - shapefile

    - pluto
        - shapefile

    - street_center
        - shapefile

    - curb
        - shapefile

    - curb_cut
        - shapefile

    - sidewalk
        - shapefile
```

## scrap/tokens.py

```python
import pathlib, tiktoken

enc  = tiktoken.get_encoding("cl100k_base")
root = pathlib.Path(r"C:\Projects\stp\src")

total = 0
for p in root.rglob("*.py"):
    try:
        text = p.read_text(encoding="utf‚Äë8")
    except UnicodeDecodeError:
        # last‚Äëditch: read as Latin‚Äë1 and ignore bad bytes
        text = p.read_text(encoding="latin‚Äë1", errors="ignore")
    total += len(enc.encode(text))

print(f"{total:,} tokens in {root}")
```

## scrap/zip.py

```python
"""
Zip up this project (no enclosing folder), excluding .git, .venv, env,
project.zip, and any .DS_Store files.
"""
import os
import zipfile

# Files or folders to skip entirely
EXCLUDES = {'.git', '.venv', 'env', 'project.zip'}
# Project-root name for the output archive
OUTPUT = 'project.zip'


def should_exclude(path: str) -> bool:
    """ Exclusions"""
    parts = path.split(os.sep)
    # skip any path containing an excluded folder
    if any(p in EXCLUDES for p in parts):
        return True
    # skip macOS DS_Store files
    if path.endswith('.DS_Store'):
        return True
    return False


def main() -> None:
    """ runs zip"""
    # remove old archive if it exists
    try:
        os.remove(OUTPUT)
    except OSError:
        pass

    with zipfile.ZipFile(OUTPUT, 'w', zipfile.ZIP_DEFLATED) as zf:
        for root, dirs, files in os.walk('.'):
            # prune dirs in-place so we never descend into .git/.venv/etc.
            dirs[:] = [d for d in dirs if d not in EXCLUDES]
            for fname in files:
                full = os.path.join(root, fname)
                if should_exclude(full):
                    continue
                # store relative path so no leading "./"
                arc = os.path.relpath(full, '.')
                zf.write(full, arc)


if __name__ == '__main__':
    main()
```

## secrets/secrets.yaml

```yaml
Socrate api: "7dUkig8gydigidDN6G638J8Lr"
```

## src/arcpy/arcpy converted/curb.py

```python
"""Generate curb polygons around lines using GeoPandas/Shapely."""
import math
from pathlib import Path

import geopandas as gpd
import yaml
from shapely.geometry import Polygon

def get_dominant_segment_angle(line):
    """Return angle (radians) of the longest segment in a LineString."""
    max_len = 0.0
    best_angle = 0.0
    coords = list(line.coords)
    for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):
        dx = x2 - x1
        dy = y2 - y1
        seg_len = math.hypot(dx, dy)
        if seg_len > max_len:
            max_len = seg_len
            best_angle = math.atan2(dy, dx)
    return best_angle

def generate_polygons(lines_gdf, extension_distance, buffer_width):
    """Return GeoDataFrame of rectangles around each line."""
    polys = []
    for line in lines_gdf.geometry:
        if line is None or len(line.coords) < 2:
            continue

        angle = get_dominant_segment_angle(line)
        sx, sy = line.coords[0]
        ex, ey = line.coords[-1]

        # Extend line ends
        sx -= extension_distance * math.cos(angle)
        sy -= extension_distance * math.sin(angle)
        ex += extension_distance * math.cos(angle)
        ey += extension_distance * math.sin(angle)

        # Buffer offset
        dx = (buffer_width / 2.0) * math.sin(angle)
        dy = (buffer_width / 2.0) * math.cos(angle)

        points = [
            (sx - dx, sy + dy),
            (ex - dx, ey + dy),
            (ex + dx, ey - dy),
            (sx + dx, sy - dy),
        ]
        polys.append(Polygon(points))

    return gpd.GeoDataFrame({"geometry": polys}, crs=lines_gdf.crs)

def main():
    """Entry point for CLI usage: build curb buffer polygons."""
    base_dir = Path.cwd()
    cfg_path = base_dir / "config" / "config.yaml"
    with open(cfg_path) as f:
        config = yaml.safe_load(f)

    curb_cfg = config.get("curb", {})
    ext_dist = float(curb_cfg.get("extension_distance", 0))
    buff_width = float(curb_cfg.get("buffer_width", 0))

    out_dir = Path(config.get("output_shapefiles", "Data/shapefiles"))
    gpkg = out_dir / "project_data.gpkg"

    lines = gpd.read_file(gpkg, layer="curb")
    polys = generate_polygons(lines, ext_dist, buff_width)

    polys.to_file(gpkg, layer="curb_buffer", driver="GPKG")
    print(f"‚úÖ wrote {gpkg} layer 'curb_buffer'")

    return gpkg


if __name__ == "__main__":
    main()
```

## src/arcpy/arcpy converted/nostandingy.py

```python
"""GeoPandas version of nostanding.py: Flag no-standing areas on sidewalks."""

import logging

import geopandas as gpd
import pandas as pd
import yaml
from shapely.affinity import translate
from shapely.geometry import Point
from shapely.ops import snap

logging.basicConfig(level=logging.INFO)  # Simple logging


def classify(raw):
    """Short code for sign text (same as original)."""
    txt = str(raw).upper()
    if "NO STANDING" in txt:
        return "NSTAND"
    if "NO PARKING" in txt:
        return "NPARK"
    if "HMP" in txt:
        return "HMP"
    if "TAXI" in txt or "HOTEL" in txt or "LOADING" in txt or "PASSENGER" in txt:
        return "CURBSIDE"
    return "OTHER"


def load_filter(csv_path, desc_f, side_f):
    """Clean and filter sign DataFrame (same as original, but returns GDF)."""
    df = pd.read_csv(csv_path)
    df[side_f] = df[side_f].astype(str).str.strip().str.upper().str[0]
    df = df[df[side_f].isin(['N', 'S', 'E', 'W'])]
    keep = "NO STANDING|NO PARKING|HMP|TAXI|HOTEL|LOADING|PASSENGER"
    df = df[df[desc_f].str.upper().str.contains(keep, na=False)]
    df = df[pd.to_numeric(df["sign_x_coord"], errors='coerce').notna()]
    df = df[pd.to_numeric(df["sign_y_coord"], errors='coerce').notna()]
    df["x_r"] = df["sign_x_coord"].round(1)
    df["y_r"] = df["sign_y_coord"].round(1)
    df = df.drop_duplicates(subset=["x_r", "y_r"]).reset_index(drop=True)

    def _pa(s):
        s = str(s).upper()
        if "<->" in s:
            return "<->"
        elif "-->" in s or "->" in s:
            return "->"
        elif "<--" in s or "<-" in s:
            return "<-"
        return None
    df["parsed_arrow"] = df[desc_f].apply(_pa)
    df = df[df["parsed_arrow"].notna()]
    df["sign_type"] = df[desc_f].map(classify)
    df["geometry"] = df.apply(lambda row: Point(row["sign_x_coord"], row["sign_y_coord"]), axis=1)
    gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:2263')
    logging.info(f"Loaded and filtered {len(gdf)} signs.")
    return gdf


def shift_points(gdf, side_f, shift_ft):
    """Shift points off curb based on side."""
    def shift_pt(pt, side):
        if side == 'N':
            return translate(pt, 0, shift_ft)
        elif side == 'S':
            return translate(pt, 0, -shift_ft)
        elif side == 'E':
            return translate(pt, shift_ft, 0)
        elif side == 'W':
            return translate(pt, -shift_ft, 0)
        return pt
    gdf['geometry'] = gdf.apply(lambda row: shift_pt(row.geometry, row[side_f]), axis=1)
    logging.info("Shifted signs.")
    return gdf


def snap_to_sidewalks(signs_gdf, sw_gdf, tolerance=60):
    """Snap signs to nearest sidewalk within tolerance feet."""
    signs_gdf = gpd.sjoin_nearest(signs_gdf, sw_gdf, distance_col='near_dist', max_distance=tolerance)
    signs_gdf = signs_gdf[signs_gdf['near_dist'].notna()]  # Drop orphans
    
    def snap_geom(row):
        if pd.isna(row['near_dist']):
            return row.geometry
        line = sw_gdf.loc[row['index_right']].geometry
        return snap(row.geometry, line, tolerance)
    signs_gdf['geometry'] = signs_gdf.apply(snap_geom, axis=1)
    logging.info(f"Snapped {len(signs_gdf)} signs to sidewalks.")
    return signs_gdf


def segment_compass(seg_geom, sign_pt):
    """Compass direction relative to sign (north/south/east/west)."""
    dx = seg_geom.centroid.x - sign_pt.centroid.x
    dy = seg_geom.centroid.y - sign_pt.centroid.y
    if abs(dx) >= abs(dy):
        return "east" if dx > 0 else "west"
    return "north" if dy > 0 else "south"


def erase_gaps_and_flag(sw_gdf, signs_gdf, side_f):
    """Erase 3ft gaps at signs, explode, flag no_stand."""
    gaps = signs_gdf.buffer(1.5)  # 3ft diameter
    sw_split = sw_gdf.difference(gaps.unary_union)
    sw_split = sw_split.explode(ignore_index=True)  # Multipart to singlepart
    sw_split = sw_split[sw_split.length > 0.05]  # Drop slivers

    # Spatial join segments to signs (within 2ft)
    joined = gpd.sjoin(sw_split, signs_gdf, how='left', predicate='within', distance=2)
    
    # Add compass and flag
    arrow_to_compass = {
        "E": {"<-": "north", "->": "south"},
        "W": {"<-": "south", "->": "north"},
        "N": {"<-": "west",  "->": "east"},
        "S": {"<-": "east",  "->": "west"},
    }

    def get_compass(row):
        if row['parsed_arrow'] == "<->":
            return "both"
        curb = row[side_f].strip().upper() if pd.notna(row[side_f]) else ""
        arr = row['parsed_arrow'].strip() if pd.notna(row['parsed_arrow']) else ""
        return arrow_to_compass.get(curb, {}).get(arr)
    joined['compass'] = joined.apply(get_compass, axis=1)
    
    def get_flag(row):
        if row['compass'] == "both":
            return 1
        seg_side = segment_compass(row.geometry, signs_gdf.loc[row['index_right']].geometry)
        return 1 if seg_side == row['compass'] else 0
    joined['no_stand'] = joined.apply(get_flag, axis=1)
    
    # Aggregate max no_stand per segment (groupby)
    aggregated = joined.groupby(joined.index)['no_stand'].max().reset_index()
    sw_split['no_stand'] = aggregated['no_stand']
    
    logging.info("Erased gaps and flagged segments.")
    return sw_split


def main(csv_path, sw_path, out_path, desc_f='desc', side_f='side', shift_ft=5.0):
    """Run the full no-standing process."""
    try:
        with open('config/config.yaml') as f:  # Use your config
            config = yaml.safe_load(f)
        # Override with config if needed
        
        signs_gdf = load_filter(csv_path, desc_f, side_f)
        sw_gdf = gpd.read_file(sw_path)  # e.g., GeoPackage layer
        signs_gdf = shift_points(signs_gdf, side_f, shift_ft)
        signs_gdf = snap_to_sidewalks(signs_gdf, sw_gdf)
        final_gdf = erase_gaps_and_flag(sw_gdf, signs_gdf, side_f)
        final_gdf.to_file(out_path, driver='GPKG')
        logging.info(f"Done! Output: {out_path}")
    except Exception as e:
        logging.error(f"Oops: {e}")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Process no-standing signs.")
    parser.add_argument('--csv', default='data/signs.csv', help='Sign CSV path')
    parser.add_argument('--sw', default='data/project_data.gpkg|layer=sidewalks', help='Sidewalk GPKG')
    parser.add_argument('--out', default='data/no_stand.gpkg', help='Output GPKG')
    args = parser.parse_args()
    main(args.csv, args.sw, args.out)
```

## src/arcpy/arcpy converted/rank.py

```python
"""GeoPandas version of rank_dominant_working.py: Prune planting points."""

import logging
import geopandas as gpd
from shapely import line_interpolate_point, length

logging.basicConfig(level=logging.INFO)


def generate_points_along_lines(lines_gdf, spacing):
    """Create points every 'spacing' feet along lines."""
    points = []
    for idx, row in lines_gdf.iterrows():
        line = row.geometry
        dist = 0
        while dist < length(line):
            pt = line_interpolate_point(line, dist)
            points.append({'geometry': pt, 'parent_fid': idx, 'parent_len': length(line)})
            dist += spacing
    pts_gdf = gpd.GeoDataFrame(points, crs=lines_gdf.crs)
    logging.info(f"Generated {len(pts_gdf)} points.")
    return pts_gdf


def resolve_conflicts(pts_gdf, lines_gdf, spacing, buffer_dist, max_iter=3):
    """Iteratively prune conflicting points."""
    for iter_n in range(1, max_iter + 1):
        # Find near pairs (like GenerateNearTable)
        joined = gpd.sjoin_nearest(pts_gdf, pts_gdf, distance_col='dist', max_distance=buffer_dist)
        joined = joined[joined.index != joined['index_right']]  # No self-joins
        
        if joined.empty:
            break
        
        # Rank and pick winners/losers (longer parent wins)
        winners, losers = set(), set()
        for _, row in joined.iterrows():
            i, j = row.name, row['index_right']
            len_i, len_j = pts_gdf.loc[i]['parent_len'], pts_gdf.loc[j]['parent_len']
            if len_i > len_j or (len_i == len_j and i < j):
                winners.add(i)
                losers.add(j)
            else:
                winners.add(j)
                losers.add(i)
        
        if not losers:
            break
        
        # Buffer losers and erase from lines
        loser_buffers = pts_gdf.loc[list(losers)].buffer(buffer_dist - 0.01)
        lines_gdf = lines_gdf.difference(loser_buffers.unary_union)
        lines_gdf = lines_gdf[lines_gdf.length >= 3]  # Filter short lines
        
        # Regenerate points
        pts_gdf = generate_points_along_lines(lines_gdf, spacing)
    
    logging.info(f"Resolved after {iter_n} iterations.")
    return pts_gdf, lines_gdf


def main(line_path, out_pts_path, out_lines_path, spacing=25.0, buffer_dist=30.0):
    "Main"
    try:
        lines_gdf = gpd.read_file(line_path)
        pts_gdf = generate_points_along_lines(lines_gdf, spacing)
        final_pts, final_lines = resolve_conflicts(pts_gdf, lines_gdf, spacing, buffer_dist)
        final_pts.to_file(out_pts_path)
        final_lines.to_file(out_lines_path)
        logging.info("Done pruning!")
    except Exception as e:
        logging.error(f"Oops: {e}")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Prune planting points.")
    parser.add_argument('--lines', default='data/sidewalks.gpkg', help='Input lines GPKG')
    parser.add_argument('--out_pts', default='data/plant_points.gpkg', help='Output points')
    parser.add_argument('--out_lines', default='data/pruned_lines.gpkg', help='Output lines')
    parser.add_argument('--spacing', type=float, default=25.0)
    parser.add_argument('--buffer', type=float, default=30.0)
    args = parser.parse_args()
    main(args.lines, args.out_pts, args.out_lines, args.spacing, args.buffer)
```

## src/arcpy/arcpy depracated/arcpy_nostanding.py

```python
import os
import pandas as pd
import arcpy

arcpy.env.overwriteOutput = True
scratch = arcpy.env.scratchGDB

def classify(raw):
    """Return a short code describing the sign text."""
    txt = str(raw).upper()
    if "NO STANDING" in txt:
        return "NSTAND"
    if "NO PARKING" in txt:
        return "NPARK"
    if "HMP" in txt:
        return "HMP"
    if "TAXI" in txt or "HOTEL" in txt or "LOADING" in txt or "PASSENGER" in txt:
        return "CURBSIDE"
    return "OTHER"

def load_filter(csv_path, desc_f, side_f):
    """Return cleaned DataFrame of sign records filtered by text and side."""
    df = pd.read_csv(csv_path)
    
    # normalize N/S/E/W
    df[side_f] = (df[side_f]
                  .astype(str).str.strip()
                  .str.upper().str[0]
                  .where(lambda s: s.isin(list("NSEW"))))
    df = df[df[side_f].notna()]

    # keep only our keywords
    keep = "NO STANDING|NO PARKING|HMP|TAXI|HOTEL|LOADING|PASSENGER"
    df = df[df[desc_f].str.upper().str.contains(keep, na=False)]

    # valid coords
    df = df[pd.to_numeric(df["sign_x_coord"], "coerce").notna()]
    df = df[pd.to_numeric(df["sign_y_coord"], "coerce").notna()]

    # dedupe on 1-ft grid
    df["x_r"] = df["sign_x_coord"].round(1)
    df["y_r"] = df["sign_y_coord"].round(1)
    df = df.drop_duplicates(subset=["x_r", "y_r"]).reset_index(drop=True)

    # parse arrow glyphs
    def _pa(s):
        s = str(s).upper()
        if "<->" in s:
            return "<->"
        elif "-->" in s or "->" in s:
            return "->"
        elif "<--" in s or "<-" in s:
            return "<-"
    df["parsed_arrow"] = df[desc_f].apply(_pa)
    df = df[df["parsed_arrow"].notna()]

    return df

# helper that classifies the segment side
def segment_compass(seg_geom, sign_pt):
    """Return 'north', 'south', 'east', or 'west' relative to sign_pt."""
    dx = seg_geom.centroid.X - sign_pt.centroid.X
    dy = seg_geom.centroid.Y - sign_pt.centroid.Y
    if abs(dx) >= abs(dy):  # horizontal distance dominates or ties
        return "east" if dx > 0 else "west"
    return "north" if dy > 0 else "south"

# ‚îÄ‚îÄ 0. Params ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 0: csv_path, 1: sw_fc, 2: cen_fc, 3: out_fc, 4: desc_f, 5: side_f, 6: shift_ft
csv_path, sw_fc, cen_fc, out_fc, desc_f, side_f, shift_ft = [
    arcpy.GetParameterAsText(i) for i in range(7)
]
shift_ft = float(shift_ft)

# ‚îÄ‚îÄ 0.1 Copy & integrate centerlines (don‚Äôt mutate source)‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
cen_work = os.path.join(scratch, "cen_work")
arcpy.management.CopyFeatures(cen_fc, cen_work)
arcpy.management.Integrate(cen_work, "0.01 Feet")

# ‚îÄ‚îÄ 1.  CSV ‚Üí point feature class  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
df = load_filter(csv_path, desc_f, side_f)
df["sign_type"] = df[desc_f].map(classify)
df["jid"] = df.index
tmp_csv = os.path.join(scratch, "clean_signs.csv")
df.to_csv(tmp_csv, index=False,
          columns=["jid", "sign_x_coord", "sign_y_coord", side_f, "parsed_arrow","sign_type"])
arcpy.AddMessage(f"Cleaned signs CSV ‚Üí {tmp_csv}")

sr = arcpy.SpatialReference(2263)          # NY State Plane Feet
signs = os.path.join(scratch, "signs_pts")
arcpy.management.XYTableToPoint(tmp_csv, signs,
                                "sign_x_coord", "sign_y_coord",
                                coordinate_system=sr)
arcpy.AddMessage(f"Signs ‚Üí point FC: {signs}")

# ‚îÄ‚îÄ 2.  Curb offset  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
signs_shifted = os.path.join(scratch, "signs_shifted")
arcpy.management.CopyFeatures(signs, signs_shifted)

with arcpy.da.UpdateCursor(signs_shifted, ["SHAPE@", side_f]) as cur:
    for shp, sd in cur:
        dx, dy = {"N": (0,  shift_ft),
                  "S": (0, -shift_ft),
                  "E": ( shift_ft, 0),
                  "W": (-shift_ft, 0)}.get((sd or "").upper(), (0, 0))
        pt = shp.centroid
        new_pt = arcpy.PointGeometry(arcpy.Point(pt.X + dx, pt.Y + dy), sr)
        cur.updateRow([new_pt, sd])

arcpy.AddMessage("‚ÜîÔ∏è  Shifted signs off original location.")

# ‚îÄ‚îÄ 3.  Build working sidewalk copy  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
arcpy.env.overwriteOutput = True
sw_work = os.path.join(scratch, "sw_work")
if arcpy.Exists(sw_work):
    arcpy.Delete_management(sw_work)
arcpy.management.CopyFeatures(sw_fc, sw_work)
arcpy.AddMessage(f"Copied sidewalks ‚Üí {sw_work}")

# ‚îÄ‚îÄ 4.  Snap curb‚Äëshifted signs to their sidewalk  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
signs_snapped_sw = os.path.join(scratch, "signs_snapped_sw")
arcpy.management.CopyFeatures(signs_shifted, signs_snapped_sw)

near_sidewalk = os.path.join(scratch, "near_sidewalk")
arcpy.analysis.GenerateNearTable(signs_snapped_sw, sw_work,
                                 near_sidewalk, search_radius="60 Feet",
                                 closest="CLOSEST")

arcpy.management.JoinField(signs_snapped_sw,      # target
                           "OBJECTID",            # sign OID
                           near_sidewalk,         # near table
                           "IN_FID",              # join key
                           ["NEAR_FID"])          # adds curb sidewalk OID
arcpy.AddMessage("üìå  Joined NEAR_FID onto curb‚Äëshifted signs.")

# lookup:  sidewalk OID  ‚Üí  geometry
sw_geom = {oid: g for oid, g in arcpy.da.SearchCursor(sw_work, ["OID@", "SHAPE@"])}

with arcpy.da.UpdateCursor(signs_snapped_sw, ["SHAPE@", "NEAR_FID"]) as cur:
    for shp, nid in cur:
        seg = sw_geom.get(nid)
        if seg:
            meas = seg.measureOnLine(shp.centroid)
            cur.updateRow([seg.positionAlongLine(meas), nid])

arcpy.AddMessage(f"Snapped signs ‚Üí sidewalks: {signs_snapped_sw}")

# ‚îÄ‚îÄ 4.2  Filter out signs that never snapped to a sidewalk ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# A.  signs whose NEAR_FID is NULL  ‚Üí  sign_errors
sign_errors = os.path.join(scratch, "sign_errors")
arcpy.analysis.Select(signs_snapped_sw, sign_errors, "NEAR_FID IS NULL")

# B.  signs we want to ignore (e.g. sign_type = 'NPARK')  ‚Üí  sign_skip
sign_skip = os.path.join(scratch, "sign_skip")
arcpy.analysis.Select(signs_snapped_sw, sign_skip, "sign_type = 'NPARK'")

# C.  delete both kinds from the working sign layer
with arcpy.da.UpdateCursor(signs_snapped_sw,
        ["NEAR_FID", "sign_type"]) as cur:

    for nid, stype in cur:
        if nid is None or stype == "NPARK":
            cur.deleteRow()

cnt_orphan = int(arcpy.management.GetCount(sign_errors)[0])
cnt_skip   = int(arcpy.management.GetCount(sign_skip)[0])

arcpy.AddMessage(f"üõà  {cnt_orphan} orphans ‚Üí sign_errors; "
                 f"{cnt_skip} NPARK signs ‚Üí sign_skip.")

# ------------------------------------------------------------
# 4.  Keep only sidewalks that have at least one matched sign
# ------------------------------------------------------------
# 4‚ÄëA  sid list from NEAR_FID
ids = {oid for oid, in arcpy.da.SearchCursor(signs_snapped_sw, ["NEAR_FID"])
        if oid is not None}

if not ids:
    arcpy.AddWarning("No sidewalks matched any sign within 60‚ÄØft ‚Äî nothing to process.")
    raise SystemExit()

id_sql = f"OBJECTID IN ({','.join(map(str, ids))})"

sw_work_has_sign = os.path.join(scratch, "sw_work_has_sign")
arcpy.analysis.Select(sw_work, sw_work_has_sign, id_sql)

arcpy.AddMessage(f"Sidewalks with signs ‚Üí {sw_work_has_sign} ({len(ids)})")

# join the side code from sidewalk layer onto the sign layer
arcpy.management.JoinField(signs_snapped_sw,       # target
                           "NEAR_FID",             # key on sign
                           sw_work_has_sign,       # source sidewalks
                           "OBJECTID",
                           [side_f])               # e.g. SIDE_CODE

# add a compass field and populate it
arcpy.management.AddField(signs_snapped_sw, "COMPASS", "TEXT", field_length=5)

arrow_to_compass = {
    "E": {"<-": "north", "->": "south"},
    "W": {"<-": "south", "->": "north"},
    "N": {"<-": "west",  "->": "east"},
    "S": {"<-": "east",  "->": "west"},
}

with arcpy.da.UpdateCursor(signs_snapped_sw,
        [side_f, "parsed_arrow", "COMPASS"]) as cur:

    for curb, arr, comp in cur:
        curb = (curb or "").strip().upper()
        arr  = (arr  or "").strip()

        if arr == "<->":
            comp = "both"
        else:
            comp = arrow_to_compass.get(curb, {}).get(arr)

        cur.updateRow([curb, arr, comp])

# ------------------------------------------------------------
# 5.  Erase a ¬±1.5‚ÄØft window around each sign (buffer‚Äëerase)
# ------------------------------------------------------------

# 5‚ÄëA  buffer the signs once
sign_buf = os.path.join(scratch, "sign_buf")
arcpy.analysis.Buffer(signs_snapped_sw, sign_buf, "1.5 Feet",
                      dissolve_option="NONE")

# Erase a ¬±1.5‚ÄØft window around each sign
sw_split_raw = os.path.join(scratch, "sw_split_raw")
arcpy.analysis.PairwiseErase(sw_work_has_sign, sign_buf, sw_split_raw)
arcpy.AddMessage(f"Erased 3‚ÄØft gaps at signs ‚Üí {sw_split_raw}")

# explode multipart to singlepart
sw_split = os.path.join(scratch, "sw_split")              # final working copy
arcpy.management.MultipartToSinglepart(sw_split_raw, sw_split)

# drop zero‚Äëlength slivers
sliver_sql = "SHAPE_Length < 0.05"          # adjust threshold if needed
arcpy.management.MakeFeatureLayer(sw_split, "split_lyr", sliver_sql)
if int(arcpy.management.GetCount("split_lyr")[0]) > 0:
    arcpy.management.DeleteFeatures("split_lyr")
arcpy.management.Delete("split_lyr")

# ------------------------------------------------------------
# 6.  Flag before / after / both via COMPASS logic
# ------------------------------------------------------------

# --- sign geometry lookup (needed inside the cursor) ----------
sign_geom = {oid: geom for oid, geom
             in arcpy.da.SearchCursor(signs_snapped_sw,
                                      ["OBJECTID", "SHAPE@"])}

# 1) ONE‚ÄëTO‚ÄëMANY spatial join: every segment ‚Üî every sign ‚â§ 2‚ÄØft
sw_join = os.path.join(scratch, "sw_split_joined")
arcpy.analysis.SpatialJoin(
    target_features   = sw_split,          # exploded single‚Äëpart segments
    join_features     = signs_snapped_sw,  # snapped signs (with COMPASS)
    out_feature_class = sw_join,
    join_operation    = "JOIN_ONE_TO_MANY",
    match_option      = "WITHIN_A_DISTANCE",
    search_radius     = "2 Feet"
)

# rename join keys for clarity
arcpy.management.AlterField(sw_join, "TARGET_FID", new_field_name="SEG_ID")
arcpy.management.AlterField(sw_join, "JOIN_FID",   new_field_name="SIGN_OID")
arcpy.management.JoinField(sw_join,          # target = sw_join rows
                           "SIGN_OID",       # key in sw_join
                           signs_snapped_sw, # source signs
                           "OBJECTID",       # key in signs
                           ["sign_type"])    # field(s) to copy

# add the flag field *before* the cursor
arcpy.management.AddField(sw_join, "no_stand", "SHORT")

# 2) row‚Äëby‚Äërow flag
with arcpy.da.UpdateCursor(
        sw_join,
        ["SEG_ID", "SIGN_OID", "COMPASS", "no_stand", "SHAPE@"]) as cur:

    for seg_id, soid, comp, flag, seg in cur:
        if comp == "both":
            flag = 1
        else:
            sign_pt  = sign_geom.get(soid)
            seg_side = segment_compass(seg, sign_pt)
            flag     = int(seg_side == comp) if seg_side else 0
        cur.updateRow([seg_id, soid, comp, flag, seg])

# 3) collapse duplicates: MAX(no_stand) per segment
stat_tbl = os.path.join(scratch, "seg_flag_stat")
arcpy.analysis.Statistics(
        sw_join, stat_tbl,
        [["no_stand", "MAX"]],
        case_field="SEG_ID")

arcpy.management.AlterField(stat_tbl, "MAX_no_stand",
                            new_field_name="no_stand")

# choose "MIN_SIGN_TYPE" = the alphabetically first type for that segment
type_tbl = os.path.join(scratch, "seg_type_stat")
arcpy.analysis.Statistics(
    sw_join, type_tbl,
    [["sign_type", "MIN"]],          # MIN, MAX, or COUNT
    case_field="SEG_ID")

arcpy.management.AlterField(type_tbl, "MIN_sign_type",
                            new_field_name="sign_type")

# join both flag + type back onto sw_split
arcpy.management.JoinField(sw_split, "OBJECTID",
                           stat_tbl,  "SEG_ID", ["no_stand"])
arcpy.management.JoinField(sw_split, "OBJECTID",
                           type_tbl,  "SEG_ID", ["sign_type"])

# 4) join the final flag back to the single‚Äëpart sidewalk layer
#    remove any placeholder no_stand first
if "no_stand" in [f.name for f in arcpy.ListFields(sw_split)]:
    arcpy.management.DeleteField(sw_split, ["no_stand"])

arcpy.management.JoinField(sw_split,          # target = segments
                           "OBJECTID",        # key in sw_split
                           stat_tbl,          # source table
                           "SEG_ID",          # key in stats table
                           ["no_stand"])

arcpy.AddMessage("‚úÖ  Segments flagged via COMPASS logic")

# ------------------------------------------------------------
# 8.  Clean up & export
# ------------------------------------------------------------
arcpy.management.CopyFeatures(sw_split, out_fc)     # overwriteOutput should be True
arcpy.AddMessage(f"üéâ  Final no‚Äëstanding layer ‚Üí {out_fc}")
```

## src/arcpy/arcpy depracated/arcpy_rank_dominant_working.py

```python
import arcpy
from pathlib import Path
import pandas as pd
from collections import defaultdict

arcpy.ImportToolbox(r"D:\ArcGIS\Projects\Street_Tree_Planting_Analysis\Street_Tree_Planting_Analysis.atbx", "stp")

def generate_initial_points(line_fc, spacing, work_gdb):
    """Copy input lines, then create and rank an initial set of points."""
    lines_working = str(work_gdb / "lines_working")
    if arcpy.Exists(lines_working):
        arcpy.management.Delete(lines_working)
    
    arcpy.management.CopyFeatures(str(line_fc), lines_working)

    pts = str(work_gdb / "pts_0")
    arcpy.stp.generatepoints(lines_working, str(spacing), pts)
    arcpy.stp.addrankfields(pts, lines_working)

    return lines_working, pts

def identify_non_conflicting_points(pts, spacing, work_gdb):
    """Remove points that intersect within 'spacing' feet of each other."""
    pts_work = str(work_gdb / "pts_0_working")
    arcpy.management.CopyFeatures(pts, pts_work)

    conflicts_fc = str(work_gdb / "potential_conflicts")
    arcpy.analysis.SpatialJoin(
        target_features=pts_work,
        join_features=pts_work,
        out_feature_class=conflicts_fc,
        join_operation="JOIN_ONE_TO_MANY",
        match_option="INTERSECT",
        search_radius=f"{spacing} Feet"
    )

    # Filter out self-joins so only overlapping points remain

    true_conflicts = str(work_gdb / "true_conflicts")
    arcpy.management.MakeFeatureLayer(conflicts_fc, "conflicts_lyr")
    arcpy.management.SelectLayerByAttribute("conflicts_lyr", "NEW_SELECTION", '"PARENT_OID" <> "PARENT_OID_1"')
    arcpy.management.CopyFeatures("conflicts_lyr", true_conflicts)

    conflict_ids = {row[0] for row in arcpy.da.SearchCursor(true_conflicts, ["TARGET_FID"])}

    # Remove points whose OID appears in the conflict table
    good_boys_fc = str(work_gdb / "pts_0_cleaned")
    with arcpy.da.UpdateCursor(pts_work, ["OBJECTID"]) as cursor:
        for row in cursor:
            if row[0] in conflict_ids:
                cursor.deleteRow()

    arcpy.management.CopyFeatures(pts_work, good_boys_fc)
    return good_boys_fc

def resolve_conflicts_iteratively(pts, lines_working, spacing, buffer_dist, work_gdb, max_iterations=3):
    """Iteratively buffer and erase lines to remove conflicting planting points."""
    iter_n = 0
    line_suppress_map = defaultdict(list)

    while True:
        iter_n += 1
        if iter_n > max_iterations:
            arcpy.AddWarning("Max iterations reached.")
            break
        
        line_suppress_map = defaultdict(list)

        near_tbl = str(work_gdb / f"near_{iter_n}")
        arcpy.analysis.GenerateNearTable(
            pts, pts, near_tbl, f"{buffer_dist} Feet", "NO_LOCATION",
            closest="ALL", method="PLANAR"
        )  # produces pairwise distances between points

        if int(arcpy.management.GetCount(near_tbl)[0]) == 0:
            break

        df = pd.DataFrame(arcpy.da.TableToNumPyArray(near_tbl, ["IN_FID", "NEAR_FID"]))
        df = df[df.IN_FID < df.NEAR_FID]

        rank = {}
        with arcpy.da.SearchCursor(pts, ["OID@", "PARENT_LEN", "PARENT_FID"]) as cur:
            for oid, ln_len, ln_fid in cur:
                rank[oid] = (ln_len, ln_fid)

        winners, losers = set(), set()
        for _, row in df.iterrows():
            i, j = int(row.IN_FID), int(row.NEAR_FID)
            if i not in rank or j not in rank or rank[i][1] == rank[j][1]:
                continue
            r_i, r_j = rank[i][0], rank[j][0]
            if r_i > r_j or (r_i == r_j and i < j):
                winners.add(i)
                losers.add(j)
                line_suppress_map[rank[j][1]].append(i)
            else:
                winners.add(j)
                losers.add(i)
                line_suppress_map[rank[i][1]].append(j)

        if not winners:
            break

        # Points to buffer are the losers grouped by parent line
        suppression_pts = list({oid for pts in line_suppress_map.values() for oid in pts})
        existing_oids = {row[0] for row in arcpy.da.SearchCursor(pts, ["OID@"])}
        suppression_pts = [oid for oid in suppression_pts if oid in existing_oids]

        if not suppression_pts:
            continue

        suppression_pt_layer = arcpy.management.MakeFeatureLayer(pts, "suppression_pts")[0]
        oid_field = arcpy.Describe(suppression_pt_layer).OIDFieldName
        arcpy.management.SelectLayerByAttribute(
            suppression_pt_layer, "NEW_SELECTION",
            f"{oid_field} IN ({','.join(map(str, suppression_pts))})"
        )

        # Defensive check before buffering
        count = int(arcpy.management.GetCount(suppression_pt_layer)[0])
        arcpy.AddMessage(f"üß™ Suppression point layer count: {count}")
        if count == 0:
            arcpy.AddMessage("‚ö†Ô∏è No suppression points selected ‚Äî skipping regeneration.")
            continue  # skip this iteration

        # Now it's safe to buffer
        buffer_fc = work_gdb / f"iter_{iter_n}_winner_buffers"
        arcpy.analysis.PairwiseBuffer(suppression_pt_layer, str(buffer_fc), f"{buffer_dist - 0.01} Feet", dissolve_option="ALL")

        losing_lines = list(line_suppress_map.keys())
        losing_line_layer = arcpy.management.MakeFeatureLayer(lines_working, "losing_losing_line_layer")[0]
        oid_field = arcpy.Describe(losing_line_layer).OIDFieldName
        arcpy.management.SelectLayerByAttribute(
            losing_line_layer, "NEW_SELECTION", f"{oid_field} IN ({','.join(map(str, losing_lines))})"
        )

        arcpy.AddMessage(f"üß™ Trying to erase these lines: {losing_lines[:5]}... (total: {len(losing_lines)})")

        count = int(arcpy.management.GetCount(str(losing_line_layer))[0])
        arcpy.AddMessage(f"üìä Selected lines for erase: {count}")
        if count == 0:
            arcpy.AddWarning("‚ö†Ô∏è No losing lines selected ‚Äî skipping erase this round.")
            continue

        # Optional geometry repair
        arcpy.management.RepairGeometry(str(buffer_fc), "DELETE_NULL")

        # Run erase
        trimmed_losing_lines = work_gdb / f"iter_{iter_n}_trimmed_losing_lines"
        arcpy.analysis.PairwiseErase(losing_line_layer, str(buffer_fc), str(trimmed_losing_lines))

        # Select survivor lines (NOT in losing_lines)
        survivor_lines = work_gdb / f"iter_{iter_n}_survivor_lines"
        arcpy.management.MakeFeatureLayer(lines_working, "lines_working_lyr")
        arcpy.management.SelectLayerByAttribute(
            "lines_working_lyr", "NEW_SELECTION",
            f"{oid_field} NOT IN ({','.join(map(str, losing_lines))})"
        )
        arcpy.management.CopyFeatures("lines_working_lyr", str(survivor_lines))

        # Merge survivors + trimmed losers
        merged_lines = work_gdb / f"iter_{iter_n}_merged_lines"
        arcpy.management.Merge([str(survivor_lines), str(trimmed_losing_lines)], str(merged_lines))

        # Filter out final lines under X feet before generating points
        min_line_length = 3
        lines_filtered = str(work_gdb / f"iter_{iter_n}_filtered_lines")

        # Create in-memory layer from merged result
        filtered_layer = arcpy.management.MakeFeatureLayer(str(merged_lines), f"merged_lines_lyr_{iter_n}")[0]

        # Select only lines that meet minimum length
        arcpy.management.SelectLayerByAttribute(
            filtered_layer, "NEW_SELECTION",
            f"Shape_Length >= {min_line_length}"
        )

        # Save filtered version to new feature class
        arcpy.management.CopyFeatures(filtered_layer, lines_filtered)

        # Set for next round
        lines_working = lines_filtered

        # Step: Regenerate points
        pts = str(work_gdb / f"pts_{iter_n}_regenerated")
        arcpy.stp.generatepoints(lines_working, str(spacing), pts)
        arcpy.stp.addrankfields(pts, lines_working)

    return None, None, iter_n, lines_working

def rank_dominant_prune(line_fc: str, out_pts_fc: str, final_lines_fc: str, spacing: float, buffer_dist: float) -> None:
    """High level prune workflow used by the ArcGIS tool."""
    work_gdb = Path(arcpy.env.scratchGDB)
    arcpy.env.workspace = str(work_gdb)
    arcpy.env.overwriteOutput = True

    # Step 1: Generate working copy of input lines and initial points
    lines_working, pts = generate_initial_points(line_fc, spacing, work_gdb)

    # Step 2: Run all pruning ‚Äî this mutates lines_working
    _, _, _, final_lines = resolve_conflicts_iteratively(
        pts, lines_working, spacing, buffer_dist, work_gdb
    )

    # Step 3: Generate final planting points from clean geometry
    arcpy.stp.generatepoints(str(final_lines), str(spacing), out_pts_fc)
    arcpy.AddMessage(f"üå≥ Final planting points generated from trimmed geometry: {out_pts_fc}")

    # Step 4: Generate final plantable sidewalk from clean geometry
    arcpy.management.CopyFeatures(final_lines, final_lines_fc)
    arcpy.AddMessage(f"üóÇ Final pruned sidewalk geometry saved to: {final_lines_fc}")

# ArcGIS Tool Entry
if __name__ == "__main__":
    line_fc = arcpy.GetParameterAsText(0)
    out_pts_fc = arcpy.GetParameterAsText(1)
    final_lines_fc = arcpy.GetParameterAsText(2)
    spacing = float(arcpy.GetParameterAsText(3))
    buffer_dist = float(arcpy.GetParameterAsText(4))

    rank_dominant_prune(line_fc, out_pts_fc, final_lines_fc, spacing, buffer_dist)
```

## src/arcpy/arcpy depracated/arcpy_Second_Step_Alternative.py

```python
# -*- coding: utf-8 -*-
"""
Generated by ArcGIS ModelBuilder on : 2025-06-10 11:33:01
"""
import arcpy

def SecondStepAlternative():  # Second_Step_Alternative
    """ArcPy model step generating alternative curb buffers."""

    # To allow overwriting outputs change overwriteOutput option to True.
    arcpy.env.overwriteOutput = False

    # Model Environment settings
    with arcpy.EnvManager(scratchWorkspace="D:\\ArcGIS\\Projects\\Street_Tree_Planting_Analysis\\Scrap.gdb"):
        SIDEWALK_3_ = "D:\\ArcGIS\\Data\\FileGDB-data-Planimetric_2022_AGOL_Link.gdb\\SIDEWALK"
        PLUTO = "D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped"
        Field_Map = "Borough \"Borough\" true false false 2 Text 0 0,First,#,D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped,Borough,0,1;Block \"Block\" true false false 4 Long 0 0,First,#,D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped,Block,-1,-1;Lot \"Lot\" true false false 2 Short 0 0,First,#,D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped,Lot,-1,-1;CD \"CD\" true true false 2 Short 0 0,First,#,D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped,CD,-1,-1"

        # Process: Copy Pluto (Export Features) (conversion)
        PLUTO_Copy = fr"{arcpy.env.scratchGDB}\Pluto_Copy"
        arcpy.conversion.ExportFeatures(in_features=PLUTO, out_features=PLUTO_Copy, field_mapping=Field_Map)

        # Process: Calculate Unique Blocks (Calculate Field) (management)
        PLUTO_Unique_Blocks = arcpy.management.CalculateField(in_table=PLUTO_Copy, field="Unique_Blocks", expression="Concatenate($feature.Borough,$feature.Block)", expression_type="ARCADE")[0]

        # Process: Pairwise Dissolve (Pairwise Dissolve) (analysis)
        PLUTO_Dissolve = fr"{arcpy.env.scratchGDB}\PLUTO_Dissolve"
        arcpy.analysis.PairwiseDissolve(in_features=PLUTO_Unique_Blocks, out_feature_class=PLUTO_Dissolve, dissolve_field=["Unique_Blocks"], multi_part="SINGLE_PART")

        # Process: Sidewalk Erase (Pairwise Erase) (analysis)
        Output_Feature_Class = fr"{arcpy.env.scratchGDB}\SIDEWALK_Erase"
        arcpy.analysis.PairwiseErase(in_features=SIDEWALK_3_, erase_features=PLUTO_Dissolve, out_feature_class=Output_Feature_Class)

        # Process: Collapse Hydro Polygon (Collapse Hydro Polygon) (cartography)
        Sidewalk_Collapse = fr"{arcpy.env.scratchGDB}\Sidewalk_Collapse"
        Output_Polygon_Feature_Class = ""
        Sidewalk_Collapse_InPoly_DecodeID, Sidewalk_Collapse_InLine_DecodeID = arcpy.cartography.CollapseHydroPolygon(in_features=[Output_Feature_Class], out_line_feature_class=Sidewalk_Collapse, out_poly_feature_class=Output_Polygon_Feature_Class)

        # Process: Multipart To Singlepart (Multipart To Singlepart) (management)
        PLUTO_Singlepart = fr"{arcpy.env.scratchGDB}\PLUTO_Singlepart"
        arcpy.management.MultipartToSinglepart(in_features=PLUTO_Dissolve, out_feature_class=PLUTO_Singlepart)

        # Process: Add Unique Block Parts (Add Field) (management)
        PLUTO_Unique = arcpy.management.AddField(in_table=PLUTO_Singlepart, field_name="Unique_Block_Parts", field_type="TEXT")[0]

        # Process: Calculate Unique PLUTO Blocks (Calculate Field) (management)
        Pluto_Blocks_2_ = arcpy.management.CalculateField(in_table=PLUTO_Unique, field="Unique_Block_Parts", expression="str(!Unique_Blocks!)+str(!OBJECTID!)")[0]

        # Process: Pluto Blocks Copy (Copy Features) (management)
        Pluto_Working = "D:\\ArcGIS\\Projects\\Street_Tree_Planting_Analysis\\Scrap.gdb\\Pluto_Blocks"
        arcpy.management.CopyFeatures(in_features=Pluto_Blocks_2_, out_feature_class=Pluto_Working)

        # Process: Pluto + Buffer (Pairwise Buffer) (analysis)
        Pluto_Buffer' = fr"{arcpy.env.scratchGDB}\Pluto_Buffer"
        arcpy.analysis.PairwiseBuffer(in_features=Pluto_Blocks_2_, out_feature_class=Pluto_Buffer', buffer_distance_or_field="25 Feet")

        # Process: Pairwise Erase (Pairwise Erase) (analysis)
        Pluto_Prep = fr"{arcpy.env.scratchGDB}\Pluto_Buffer_Topology"
        arcpy.analysis.PairwiseErase(in_features=Pluto_Buffer', erase_features=PLUTO_Dissolve, out_feature_class=Pluto_Prep)

        # Process: Pairwise Intersect (Pairwise Intersect) (analysis)
        Sidewalk_Pluto = "D:\\ArcGIS\\Projects\\Street_Tree_Planting_Analysis\\Street_Tree_Planting_Analysis.gdb\\Sidewalk_Pluto"
        arcpy.analysis.PairwiseIntersect(in_features=[Sidewalk_Collapse, Pluto_Prep], out_feature_class=Sidewalk_Pluto)

        # Process: Delete Field (2) (Delete Field) (management)
        Sidewalk_Pluto_3_ = arcpy.management.DeleteField(in_table=Sidewalk_Pluto, drop_field=["FID_Sidewalk_Collapse", "InPoly_ID", "InPoly_FID", "InLine_ID", "InLine_FID", "COLLAPSED", "FID_Pluto_Buffer_Topology_Erase_Clean", "ORIG_FID"])[0]

if __name__ == '__main__':
    # Global Environment settings
    with arcpy.EnvManager(autoCommit=1000, baUseDetailedAggregation=False, cellAlignment="DEFAULT", 
                          cellSize="MAXOF", cellSizeProjectionMethod="CONVERT_UNITS", coincidentPoints="MEAN", 
                          compression="LZ77", maintainAttachments=True, maintainSpatialIndex=False, 
                          matchMultidimensionalVariable=True, nodata="NONE", outputMFlag="Same As Input", 
                          outputZFlag="Same As Input", preserveGlobalIds=False, pyramid="PYRAMIDS -1 NEAREST DEFAULT 75 NO_SKIP NO_SIPS", 
                          qualifiedFieldNames=True, randomGenerator="0 ACM599", rasterStatistics="STATISTICS 1 1", 
                          resamplingMethod="NEAREST", terrainMemoryUsage=False, tileSize="128 128", 
                          tinSaveVersion="CURRENT", transferDomains=False, transferGDBAttributeProperties=False, 
                          unionDimension=False, workspace="D:\\ArcGIS\\Projects\\Street_Tree_Planting_Analysis\\Scrap.gdb"):
        SecondStepAlternative()
```

## src/stp/cli/stp_pipeline.py

```python
# -*- coding: utf-8 -*-
"""
Entry point for the Street Tree Planting (stp) pipeline.
Follows the primary scope defined in the README:

1. Read established parameters
2. Download files from ArcGIS/NYC OpenData
3. Convert JSON into GeoJSON features
4. Clean files of unnecessary fields
5. Apply buffers, filters, custom scripts
6. Create mutable and immutable sidewalk polylines
7. Merge all polygons where plantings cannot occur
8. Clip do-not-plant locations against sidewalk polyline
9. Process traffic signs, parking rules, MTA no-bus zones
10. Generate potential planting locations
11. Join planting locations with sidewalk info
12. Finish and export results
"""

import argparse
import logging
import sys


def parse_args():  # noqa: D103
    """
    Parse command-line arguments for pipeline parameters.

    Returns:
        argparse.Namespace: Parsed arguments
    """
    parser = argparse.ArgumentParser(
        description="Run the STP pipeline with user parameters"
    )
    parser.add_argument(
        "--config", required=True, help="Path to config YAML file"
    )
    return parser.parse_args()


def load_parameters(config_path):  # noqa: D103
    """
    Load pipeline parameters from a YAML config file.

    Args:
        config_path (str): Path to config file

    Returns:
        dict: Parameters dictionary
    """
    import yaml

    with open(config_path) as fh:
        params = yaml.safe_load(fh)
    return params


def download_sources(params):  # noqa: D103
    """
    Download source datasets from ArcGIS/NYC OpenData.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement download logic
    pass


def convert_to_geojson(params):  # noqa: D103
    """
    Convert downloaded JSON to GeoJSON point/polygon files.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement conversion
    pass


def clean_datasets(params):  # noqa: D103
    """
    Remove unnecessary fields from datasets.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement cleaning
    pass


def apply_spatial_ops(params):  # noqa: D103
    """
    Apply buffers, filters, and custom scripts.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement spatial operations
    pass


def build_sidewalk_polylines(params):  # noqa: D103
    """
    Create mutable and immutable sidewalk polylines.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement polyline creation
    pass


def merge_no_plant_zones(params):  # noqa: D103
    """
    Merge polygons where plantings cannot occur.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement merge logic
    pass


def clip_sidewalk(params):  # noqa: D103
    """
    Clip do-not-plant zones with sidewalk polyline.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement clipping
    pass


def process_parking_and_signs(params):  # noqa: D103
    """
    Build parking zones and classify rules, and process MTA zones.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement parking and sign logic
    pass


def generate_planting_locations(params):  # noqa: D103
    """
    Generate potential planting locations within allowed areas.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement location generation
    pass


def join_and_export(params):  # noqa: D103
    """
    Join planting points with sidewalk attributes and export.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement join and export
    pass


def main():  # noqa: D103
    """
    Main function orchestrating the pipeline steps.
    """
    args = parse_args()
    params = load_parameters(args.config)

    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
        stream=sys.stdout,
    )

    logging.info("Starting STP pipeline")
    download_sources(params)
    convert_to_geojson(params)
    clean_datasets(params)
    apply_spatial_ops(params)
    build_sidewalk_polylines(params)
    merge_no_plant_zones(params)
    clip_sidewalk(params)
    process_parking_and_signs(params)
    generate_planting_locations(params)
    join_and_export(params)
    logging.info("STP pipeline completed successfully")


if __name__ == "__main__":  # noqa: G004
    main()
```

## src/stp/core/config.py

```python
"""
Config loader for STP.

Loads YAML defaults, optional user overrides, and environment variables
with deep-merge logic and caching via lru_cache.
Provides get_setting() and get_constant() for dot-path access.
Also loads and validates workflow.yaml for pipeline orchestration.
"""

from __future__ import annotations

import os
import logging
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict

import yaml
from dotenv import load_dotenv

logging.basicConfig(level=logging.INFO)

# Load environment variables from .env into os.environ (if .env exists)
load_dotenv()


def load_user_config() -> Dict[str, Any]:
    """Load critical overrides from environment (.env or system env)."""
    # Pull API keys and database creds directly from environment variables
    return {
        "api_key": os.getenv("API_KEY"),
        "db_user": os.getenv("DB_USER"),
        "db_pass": os.getenv("DB_PASS"),
    }


def _deep_get(mapping: Dict[str, Any], keys: list[str]) -> Any:
    """Walk a nested dict by a list of keys; return None if any key is missing."""
    current: Any = mapping
    for key in keys:
        if not isinstance(current, dict):
            # If current level isn't a dict, path is invalid
            return None
        current = current.get(key)
    return current


def _merge(base: Dict[str, Any], update: Dict[str, Any]) -> Dict[str, Any]:
    """Recursively deep-merge two dicts: nested dicts merge, others overwrite."""
    # Create a shallow copy so we don't mutate the original base dict
    result = dict(base)
    for key, val in update.items():
        # If both base and update have dicts at this key, merge them recursively
        if key in result and isinstance(result[key], dict) and isinstance(val, dict):
            result[key] = _merge(result[key], val)
        else:
            # Otherwise, the update value replaces or adds to the result
            result[key] = val
    return result


@lru_cache(maxsize=1)
def _load_defaults() -> Dict[str, Any]:
    """Load defaults.yaml once and cache it for fast subsequent access."""
    # Locate the repo root relative to this file
    root = Path(__file__).resolve().parents[2]
    path = root / "config" / "defaults.yaml"
    with open(path, encoding="utf-8") as f:
        # Safe-load YAML; return empty dict if file is empty
        return yaml.safe_load(f) or {}


@lru_cache(maxsize=1)
def _load_overrides() -> Dict[str, Any]:
    """Load user.yaml overrides once and cache; return empty dict if absent."""
    root = Path(__file__).resolve().parents[2]
    path = root / "config" / "user.yaml"
    if path.exists():
        with open(path, encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    # No overrides file means no override entries
    return {}


@lru_cache(maxsize=1)
def load_config() -> Dict[str, Any]:
    """Merge defaults and user overrides into one config dict (cached)."""
    defaults = _load_defaults()
    overrides = _load_overrides()
    # Combine with override taking precedence
    return _merge(defaults, overrides)


def get_setting(
    key: str,
    default: Any | None = None,
    required: bool = False,
    env_override: bool = True,
) -> Any:
    """
    Retrieve a config value using dot-path lookup with this precedence:
      1. Environment variable (if env_override=True)
      2. user.yaml overrides
      3. defaults.yaml
      4. provided default arg

    Raises if 'required' is True and resulting value is missing or placeholder.
    """
    # 1) Check environment variables first (e.g., 'DB_HOST' for 'db.host')
    if env_override:
        env_key = key.upper().replace('.', '_')
        if (val := os.getenv(env_key)) is not None:
            return val

    # 2) Load merged config and attempt lookup in overrides then defaults
    keys = key.split('.')
    value = _deep_get(_load_overrides(), keys)
    if value is None:
        value = _deep_get(_load_defaults(), keys)
    if value is None:
        # 3) Fall back to function default if still missing
        value = default

    # 4) If marked required but missing or placeholder, error out
    if required and (value is None or value == "REPLACE_ME"):
        raise RuntimeError(f"Missing required setting: {key}")
    return value


def get_constant(key: str, default: Any | None = None) -> Any:
    """Fetch a constant from defaults.yaml only, ignoring user overrides."""
    keys = key.split('.')
    value = _deep_get(_load_defaults(), keys)
    # If not found, use provided default
    return default if value is None else value


@lru_cache(maxsize=1)
def load_workflow(path: str = 'config/workflow.yaml') -> Dict[str, Any]:
    """
    Parse workflow.yaml, merge with config globals (e.g., from defaults.yaml),
    and perform basic validation. This drives the pipeline orchestrator.
    Cached for efficiency like load_config().
    """
    root = Path(__file__).resolve().parents[2]
    full_path = root / path
    with open(full_path, 'r', encoding='utf-8') as f:
        workflow = yaml.safe_load(f)

    # Merge globals from the main config (e.g., EPSG defaults, limits)
    config = load_config()
    workflow['globals'] = config  # Merge the full config for broader access

    # Basic validation to catch errors early (expand as needed)
    required_sections = ['sources', 'prep_ops']  # Add 'final_clip', 'big_scripts' if always needed
    for section in required_sections:
        if section not in workflow:
            raise ValueError(f"workflow.yaml missing '{section}' section")
    for dataset_id, dataset in workflow['sources'].items():
        if 'url' not in dataset or 'format' not in dataset:
            logging.warning("Source %s missing 'url' or 'format'", dataset_id)

    logging.info("Parsed and merged workflow.yaml with config globals")
    return workflow


__all__ = [
    "load_user_config",
    "load_config",
    "get_setting",
    "get_constant",
    "load_workflow"
    ]
```

## src/stp/core/http.py

```python
"""Simple HTTP client helpers."""

from typing import Optional

import requests

_session = requests.Session()


def fetch_bytes(url: str, session: Optional[requests.Session] = None) -> bytes:
    """Return response content for GET request."""
    sess = session or _session
    resp = sess.get(url)
    resp.raise_for_status()
    return resp.content
```

## src/stp/core/settings.py

```python
"""Global constants for the STP package, pulled from config."""

from config import get_constant

DEFAULT_EPSG = get_constant('epsg.default', 4326)
NYSP_EPSG = get_constant('epsg.nysp', 2263)

# TODO: All done! No more hardcodes‚Äîconstants now come from defaults.yaml.
# Remove this file later if you want to access directly via get_constant everywhere.
```

## src/stp/fetch/arcgis.py

```python
"""Fetchers for ArcGIS REST services."""

from __future__ import annotations

from io import BytesIO
from pathlib import Path
from typing import List, Tuple

import geopandas as gpd

from .. import http_client
from ..storage.file_storage import sanitize_layer_name
from ..settings import DEFAULT_EPSG

__all__ = ["fetch_arcgis_vector", "fetch_arcgis_table"]


def _build_query_url(service_url: str, as_geojson: bool = True) -> str:
    """Return an ArcGIS REST query URL for *service_url*."""
    base = service_url.rstrip("/")
    if not base.lower().endswith("query"):
        base = f"{base}/query"
    params = "where=1%%3D1&outFields=*&returnGeometry=true"
    if as_geojson:
        params += "&outSR=4326&f=geojson"
    else:
        params += "&f=json"
    return f"{base}?{params}"


def fetch_arcgis_vector(
    service_url: str,
) -> List[Tuple[str, gpd.GeoDataFrame, int, int]]:
    """Fetch vector data from an ArcGIS FeatureServer layer."""
    url = _build_query_url(service_url, as_geojson=True)
    data = http_client.fetch_bytes(url)
    gdf = gpd.read_file(BytesIO(data))
    epsg = gdf.crs.to_epsg() or DEFAULT_EPSG
    layer_name = sanitize_layer_name(Path(service_url).stem)
    return [(layer_name, gdf, epsg, 4326)]


def fetch_arcgis_table(
    service_url: str,
) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Fetch a non-spatial table from an ArcGIS service."""
    url = _build_query_url(service_url, as_geojson=True)
    data = http_client.fetch_bytes(url)
    gdf = gpd.read_file(BytesIO(data))
    gdf.set_crs(epsg=DEFAULT_EPSG, inplace=True)
    layer_name = sanitize_layer_name(Path(service_url).stem)
    return [(layer_name, gdf, DEFAULT_EPSG)]
```

## src/stp/fetch/csv.py

```python
"""CSV direct download helper."""

from io import BytesIO
from pathlib import Path
from typing import List, Tuple
import logging

import geopandas as gpd
import pandas as pd
from pandas.errors import ParserError
from shapely.geometry import Point

from ..core import http
from ..storage.file_storage import sanitize_layer_name
from ..core import DEFAULT_EPSG #TODO use config.py to get default values

logger = logging.getLogger(__name__)


def fetch_csv_direct(url: str) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Download and parse a CSV URL."""
    data = http.fetch_bytes(url)
    try:
        df = pd.read_csv(BytesIO(data))
    except ParserError as err:
        logger.warning("CSV parse failed for %s: %s", url, err)
        return []
    if "latitude" not in df.columns or "longitude" not in df.columns:
        return []
    geometry = [Point(xy) for xy in zip(df.longitude, df.latitude)]
    gdf = gpd.GeoDataFrame(
        df.drop(columns=["latitude", "longitude"]),
        geometry=geometry,
        crs=f"EPSG:{DEFAULT_EPSG}",
    )
    layer_name = sanitize_layer_name(Path(url).stem)
    return [(layer_name, gdf, DEFAULT_EPSG)]
```

## src/stp/fetch/download.py

```python
"""Dispatcher for direct dataset downloads."""

from typing import List, Tuple

import geopandas as gpd

from .geojson import fetch_geojson_direct
from .csv import fetch_csv_direct


def fetch_direct(url: str) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Fetch data from *url* using the appropriate fetcher."""
    lower = url.lower()
    if lower.endswith((".geojson", ".json")):
        return fetch_geojson_direct(url)
    if lower.endswith(".csv"):
        return fetch_csv_direct(url)
    raise ValueError(f"Unsupported format: {url}")
```

## src/stp/fetch/gdb.py

```python
"""Fetchers for zipped shapefiles or geodatabases."""

from __future__ import annotations

from pathlib import Path
from typing import List, Tuple
from tempfile import TemporaryDirectory
import zipfile

import fiona
import geopandas as gpd

from .. import http_client
from ..storage.file_storage import sanitize_layer_name
from ..settings import DEFAULT_EPSG

__all__ = ["fetch_gdb_or_zip"]


def fetch_gdb_or_zip(url: str) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Download a zipped archive and extract layers."""
    data = http_client.fetch_bytes(url)
    results: List[Tuple[str, gpd.GeoDataFrame, int]] = []
    with TemporaryDirectory() as tmpdir:
        zip_path = Path(tmpdir) / "data.zip"
        with open(zip_path, "wb") as fh:
            fh.write(data)
        with zipfile.ZipFile(zip_path, "r") as zf:
            zf.extractall(tmpdir)
        for shp in Path(tmpdir).rglob("*.shp"):
            gdf = gpd.read_file(shp)
            epsg = gdf.crs.to_epsg() or DEFAULT_EPSG
            results.append((sanitize_layer_name(shp.stem), gdf, epsg))
        for gdb in Path(tmpdir).rglob("*.gdb"):
            for layer in fiona.listlayers(str(gdb)):
                gdf = gpd.read_file(gdb, layer=layer)
                epsg = gdf.crs.to_epsg() or DEFAULT_EPSG
                results.append((sanitize_layer_name(layer), gdf, epsg))
    return results
```

## src/stp/fetch/geojson.py

```python
"""GeoJSON direct download helper."""

from io import BytesIO
from pathlib import Path
from typing import List, Tuple
import logging

import geopandas as gpd
from fiona.errors import DriverError, FionaValueError

from .. import http_client
from ..storage.file_storage import sanitize_layer_name
from ..settings import DEFAULT_EPSG

logger = logging.getLogger(__name__)


def fetch_geojson_direct(url: str) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Download and parse a GeoJSON URL."""
    data = http_client.fetch_bytes(url)
    try:
        gdf = gpd.read_file(BytesIO(data))
    except (FionaValueError, DriverError) as err:
        logger.warning("GeoJSON read failed for %s: %s", url, err)
        return []
    gdf.set_crs(epsg=DEFAULT_EPSG, inplace=True)
    layer_name = sanitize_layer_name(Path(url).stem)
    return [(layer_name, gdf, DEFAULT_EPSG)]
```

## src/stp/fetch/gpkg.py

```python
"""Fetch layers from a GeoPackage."""

from __future__ import annotations

from pathlib import Path
from typing import List, Tuple
from tempfile import TemporaryDirectory

import fiona
import geopandas as gpd

from .. import http_client
from ..storage.file_storage import sanitize_layer_name
from ..settings import DEFAULT_EPSG

__all__ = ["fetch_gpkg_layers"]


def fetch_gpkg_layers(
    path_or_url: str,
) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Load all layers from a GeoPackage file or URL."""
    tmpdir: TemporaryDirectory | None = None
    gpkg_path = Path(path_or_url)
    if path_or_url.startswith("http"):
        tmpdir = TemporaryDirectory()
        gpkg_path = Path(tmpdir.name) / "data.gpkg"
        gpkg_path.write_bytes(http_client.fetch_bytes(path_or_url))
    try:
        results: List[Tuple[str, gpd.GeoDataFrame, int]] = []
        for layer in fiona.listlayers(str(gpkg_path)):
            gdf = gpd.read_file(gpkg_path, layer=layer)
            epsg = gdf.crs.to_epsg() or DEFAULT_EPSG
            results.append((sanitize_layer_name(layer), gdf, epsg))
        return results
    finally:
        if tmpdir is not None:
            tmpdir.cleanup()
```

## src/stp/fetch/lookup.py

```python
"""
Fetcher utilization
"""
# src/stp/scripts/download_utils.py

from stp.fetch.socrata import dispatch_socrata_table
from stp.fetch.arcgis import fetch_arcgis_table, fetch_arcgis_vector
from stp.fetch.csv import fetch_csv_direct
from stp.fetch.geojson import fetch_geojson_direct
from stp.fetch.gdb import fetch_gdb_or_zip
from stp.fetch.gpkg import fetch_gpkg_layers

FETCHERS = {
    ("socrata", "csv"):   dispatch_socrata_table,
    ("socrata", "json"):  dispatch_socrata_table,
    ("socrata", "geojson"): dispatch_socrata_table,
    ("socrata", "shapefile"): dispatch_socrata_table,
    ("arcgis", "csv"):    fetch_arcgis_table,
    ("arcgis", "json"):   fetch_arcgis_table,
    ("arcgis", "geojson"): fetch_arcgis_vector,
    ("arcgis", "shapefile"): fetch_arcgis_vector,
    (None, "csv"):        fetch_csv_direct,
    (None, "geojson"):    fetch_geojson_direct,
    (None, "shapefile"):  fetch_gdb_or_zip,
    (None, "gpkg"):       fetch_gpkg_layers,
}
```

## src/stp/fetch/socrata.py

```python
"""Placeholder Socrata fetcher."""

from __future__ import annotations

from typing import List, Tuple, Optional

import geopandas as gpd

__all__ = ["dispatch_socrata_table"]


def dispatch_socrata_table(url: str, app_token: Optional[str] = None
                           ) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Temporary stub for Socrata dataset fetching."""
    raise NotImplementedError("Socrata fetcher not implemented")
```

## src/stp/fetch/__init__.py

```python
"""Spatial data fetcher helpers."""

from .csv import fetch_csv_direct
from .geojson import fetch_geojson_direct
from .arcgis import fetch_arcgis_vector, fetch_arcgis_table
from .gdb import fetch_gdb_or_zip
from .gpkg import fetch_gpkg_layers
from .socrata import dispatch_socrata_table

__all__ = [
    "fetch_csv_direct",
    "fetch_geojson_direct",
    "fetch_arcgis_vector",
    "fetch_arcgis_table",
    "fetch_gdb_or_zip",
    "fetch_gpkg_layers",
    "dispatch_socrata_table",
]
```

## src/stp/process/clean/address.py

```python
"""Address-related cleaning routines."""

from typing import Optional

import geopandas as gpd
import pandas as pd


def clean_street_signs(
    gdf: gpd.GeoDataFrame,
    *,
    require_record_type: str = "Current",
    date_fields: Optional[list[str]] = None,
    int_fields: Optional[list[str]] = None,
    drop_suffixes: Optional[list[str]] = None,
    keep_fields: Optional[list[str]] = None,
) -> gpd.GeoDataFrame:
    """Clean street sign records and drop non-current entries."""
    df = gdf.copy()
    if date_fields is None:
        date_fields = [
            "order_completed_on_date",
            "sign_design_voided_on_date",
        ]
    if int_fields is None:
        int_fields = ["distance_from_intersection"]
    if drop_suffixes is None:
        drop_suffixes = [
            "on_street_suffix",
            "from_street_suffix",
            "to_street_suffix",
        ]
    if keep_fields is None:
        keep_fields = [
            "order_number",
            "record_type",
            "order_type",
            "borough",
            "on_street",
            "from_street",
            "side_of_street",
            "order_completed_on_date",
            "sign_code",
            "sign_description",
            "sign_size",
            "sign_location",
            "distance_from_intersection",
            "arrow_direction",
            "sheeting_type",
            "support",
            "to_street",
            "facing_direction",
            "sign_notes",
            "sign_design_voided_on_date",
        ]
    df = df[df["record_type"].str.strip().str.title() == require_record_type]
    for fld in date_fields:
        if fld in df:
            df[fld] = pd.to_datetime(df[fld], errors="coerce")
    for fld in int_fields:
        if fld in df:
            df[fld] = pd.to_numeric(df[fld], errors="coerce")
    df = df.drop(columns=[c for c in drop_suffixes if c in df.columns])
    final_cols = [c for c in keep_fields if c in df.columns] + ["geometry"]
    df = df[final_cols].copy()
    df["record_type"] = df["record_type"].str.strip().str.title()
    df["side_of_street"] = df["side_of_street"].str.strip().str.upper()
    df["arrow_direction"] = df["arrow_direction"].str.strip()
    df["sign_description"] = df["sign_description"].str.strip()
    return gpd.GeoDataFrame(df, geometry="geometry", crs=gdf.crs)
```

## src/stp/process/clean/trees.py

```python
"""Tree-related cleaning routines."""

from typing import Optional

import geopandas as gpd

MIN_DBH = 0.01


def clean_trees_basic(
    trees: gpd.GeoDataFrame,
    *,
    structure_field: str = "tpstructure",
    require_structure: str = "Full",
    id_field: str = "objectid",
    out_id: str = "TreeID",
) -> gpd.GeoDataFrame:
    """Return trees with full structure."""
    df = trees.loc[
        trees[structure_field] == require_structure, [id_field, "geometry"]
    ].copy()
    return df.rename(columns={id_field: out_id})


def clean_trees_advanced(
    trees: gpd.GeoDataFrame,
    planting_spaces: gpd.GeoDataFrame,
    *,
    condition_field: str = "tpcondition",
    drop_conditions: Optional[list[str]] = None,
    structure_field: str = "tpstructure",
    require_structure: str = "Full",
    dbh_field: str = "dbh",
    min_dbh: float = MIN_DBH,
    ps_key: str = "plantingspaceglobalid",
    ps_globalid: str = "globalid",
    ps_status_field: str = "psstatus",
    keep_ps_status: str = "Populated",
    ps_jur_field: str = "jurisdiction",
    exclude_jur: str = "Private",
    id_field: str = "objectid",
    out_id: str = "TreeID",
) -> gpd.GeoDataFrame:
    """Return cleaned trees joined to planting spaces."""
    if drop_conditions is None:
        drop_conditions = ["Unknown", "Dead"]
    mask = (
        ~trees[condition_field].isin(drop_conditions)
        & (trees[structure_field] == require_structure)
        & trees[dbh_field].gt(min_dbh)
    )
    df = trees.loc[mask].copy()
    df = df.merge(
        planting_spaces[[ps_globalid, ps_status_field, ps_jur_field]],
        left_on=ps_key,
        right_on=ps_globalid,
        how="inner",
    )
    ps_mask = (
        (df[ps_status_field] == keep_ps_status)
        & (df[ps_jur_field] != exclude_jur)
    )
    df = df.loc[ps_mask]
    df = df[[id_field, "geometry"]]
    return gpd.GeoDataFrame(df, geometry="geometry", crs=trees.crs).rename(
        columns={id_field: out_id}
    )


def canceled_work_orders(
    wo: gpd.GeoDataFrame,
    *,
    wo_type_field: str = "wotype",
    wo_cat_field: str = "wocategory",
    wo_status_field: str = "wostatus",
    allowed_types: Optional[list[str]] = None,
    allow_category: str = "Tree Planting",
    cancel_status: str = "Cancel",
    id_field: str = "objectid",
    out_id: str = "WOID",
) -> gpd.GeoDataFrame:
    """Filter work orders to cancelled planting jobs."""
    if allowed_types is None:
        allowed_types = [
            "Tree Plant-Park Tree",
            "Tree Plant-Street Tree",
            "Tree Plant-Street Tree Block",
        ]
    mask = (
        wo[wo_type_field].isin(allowed_types)
        & (wo[wo_cat_field] == allow_category)
        & (wo[wo_status_field] == cancel_status)
    )
    df = wo.loc[mask, [id_field, "geometry"]].copy()
    return df.rename(columns={id_field: out_id})


def clean_planting_spaces(
    ps: gpd.GeoDataFrame,
    *,
    status_field: str = "psstatus",
    keep_status: str = "Populated",
    jur_field: str = "jurisdiction",
    exclude_jur: str = "Private",
    id_field: str = "globalid",
    out_id: str = "PSID",
) -> gpd.GeoDataFrame:
    """Return populated planting spaces excluding private sites."""
    mask = ps[status_field] == keep_status
    if exclude_jur:
        mask &= ps[jur_field] != exclude_jur
    df = ps.loc[mask, [id_field, "geometry"]].copy()
    return df.rename(columns={id_field: out_id})
```

## src/stp/process/clean/__init__.py

```python

```

## src/stp/process/custom_ops.py

```python
"""Custom operations for STP pipeline (special/complex functions)."""

import logging

import geopandas as gpd
from pygeoops import centerline
# pip install pygeoops (medial axis for polygon centerlines)

logging.basicConfig(level=logging.INFO)


def collapse_to_centerline(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
    """Collapse polygons to centerlines (like CollapseHydroPolygon).

    Extracts medial axis (midpoints between edges).
    """
    centerlines = []
    for geom in gdf.geometry:
        if geom.is_valid and not geom.is_empty:
            cl = centerline(geom)
            if cl:
                centerlines.append(cl)
    collapsed = gpd.GeoDataFrame(geometry=centerlines, crs=gdf.crs)
    logging.info("Collapsed to centerlines")
    return collapsed
```

## src/stp/process/data_cleaning.py

```python
"""High-level cleaning dispatch functions."""

from .clean.trees import (
    clean_trees_basic,
    clean_trees_advanced,
    canceled_work_orders,
    clean_planting_spaces,
)
from .clean.address import clean_street_signs

__all__ = [
    "clean_trees_basic",
    "clean_trees_advanced",
    "canceled_work_orders",
    "clean_planting_spaces",
    "clean_street_signs",
]
```

## src/stp/process/field_ops.py

```python
"""Field operations for STP pipeline (attribute tweaks)."""

import logging

import geopandas as gpd

logging.basicConfig(level=logging.INFO)

def calculate_unique_blocks(
    gdf: gpd.GeoDataFrame,
    borough_field: str = 'Borough',
    block_field: str = 'Block',
    output_field: str = 'Unique_Blocks'
) -> gpd.GeoDataFrame:
    """Calculate unique blocks (concat Borough + Block, like CalculateField)."""
    gdf[output_field] = gdf[borough_field].astype(str) + gdf[block_field].astype(str)
    logging.info("Calculated %s", output_field)
    return gdf


def add_unique_parts(
    gdf: gpd.GeoDataFrame,
    unique_field: str = 'Unique_Blocks',
    output_field: str = 'Unique_Block_Parts'
) -> gpd.GeoDataFrame:
    """Add unique parts field(Unique_Blocks + index,
      like AddField + CalculateField)."""
    gdf[output_field] = gdf[unique_field].astype(str) + gdf.index.astype(str)
    logging.info("Added %s", output_field)
    return gdf


def delete_fields(
        gdf: gpd.GeoDataFrame, fields_to_drop: list
        ) -> gpd.GeoDataFrame:
    """Delete fields (like DeleteField). Drops specified columns."""
    gdf = gdf.drop(columns=fields_to_drop, errors='ignore')
    logging.info("Deleted fields")
    return gdf
```

## src/stp/process/geometry_ops.py

```python
"""Geometry operations for STP pipeline (shape-changing functions)."""

import logging

import geopandas as gpd

logging.basicConfig(level=logging.INFO)


def dissolve_gdf(gdf: gpd.GeoDataFrame, by_field: str, single_part: bool = True
                 ) -> gpd.GeoDataFrame:
    """Dissolve by field (like PairwiseDissolve).

    Groups features with same value in by_field, merging geometries.
    """
    dissolved = gdf.dissolve(by=by_field)
    if single_part:
        # Multipart to singlepart
        dissolved = dissolved.explode(ignore_index=True)
    logging.info("Dissolved by %s", by_field)
    return dissolved


def erase_gdf(input_gdf: gpd.GeoDataFrame, erase_gdf: gpd.GeoDataFrame
              ) -> gpd.GeoDataFrame:
    """Erase input by erase geometry (like PairwiseErase).

    Removes parts of input that overlap erase.
    """
    # unary_union combines erase shapes into one
    erased_geom = input_gdf.difference(erase_gdf.unary_union)
    erased = gpd.GeoDataFrame(
        geometry=erased_geom, crs=input_gdf.crs
        ).explode(ignore_index=True)
    logging.info("Erased geometries")
    return erased


def buffer_gdf(gdf: gpd.GeoDataFrame, distance: float
               ) -> gpd.GeoDataFrame:
    """Buffer geometries (like PairwiseBuffer).

    Adds a 'halo' around shapes.
    """
    buffered = gdf.buffer(distance)
    buffered_gdf = gpd.GeoDataFrame(geometry=buffered, crs=gdf.crs)
    logging.info("Buffered by %s", distance)
    return buffered_gdf


def intersect_gdf(input_gdf: gpd.GeoDataFrame, intersect_gdf: gpd.GeoDataFrame
                  ) -> gpd.GeoDataFrame:
    """Intersect (like PairwiseIntersect).

    Keeps overlapping parts, combines attributes.
    """
    intersected = gpd.overlay(input_gdf, intersect_gdf, how='intersection')
    logging.info("Intersected geometries")
    return intersected


def repair_geometry(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
    """Repair invalid geometries (like RepairGeometry).

    Fixes issues like self-intersects or bad rings.
    """
    gdf['geometry'] = gdf.make_valid()
    logging.info("Repaired geometries")
    return gdf
```

## src/stp/process/table.py

```python
"""Backward-compatible passthroughs for inventory and metadata helpers."""

from __future__ import annotations

from ..record.db import record as record_layer_metadata_db
from ..record.csv import record as record_layer_metadata_csv
from ..fetch.gpkg import (
    from_gpkg as build_fields_inventory_gpkg,
)
from ..fetch.postgis import (
    from_postgis as build_fields_inventory_postgis,
)
from ..fetch.export import to_csv as write_inventory

__all__ = [
    "record_layer_metadata_db",
    "record_layer_metadata_csv",
    "build_fields_inventory_gpkg",
    "build_fields_inventory_postgis",
    "write_inventory",
]
```

## src/stp/record/csv.py

```python
"""CSV metadata recording functions."""

from __future__ import annotations

import csv
import logging
from datetime import datetime
from pathlib import Path

__all__ = ["record"]


def record(
        csv_path: Path,
        layer_id: str,
        url: str,
        source_epsg: int,
        service_wkid: int | None = None) -> None:
    """Append a row to ``layers_inventory.csv``.

    Parameters
    ----------
    csv_path:
        Destination CSV path.
    layer_id:
        Identifier for the layer being recorded.
    url:
        Source URL of the dataset.
    source_epsg:
        EPSG code of the dataset.
    service_wkid:
        Optional WKID from an ArcGIS service.
    """
    logger = logging.getLogger(__name__)
    write_header = not csv_path.exists()
    try:
        csv_path.parent.mkdir(parents=True, exist_ok=True)
        with csv_path.open("a", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            if write_header:
                writer.writerow([
                    "layer_id",
                    "source_url",
                    "source_epsg",
                    "service_wkid",
                    "downloaded_at",
                ])
            writer.writerow([
                layer_id,
                url,
                source_epsg,
                service_wkid if service_wkid is not None else "",
                datetime.utcnow().isoformat(),
            ])
    except Exception as exc:  # pragma: no cover - log and continue
        logger.error("Failed to record metadata CSV %s: %s", csv_path, exc)
```

## src/stp/record/db.py

```python
"""Database metadata recording functions."""

from __future__ import annotations

import logging
from sqlalchemy.engine import Engine
from sqlalchemy import text

__all__ = ["record"]


def record(
        engine: Engine,
        layer_id: str,
        url: str,
        source_epsg: int,
        service_wkid: int | None = None) -> None:
    """Insert a row into the ``layers_inventory`` table.

    Parameters
    ----------
    engine:
        SQLAlchemy database engine.
    layer_id:
        Identifier for the layer being recorded.
    url:
        Source URL of the dataset.
    source_epsg:
        EPSG code of the dataset.
    service_wkid:
        Optional WKID from an ArcGIS service.
    """
    if engine is None:
        return

    logger = logging.getLogger(__name__)
    stmt = text(
        """
        INSERT INTO layers_inventory (
            layer_id, source_url, source_epsg, service_wkid, downloaded_at
        ) VALUES (
            :layer_id, :url, :epsg, :service_wkid, NOW()
        )
        ON CONFLICT (layer_id) DO NOTHING
        """
    )
    try:
        engine.execute(
            stmt,
            {
                "layer_id": layer_id,
                "url": url,
                "epsg": source_epsg,
                "service_wkid": service_wkid,
            },
        )
    except Exception as exc:  # pragma: no cover - log and continue
        logger.error("Failed to record metadata for %s: %s", layer_id, exc)
```

## src/stp/record/export.py

```python
"""Export inventory DataFrames."""

from __future__ import annotations

from pathlib import Path
import pandas as pd

__all__ = ["to_csv"]


def to_csv(df: pd.DataFrame, out_csv: Path, show_path: bool = True) -> None:
    """Write *df* to *out_csv* and optionally print the path."""
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(out_csv, index=False)
    if show_path:
        print(f"Schema inventory written to {out_csv}")
```

## src/stp/record/gpkg.py

```python
"""Extract field inventory from a GeoPackage."""

from __future__ import annotations

from pathlib import Path
from typing import List, Dict

import fiona
import pandas as pd

__all__ = ["from_gpkg"]


def from_gpkg(gpkg_path: Path) -> pd.DataFrame:
    """Return field inventory for all layers in *gpkg_path*.

    The returned ``DataFrame`` has columns ``layer_name``, ``field_name`` and
    ``field_type``.
    """
    rows: List[Dict[str, str]] = []
    for layer in fiona.listlayers(str(gpkg_path)):
        with fiona.open(str(gpkg_path), layer=layer) as src:
            for field, ftype in src.schema["properties"].items():
                rows.append(
                    {
                        "layer_name": layer,
                        "field_name": field,
                        "field_type": ftype,
                    }
                )
    return pd.DataFrame(rows)
```

## src/stp/record/postgis.py

```python
"""Extract field inventory from a PostGIS database."""

from __future__ import annotations

from sqlalchemy import text
from sqlalchemy.engine import Engine
import pandas as pd

__all__ = ["from_postgis"]


def from_postgis(engine: Engine, schema: str = "public") -> pd.DataFrame:
    """Return field inventory for all tables in a PostGIS schema."""
    sql = text(
        """
        SELECT table_name AS layer_name,
               column_name AS field_name,
               data_type AS field_type
        FROM information_schema.columns
        WHERE table_schema = :schema
        ORDER BY table_name, ordinal_position
        """
    )
    return pd.read_sql(sql, engine, params={"schema": schema})
```

## src/stp/record/__init__.py

```python
"""Schema inventory helpers."""
```

## src/stp/scaffold.md

````markdown
# Scaffold.md: Tree Planting Analysis Pipeline Overview

This document outlines the high-level workflow for the STP (Spatial Tabular Pipeline) GIS project, originally built in ArcPy/ModelBuilder and now converted to pure Python (using GeoPandas for ops like buffers/unions/clips). The focus is NYC tree planting analysis, but it's designed for flexibility (e.g., toggle steps for other cities via config edits). We use a unified YAML config to drive the pipeline dynamically, with Docker for automatic PostGIS setup (spatial DB storage for efficient queries/filters).

## 1. Define Configuration
Configuration is centralized for ease‚Äîedit files to customize sources, ops, filters, and params without changing code. This replaces a "locked" pipeline with variable/user-defined flows (e.g., nest ops in YAML for reordering/toggling).

- **root/config/workflow.yaml**: Main unified config (combines sources, prep ops, filters, and final steps).
  - Data sources from open data portals (e.g., Socrata/ArcGIS REST).
  - Includes URL, type/format, schema, filter parameters (e.g., Socrata $where queries).
  - Nested ops/steps for processing (e.g., copy ‚Üí select ‚Üí buffer), with descriptions, enabled toggles, inputs/outputs.
  - Users can fill out or override (e.g., add local paths for offline use).
  - For reusing a feature class multiple times (e.g., curb_cut in different filters): Reference the same output_layer in steps‚Äîno duplication needed. If mutable (changes needed without overwriting), nest a 'copy' op first. If non-mutable download, use local_path fallback in sources section.
  - Example snippet (from our merged version):
    ```yaml
    data:  # Top-level for all datasets (renamed from 'data_id' for clarity)
        trees:
            sources:  # Fetch details
                type: "json"
                source_type: "socrata"
                format: "json"
                url: "https://data.cityofnewyork.us/resource/hn5i-inap.json"
                schema: "political"
                filter: "SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')"
                to_layer: "trees_raw"
                enabled: true
            prep_ops:  # Nested processing
                description: "Existing tree locations to avoid"
                enabled: true
                steps:
                    - op: copy
                    input: "trees_raw"  # References the source's to_layer
                    output: "trees_copy"
                    - op: xy_to_point
                    output: "trees_1"
                    - op: buffer
                    distance: 25
                    output: "trees_ready"

        nyzd:  # Another dataset bundle
            sources:
            # ... (URL/type/filter)
            prep_ops:
            # ... (steps like copy/select)
    ```

- **root/config/defaults.yaml**: Stores default parameters for tools/ops (e.g., EPSG codes, limits). Optional but useful for globals; can be loaded into pipeline.py and overridden in workflow.yaml if needed.
  - Example:
    ```yaml
    epsg:
      default: 4326
      nysp: 2263
    limits:
      socrata: 50000
      arcgis_default_max_records: 1000
    validation:
      layer_name_max_length: 60
      min_dbh: 0.01
    ```

- **root/config/user.yaml**: User-specific overrides (e.g., API keys, batch params, DB creds). Loads defaults.yaml if none specified. With variable pipelines, this is key for personalization (e.g., change Socrata batch size).
  - Example for DB/Docker:
    ```yaml
    db:
      user: 'admin'
      pass: 'admin'
      host: 'localhost'
      port: 5432
      db_name: 'tree_pipeline'
    api:
      socrata_key: 'your_key_here'  # If needed for throttled APIs
      batch_size: 50000  # Override defaults never above 50000, if above revert to 50,000
    ```

## 2. Download & Prep Sources
Primary goal: Fetch and prep NYC-relevant data for tree planting (e.g., trees, hydrants, sidewalks) from open sources. Variable for other cities (edit workflow.yaml URLs/filters). Downloads via fetchers/ (Socrata/ArcGIS REST handlers in STP tools), with API keys/batch params from user.yaml (e.g., batch to avoid limits on big datasets like census blocks).

- Sources located in workflow.yaml 'sources' section (merged from old sources.json).
- Sources can be from anywhere, but default to Socrata and ArcGIS REST.
  - Requires API key/parameters (e.g., batch size/combining for large fetches) from user.yaml.
- Storage: Use Docker to automatically install/setup a DB with PostgreSQL/PostGIS for spatial efficiency (queries/filters on layers like zoning districts).
  - Docker auto-creates the environment (isolated, portable‚Äîrun `docker compose up -d` from docker-compose.yml).
  - Load creds from user.yaml (e.g., user: 'admin', pass: 'admin', host: 'localhost', port: 5432).
  - Connect: `engine = create_engine('postgresql://admin:admin@localhost:5432/tree_pipeline')`.
  - Load data: After download, `gdf.to_postgis('trees_raw', engine, if_exists='replace')` (GeoPandas handles conversion to spatial tables).
  - Why PostGIS? Faster for ops (e.g., SQL filters during select) than files; fallback to GPKG if needed.
- Prep: Run nested steps from workflow.yaml 'prep_ops' (e.g., copy ‚Üí xy_to_point ‚Üí buffer). Outputs saved as new tables/layers.
- Final clip/big scripts: Handled in workflow.yaml sections (nested steps for union/clip/points generation, then nostanding/rank/curb).

3. Define Queries
    - 


3. Download & Convert JSON ‚Üí GeoJSON / in-geodatabase exports
    - Data sources: 
        - Socrata
        - ArcGIS REST 
    - Data is initially stored in ./data 
    * files are stored in a gpkg in ./data/gpkg

4. Data Cleaning
    - 

5. Preliminary Operations
   3.1 ExportFeatures(Online_NYZD ‚Üí NYZD)
   3.2 PairwiseBuffer(NYZD ‚Üí NYZD_Buffer, 20 ft)
   3.3 ExportFeatures(BK_Vaults ‚Üí BK_Vaults_2)
   3.4 PairwiseBuffer(BK_Vaults_2 ‚Üí BK_Vaults_Buffer, 20 ft)
   3.5 ExportFeatures(STP_Apps_Vaults ‚Üí DOT_Vault_ExportFeatures)
   3.6 PairwiseBuffer(DOT_Vault_ExportFeatures ‚Üí DOT_Vault_Buffer, 20 ft)
   3.7 ExportFeatures(ConEd_Transformer_Vault ‚Üí ConEd_Transformer_Vault)
   3.8 PairwiseBuffer(ConEd_Transformer_Vault ‚Üí ConEd_Transformer_Buffer, 20 ft)
   3.9 ExportFeatures(DEP_GI_Assets ‚Üí DEP_GI_Assets)
   3.10 PairwiseBuffer(DEP_GI_Assets ‚Üí DEP_GI_Assets_Buffer, 20 ft)

4. Copy & select raw features
   4.1 CopyFeatures(SIDEWALK ‚Üí SIDEWALK_2)
   4.2 CopyFeatures(HVI_CensusTracts_v2013 ‚Üí HVI_CensusTracts_v2013_Copy)
   4.3 CopyFeatures(Curb_Cuts_Intersections_2 ‚Üí CURB_CUT_CopyFeatures)
   4.4 Select(CURB_CUT_CopyFeatures ‚Üí Curb_Cuts_Intersections, where SUB_FEATURE_CODE=222700)
   4.5 PairwiseBuffer(Curb_Cuts_Intersections ‚Üí Curb_Cuts_Intersection_Buffer, 30 ft)
   4.6 CopyFeatures(Subway_Lines ‚Üí Subway_Lines_2)
   4.7 PairwiseBuffer(Subway_Lines_2 ‚Üí Subway_Lines_Buffer, 80 ft)

5. Convert CSV ‚Üí points
   5.1 ExportTable(Workorders.csv ‚Üí Workorders)
   5.2 XYTableToPoint(Workorders ‚Üí Workorders_XYTableToPoint)
   5.3 PairwiseBuffer(Workorders_XYTableToPoint ‚Üí WorkOrders_Buffer, 25 ft)
   5.4 ExportTable(TreeandSite.csv ‚Üí TreenSite)
   5.5 XYTableToPoint(TreenSite ‚Üí TreenSite_XYTableToPoint)
   5.6 PairwiseBuffer(TreenSite_XYTableToPoint ‚Üí TreeAndSite_Buffer, 25 ft)

6. Clean & prep shrub layer
   6.1 CopyFeatures(Grass_Shrub_ExportFeatures ‚Üí Grass_Shrub)
   6.2 DeleteField(Grass_Shrub, ["Id", "gridcode"])
   6.3 AddField("Pit_Type", TEXT, length=10)
   6.4 CalculateField(Pit_Type = "EP/LP")

7. Clean & prep HVI tracts
   7.1 CopyFeatures(FHNR_Datasets_HVI_CensusTracts ‚Üí HVI_CensusTracts_v2013_CopyFeatures)
   7.2 DeleteField(HVI_CensusTracts_v2013_CopyFeatures, [...lots of fields...])
   7.3 AlterField("QUINTILES" ‚Üí "HVI_CT_2013")

8. Union political & HVI boundaries
   8.1 Union([
         Political_Boundary,
         HVI_CensusTracts_v2013_CopyFeatures_2,
         NYCDOHMH_HVI_CensusTracts_2018_Clip,
         NYCDOHMH_HVI_CensusTracts_2023,
         NYCDOHMH_HVI_NeighborhoodTabulationAreas_2018,
         NYCDOHMH_HVI_NeighborhoodTabulationAreas_2023,
         NYCDCP_Borough_Boundaries_Water_Included,
         NYCDCP_Borough_Boundaries
       ] ‚Üí Political_Boundary_Final)
   8.2 DeleteField(Political_Boundary_Final, [all FID_* fields])

9. Further buffers & selections
   9.1 PairwiseBuffer(Curb_Cuts_Intersections ‚Üí Curb_Cuts_Intersection_20ft, 20 ft)
   9.2 Select(Street_Centerline WHERE L_LOW_HN <> '' ‚Üí Street_Centerline_Select)
   9.3 SimplifyLine(Street_Centerline_Select ‚Üí Street_Centerli_SimplifyLine, POINT_REMOVE, 1 ft)
   9.4 FeatureVerticesToPoints(Street_Centerli_SimplifyLine ‚Üí Street_Vertices_Points)
   9.5 PairwiseBuffer(Street_Vertices_Points ‚Üí Street_Vertice_Buffer, 40 ft)

10. Hydrant proximity & cleanup
   10.1 Near(DEP_Hydrants ‚Üí Sidewalk_Pluto, radius=10 ft, location)
   10.2 MoveStreetSigns(Hydrants_Near ‚Üí Hydrants_Corrected)
   10.3 PairwiseBuffer(Hydrants_Corrected ‚Üí DEP_Hydrants_PairwiseBuffer, 3 ft)

11. Driveway curb-cut processing
    11.1 Select(CURB_CUT_CopyFeatures WHERE SUB_FEATURE_CODE=222600 ‚Üí Curb_Cuts_Driveways)
    11.2 CurbCuts(input=Curb_Cuts_Driveways, extension=7, buffer=15 ‚Üí Curb_Cuts_Driveways_Buffer)

12. Vault union & cleanup
    12.1 Union([BK_Vaults_Buffer, DOT_Vault_Buffer] ‚Üí Vaults)
    12.2 DeleteField(Vaults, [a long list of original fields])
    12.3 AddField("Vaults", LONG)
    12.4 CalculateField(Vaults = 1)
    12.5 (Placeholder) Union(‚Ä¶ ‚Üí Output_Feature_Class)

13. Final buffer
    13.1 PairwiseBuffer(Curb_Cuts_Intersections ‚Üí Curb_Cuts_Intersection_10ft, 10 ft)
````

## src/stp/scrap.md

```markdown
# To Use / How To Use

## No Planting Areas / Locations to clip sidewalk

- NYZD  
  - Description: Zoning districts where planting is prohibited  
  - Filters: `"ZONE_DIST" IN ('M1','M2','M3','IG','IH')`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `nyzd_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `nyzd_ready`

- DEP_GI_Assets  
  - Description: Green infrastructure assets to avoid  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `dep_gi_assets_copy`  
    - Operation b: Buffer  
      - Params: distance = 20 Feet  
      - Output: `dep_gi_assets_ready`

- Sidewalk  
  - Description: Raw sidewalk polygons, split for different logic  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `sidewalk_copy`  
    - Operation b: Polygon‚ÜíPolyline  
      - Params: input = `sidewalk_copy`  
      - Output: `sidewalk_1`  
    - Operation c: Split immutable/mutable  
      - Params: input = `sidewalk_1`  
      - Outputs: `sidewalk_immutable_ready`, `sidewalk_mutable_ready`

- Curb_Cuts  
  - Description: Sidewalk curb-cut intersections  
  - Filters: `SUB_FEATURE_CODE = 222700`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `curb_cuts_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `curb_cuts_1`  
    - Operation c: Buffer  
      - Params: input = `curb_cuts_1`, distance = 30 Feet  
      - Output: `curb_cuts_ready`

- Subway_Lines  
  - Description: Subway buffers to exclude planting near tracks  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `subway_lines_copy`  
    - Operation b: Buffer  
      - Params: distance = 80 Feet  
      - Output: `subway_lines_ready`

- Workorders  
  - Description: Active DOT work orders  
  - Filters: `STATUS <> 'Cancelled'`  
  - Operations  
    - Operation a: CopyFeatures (ExportTable/Select)  
      - Output: `workorders_copy`  
    - Operation b: XYTableToPoint  
      - Params: input = `workorders_copy`  
      - Output: `workorders_1`  
    - Operation c: Buffer  
      - Params: input = `workorders_1`, distance = 25 Feet  
      - Output: `workorders_ready`

- TreeandSite  
  - Description: Existing tree locations to avoid  
  - Filters: `SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')`  
  - Operations  
    - Operation a: CopyFeatures (ExportTable/Select)  
      - Output: `treeandsite_copy`  
    - Operation b: XYTableToPoint  
      - Params: input = `treeandsite_copy`  
      - Output: `treeandsite_1`  
    - Operation c: Buffer  
      - Params: input = `treeandsite_1`, distance = 25 Feet  
      - Output: `treeandsite_ready`

- Grass_Shrub  
  - Description: 2017 land-use raster converted to shrub/grass polygons  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures (Copy Raster)  
      - Output: `grass_shrub_copy`  
    - Operation b: RasterToPolygon_conversion  
      - Params: simplify = NO_SIMPLIFY  
      - Output: `grass_shrub_1`  
    - Operation c: DeleteField  
      - Params: fields = `["gridcode","Id"]`  
      - Output: `grass_shrub_ready`

- Political_Boundary  
  - Description: Union of all political boundaries for spatial join  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `political_boundary_copy`  
    - Operation b: Union_analysis  
      - Params: inputs = borough, community boards, council, senate, assembly, NTA, tracts  
      - Output: `political_boundary_1`  
    - Operation c: DeleteField  
      - Params: fields = all `FID_*`  
      - Output: `political_boundary_ready`

- Street_Centerline  
  - Description: Simplify street geometry and buffer vertices  
  - Filters: `L_LOW_HN IS NOT NULL`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `street_centerline_copy`  
    - Operation b: SimplifyLine_cartography  
      - Params: method = POINT_REMOVE, tolerance = 1 Feet  
      - Output: `street_centerline_1`  
    - Operation c: FeatureVerticesToPoints  
      - Params: input = `street_centerline_1`  
      - Output: `street_centerline_2`  
    - Operation d: Buffer  
      - Params: input = `street_centerline_2`, distance = 40 Feet  
      - Output: `street_centerline_ready`

- DEP_Hydrants  
  - Description: Hydrant proximity adjustments  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `dep_hydrants_copy`  
    - Operation b: GenerateNearTable  
      - Params: distance = 10 Feet  
      - Output: `dep_hydrants_1`  
    - Operation c: MoveStreetSigns (custom)  
      - Params: as defined in script  
      - Output: `dep_hydrants_2`  
    - Operation d: Buffer  
      - Params: input = `dep_hydrants_2`, distance = 3 Feet  
      - Output: `dep_hydrants_ready`

- Curb_Cuts_Driveways  
  - Description: Driveway curb cuts to exclude planting  
  - Filters: `SUB_FEATURE_CODE = 222600`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `curb_cuts_driveways_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `curb_cuts_driveways_1`  
    - Operation c: Buffer  
      - Params: input = `curb_cuts_driveways_1`, distance = 15 Feet  
      - Output: `curb_cuts_driveways_ready`

## Final first step

- Final Clip  
  - Description: Compute allowable planting points within clipped sidewalk  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures (non-mutable backup)  
      - Params: input = `sidewalk_mutable_ready`  
      - Output: `sidewalk_mutable_backup`  
    - Operation b: Union no-plant zones  
      - Params: inputs = nyzd_ready, dep_gi_assets_ready, curb_cuts_ready, subway_lines_ready, workorders_ready, treeandsite_ready, grass_shrub_ready  
      - Output: `no_plant_zones_union`  
    - Operation c: Clip_analysis  
      - Params: input = `sidewalk_mutable_ready`, clip_features = `no_plant_zones_union`  
      - Output: `sidewalk_availability_ready`  
    - Operation d: Create planting points  
      - Params: method = CreateFishnet or custom  
      - Output: `plant_pts_1`  
    - Operation e: SpatialJoin  
      - Params: target = `plant_pts_1`, join_features = `sidewalk_availability_ready`  
      - Output: `plant_pts_ready`
```

## src/stp/storage/file_storage.py

```python
"""File-based GeoDataFrame helpers."""

from pathlib import Path

import geopandas as gpd
import pandas as pd

__all__ = [
    "get_geopackage_path",
    "sanitize_layer_name",
    "export_spatial_layer",
    "reproject_all_layers",
]

LAYER_NAME_MAX_LENGTH = 60


def get_geopackage_path(
    output_dir: Path, filename: str = "project_data.gpkg"
) -> Path:
    """Return a fresh GeoPackage path under *output_dir*."""
    gpkg = Path(output_dir) / filename
    if gpkg.exists():
        try:
            gpkg.unlink()
        except PermissionError as err:
            print(f"\u26a0\ufe0f Could not delete '{gpkg}': {err}")
    return gpkg


def sanitize_layer_name(name: str) -> str:
    """Return *name* cleaned for file or database layers."""
    safe = "".join(ch if (ch.isalnum() or ch == "_") else "_" for ch in name)
    if safe and safe[0].isdigit():
        safe = "_" + safe
    return safe[:LAYER_NAME_MAX_LENGTH]


def export_spatial_layer(gdf: gpd.GeoDataFrame, layer_name: str,
                         gpkg_path: Path) -> None:
    """Write ``gdf`` to ``gpkg_path`` under ``layer_name``."""
    gdf.to_file(gpkg_path, layer=layer_name, driver="GPKG")


def reproject_all_layers(
    gpkg_path: Path, metadata_csv: Path, target_epsg: int
) -> None:
    """Reproject each layer in the GeoPackage in place."""
    meta = pd.read_csv(metadata_csv)
    for _, row in meta.iterrows():
        layer_name = row["layer_id"]
        source_epsg = int(row["source_epsg"])
        raw_wkid = row.get("service_wkid")
        try:
            service_wkid = int(raw_wkid) if raw_wkid not in (None, "") else ""
        except ValueError:
            service_wkid = ""
        gdf = gpd.read_file(gpkg_path, layer=layer_name)
        if gdf.crs is None:
            gdf = gdf.set_crs(epsg=source_epsg, allow_override=True)
        else:
            gdf = gdf.to_crs(epsg=source_epsg)
        gdf = gdf.to_crs(epsg=target_epsg)
        gdf.to_file(gpkg_path, layer=layer_name, driver="GPKG")
        if service_wkid:
            print(
                f"Reprojected '{layer_name}': {service_wkid} ‚Üí {target_epsg}"
            )
        else:
            print(
                f"Reprojected '{layer_name}': {source_epsg} ‚Üí {target_epsg}"
            )
```

## src/stp/storage/__init__.py

```python

```

## src/stp/stp.md

`````markdown
# STP ‚Äì Spatial / Tabular Pipeline

`src/stp` is a lightweight toolkit for fetching raw spatial / tabular data,
cleaning it, and persisting it to GeoPackage or PostGIS.  
Everything is pure-Python, built on GeoPandas, Shapely and SQLAlchemy.

---

## Folder structure

```
![alt text](image.png)
stp/
‚îÇ
‚îú‚îÄ‚îÄ cleaning/        ‚Üê cleaning steps for specific datasets
‚îÇ   ‚îú‚îÄ‚îÄ address.py
‚îÇ   ‚îú‚îÄ‚îÄ trees.py
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îú‚îÄ‚îÄ fetchers/        ‚Üê source-specific download helpers
‚îÇ   ‚îú‚îÄ‚îÄ arcgis.py         # ArcGIS REST ‚Üí GeoJSON/Parquet
‚îÇ   ‚îú‚îÄ‚îÄ csv.py            # Plain CSV files (HTTP/local)
‚îÇ   ‚îú‚îÄ‚îÄ gdb.py            # FileGDB via ogr2ogr
‚îÇ   ‚îú‚îÄ‚îÄ geojson.py        # Arbitrary GeoJSON endpoints
‚îÇ   ‚îú‚îÄ‚îÄ gpkg.py           # Remote GeoPackage layers
‚îÇ   ‚îú‚îÄ‚îÄ socrata.py        # Socrata Open Data API
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îú‚îÄ‚îÄ inventory/       ‚Üê schema inspection & export utilities
‚îÇ   ‚îú‚îÄ‚îÄ export.py         # dump layer ‚Üí CSV/Markdown schema
‚îÇ   ‚îú‚îÄ‚îÄ gpkg.py           # field inventory for GeoPackage
‚îÇ   ‚îú‚îÄ‚îÄ postgis.py        # field inventory for PostGIS
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îú‚îÄ‚îÄ metadata/        ‚Üê read / write layer-level metadata
‚îÇ   ‚îú‚îÄ‚îÄ csv.py            # side-car CSV metadata files
‚îÇ   ‚îú‚îÄ‚îÄ db.py             # PostGIS / SQLite layer comments
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îú‚îÄ‚îÄ scripts/         ‚Üê one-off CLI entry points
‚îÇ   ‚îî‚îÄ‚îÄ download_utils.py # `python -m stp.scripts.download_utils`
‚îÇ
‚îú‚îÄ‚îÄ storage/         ‚Üê persistence back-ends
‚îÇ   ‚îú‚îÄ‚îÄ db\_storage.py     # PostGIS via SQLAlchemy
‚îÇ   ‚îú‚îÄ‚îÄ file\_storage.py   # GeoPackage / Shapefile
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îî‚îÄ‚îÄ (root modules)   ‚Üê orchestration & shared helpers
‚îú‚îÄ‚îÄ config\_loader.py
‚îú‚îÄ‚îÄ data\_cleaning.py
‚îú‚îÄ‚îÄ download.py
‚îú‚îÄ‚îÄ fields\_inventory.py
‚îú‚îÄ‚îÄ http\_client.py
‚îú‚îÄ‚îÄ settings.py
‚îú‚îÄ‚îÄ table.py
‚îî‚îÄ‚îÄ **init**.py

```

---

## Top-level module cheat-sheet

| Module | Purpose |
|--------|---------|
| **config_loader.py** | Read YAML/ENV configuration & expose `get_setting`, `get_constant`. |
| **settings.py** | Hard-coded fall-backs (NYSP EPSG 2263, default filenames, etc.). |
| **download.py** | ‚ÄúOrchestrator‚Äù ‚Äì loops through every fetcher listed in config and drops raw files into `Data/raw/`. |
| **http_client.py** | Thin `requests.Session` wrapper with retry & back-off. |
| **data_cleaning.py** | Pipeline runner ‚Äì re-projects, trims fields, fixes datatypes using functions from `cleaning/`. |
| **fields_inventory.py** | Generates a schema report (dtype, null %, sample) for any GeoDataFrame or DB layer. |
| **table.py** | Helpers to convert between CSV ‚Üî GeoJSON ‚Üî GeoPackage with consistent field ordering. |

---

## How the pieces fit

```

fetchers/        ‚Üí  Data/raw/\*.geojson / .csv
‚îÇ
‚ñº
cleaning/         GeoDataFrame in EPSG:2263
‚îÇ
‚ñº
storage/          PostGIS (db\_storage)  or  GeoPackage (file\_storage)
‚îÇ
‚ñº
inventory/        Optional schema dump / metadata write-back

````

---

### Quickstart

```bash
# 1) install deps
pip install -r requirements.txt   # add psycopg2-binary for PostGIS

# 2) pull all configured layers
python -m stp.download

# 3) clean & normalise
python -m stp.data_cleaning

# 4) inspect schema (optional)
python -m stp.fields_inventory Data/clean/addresses.gpkg
````

That‚Äôs it!
Drop this `README_stp.md` into `src/stp/` to give new contributors an instant
map of the toolkit.
`````

## src/stp/zip.py

```python
import os
import zipfile

def main():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    root_folder_name = os.path.basename(script_dir)
    zip_path = os.path.join(script_dir, 'archived_py_files.zip')
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(script_dir):
            for file in files:
                if file.endswith('.py'):
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(root, script_dir)
                    path_parts = [root_folder_name] if rel_path == '.' else [root_folder_name] + rel_path.split(os.sep)
                    prefix = '_'.join(path_parts) + '_'
                    new_name = prefix + file
                    with open(full_path, 'rb') as f:
                        content = f.read()
                    zipf.writestr(new_name, content)

if __name__ == '__main__':
    main()
```

## src/stp/__init__.py

```python

```

## src/temp/download_data.py

```python
"""
download_data.py

Main entry point for fetching spatial and tabular datasets based on a
configuration file and source registry. Supports Socrata, ArcGIS, and
direct URLs (CSV, GeoJSON, Shapefile, GPKG). Records metadata and
optionally reprojects in a GeoPackage or loads into PostGIS.
"""

import json
import logging
from pathlib import Path

# use get_setting (aliased to 'get') so both settings.yaml overrides and
# defaults.yaml fallbacks work the same way
from stp.config_loader import get_setting as get, get_constant

from stp.fetch import fetch_arcgis_vector

from stp.storage.file_storage import (
    get_geopackage_path,
    get_postgis_engine,
    reproject_all_layers,
    sanitize_layer_name,
    export_spatial_layer,
)
from stp.process.table import (
    record_layer_metadata_csv,
    record_layer_metadata_db,
)
from stp.fetch.lookup import FETCHERS

logger = logging.getLogger(__name__)


def setup_destinations():
    """Read config settings and prepare output destinations."""
    socrata_token = get("socrata.app_token")
    db_cfg = get("db", {})

    if db_cfg.get("enabled", False):
        db_engine = get_postgis_engine(db_cfg)
    else:
        db_engine = None

    output_epsg = get("data.output_epsg", get_constant("nysp_epsg"))
    out_shp_dir = Path(get("data.output_shapefile"))
    out_tbl_dir = Path(get("data.output_tables"))
    out_shp_dir.mkdir(parents=True, exist_ok=True)
    out_tbl_dir.mkdir(parents=True, exist_ok=True)

    if not db_engine:
        metadata_csv = out_tbl_dir / get_constant(
            "data_inventory_filename"
        )
        gpkg = get_geopackage_path(out_shp_dir)
        if metadata_csv.exists():
            try:
                metadata_csv.unlink()
            except OSError as e:
                logger.warning(
                    "Could not delete existing CSV '%s': %s", metadata_csv, e
                )
    else:
        metadata_csv = None
        gpkg = None

    return socrata_token, db_engine, gpkg, metadata_csv, output_epsg


def load_layer_list():
    """Load the list of layers to fetch from the JSON registry."""
    data_sources = Path("config") / "sources.json"
    with open(data_sources, encoding="utf-8") as f:
        return json.load(f)


def process_layer(
    layer, idx, total, socrata_token, db_engine, gpkg, metadata_csv
):
    """Fetch, record metadata, and store one layer."""
    layer_id = layer["id"]
    url = layer["url"]
    stype = layer.get("source_type")
    fmt = layer.get("format", "").lower()
    helper_fn = FETCHERS.get((stype, fmt))

    logger.info(
        "[%d/%d] %s (source_type=%s, format=%s)",
        idx,
        total,
        layer_id,
        stype,
        fmt,
    )

    if stype == "arcgis":
        raw = fetch_arcgis_vector(url)
        results = [
            (layer_id, gdf, src_epsg, wkid)
            for (_, gdf, src_epsg, wkid) in raw
        ]
    elif stype == "socrata":
        raw = helper_fn(url, app_token=socrata_token)
        results = [
            (layer_id, gdf, src_epsg, None)
            for (_, gdf, src_epsg) in raw
        ]
    else:
        raw = helper_fn(url)
        results = [
            (layer_id, gdf, src_epsg, None)
            for (_, gdf, src_epsg) in raw
        ]

    for raw_name, gdf, source_epsg, service_wkid in results:
        clean_name = sanitize_layer_name(raw_name)

        if db_engine:
            record_layer_metadata_db(
                db_engine,
                clean_name,
                url,
                source_epsg,
                service_wkid,
            )
            gdf.to_postgis(
                clean_name,
                db_engine,
                if_exists="replace",
                index=False,
            )
        else:
            record_layer_metadata_csv(
                metadata_csv,
                clean_name,
                url,
                source_epsg,
                service_wkid,
            )
            export_spatial_layer(gdf, clean_name, gpkg)


def finalize(gpkg, metadata_csv, output_epsg):
    """Reproject all layers in the GeoPackage to the target EPSG."""
    if gpkg and metadata_csv:
        reproject_all_layers(gpkg, metadata_csv, target_epsg=output_epsg)


def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
    )
    socrata_token, db_engine, gpkg, metadata_csv, output_epsg = (
        setup_destinations()
    )
    layers = load_layer_list()
    total = len(layers)
    for idx, layer in enumerate(layers, start=1):
        process_layer(
            layer,
            idx,
            total,
            socrata_token,
            db_engine,
            gpkg,
            metadata_csv,
        )
    finalize(gpkg, metadata_csv, output_epsg)


if __name__ == "__main__":
    main()
```

## src/temp/political_boundary.py

```python
"""
tempp
"""
from pathlib import Path
import geopandas as gpd
from sqlalchemy import create_engine
from shapely.ops import unary_union
from stp.config_loader import get_setting, get_constant
from stp.config_loader import get_setting, get_constant

# 1) Paths and config
base_dir = Path.cwd()
db_cfg = get_setting("db", {})
output_epsg = get_setting("data.output_epsg", get_constant("nysp_epsg", 2263))
output_dir = Path(get_setting("data.output_shapefile", "Data/shapefiles"))
output_dir.mkdir(parents=True, exist_ok=True)

# 2) Setup storage mode
engine = None
if db_cfg.get("enabled"):
    conn_url = (
        f"{db_cfg['driver']}://{db_cfg['user']}:{db_cfg['password']}@"
        f"{db_cfg['host']}:{db_cfg['port']}/{db_cfg['database']}"
    )
    engine = create_engine(conn_url)
else:
    gpkg_path = output_dir / get_constant(
        "default_gpkg_name", "project_data.gpkg"
    )
    if gpkg_path.exists():
        gpkg_path.unlink()

# 3) Define layers to process
layer_ids = [
    "borough", "community_districts", "city_council_districts",
    "us_congressional_districts", "state_senate_districts",
    "state_assembly_districts", "community_district_tabulation_areas",
    "neighborhood_tabulation_areas", "census_tracts", "census_blocks",
    "zoning_districts", "commercial_districts", "special_purpose_districts"
]

# 4) Load each layer into GeoDataFrames
gdfs = []
for layer in layer_ids:
    if engine:
        # Read from PostGIS
        gdf = gpd.read_postgis(
            f"SELECT * FROM {layer}", engine, geom_col="geometry"
        )
        gdf = gpd.read_postgis(
            f"SELECT * FROM {layer}", engine, geom_col="geometry"
        )
        gdf.set_crs(epsg=output_epsg, inplace=True)
    else:
        # Read from GeoPackage
        gdf = gpd.read_file(gpkg_path, layer=layer)
    gdfs.append(gdf)

# 5) Union all boundaries
all_union = unary_union([gdf.unary_union for gdf in gdfs])
result_gdf = gpd.GeoDataFrame([{"geometry": all_union}], crs=output_epsg)

# 6) Persist the result
if engine:
    result_gdf.to_postgis(
        "political_boundaries", engine, if_exists="replace", index=False
    )
else:
    result_gdf.to_file(
        gpkg_path, layer="political_boundaries", driver="GPKG"
    )
print("‚úÖ political_boundaries created")
```

## stp_repo.md

``````markdown
This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-07-11 12:16:40

# File Summary

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
   a. A header with the file path (## File: path/to/file)
   b. The full contents of the file in a code block

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Information:

For more information about Repomix, visit: https://github.com/andersonby/python-repomix


# Repository Structure

```
.codexignore
.gitignore
AGENTS.md
config
  defaults.yaml
  sources.json
  user.example.yaml
  user.yaml
  worfklow.yaml
data
  gpkg
    _gpkg.md
  table
    _table.md
docker.yaml
docs
  todo.md
License
pyproject.toml
README.md
requirements.txt
requirements_dev.txt
scrap
  01.md
  02.json
  03.md
  tokens.py
  zip.py
secrets
  secrets.yaml
src
  arcpy
    arcpy converted
      curb.py
      nostandingy.py
      rank.py
    arcpy depracated
      arcpy_First Step.py
      arcpy_nostanding.py
      arcpy_rank_dominant_working.py
      arcpy_Second_Step_Alternative.py
  stp
    cli
      stp_pipeline.py
    core
      config.py
      http.py
      settings.py
    fetch
      arcgis.py
      csv.py
      download.py
      gdb.py
      geojson.py
      gpkg.py
      lookup.py
      socrata.py
      __init__.py
    process
      clean
        address.py
        trees.py
        __init__.py
      custom_ops.py
      data_cleaning.py
      fields_inventory.py
      field_ops.py
      geometry_ops.py
      table.py
    record
      csv.py
      db.py
      export.py
      gpkg.py
      postgis.py
      __init__.py
    scaffold.md
    scrap.md
    storage
      db_storage.py
      file_storage.py
      __init__.py
    stp.md
    zip.py
    __init__.py
  temp
    download_data.py
    political_boundary.py
tests
  conftest.py
  test_address.py
  test_config_loader.py
  test_csv.py
  test_db_storage.py
  test_download.py
  test_file_storage.py
  test_geojson.py
  test_http_client.py
  test_trees.py
```

# Repository Files


## .codexignore

```text
# ignore VCS
.git/
# ignore virtual environments
.venv/
env/
# ignore data blobs
Data/
docs/
# ignore miscellaneous
*.tar
*.zip
AGENTS.md
hello.txt
scratch.py
```

## .gitignore

```text
# -------------------------------
# üêç Python build artifacts
# -------------------------------
__pycache__/
*.py[cod]
.venv/
.env/
.pytest_cache
*.log
/scrap

# -------------------------------
# üß™ Project-specific junk & output
# -------------------------------
*.tar
*.zip
project.zip
config/user.yaml
tokens.py
# -------------------------------
# üßæ Secret or local-only folders
# -------------------------------
secrets/

# -------------------------------
# üìä Data outputs (selectively ignored)
# -------------------------------
Data/table/*
Data/gpkg/*

# Keep placeholder files and readmes
!Data/table/
!Data/gpkg/
!Data/table/table.md
!Data/gpkg/gpkg.md

# -------------------------------
# üîß Developer tools/utilities
# -------------------------------
zip.py
project.zip
```

## AGENTS.md

````markdown
# AGENTS.md

## ‚úèÔ∏è Code Style Guide

This project enforces basic Python style conventions across `src/stp/`.

---

### üîπ Formatting Rules

- **Indentation**: 4 spaces  
- **Line length**: max 79 characters  
- **No trailing whitespace**  
- **Blank lines**:  
  - 2 between top-level functions and classes  
  - 1 between method definitions inside classes  

---

### üîπ Import Order

1. **Standard library**
2. **Third-party**
3. **Local modules** (`src/stp/...`)

Use `isort` or arrange manually to match.

---

### üîπ Docstrings

- Required on all **public functions** and **classes**
- Use triple double quotes (`"""docstring"""`)
- Be concise and descriptive

---

### üîπ Comments

- Use inline `#` comments sparingly ‚Äî only for **non-obvious logic**
- Avoid restating what the code already expresses clearly

---

### üîπ Enforcement

You can check for style issues using:

```bash
flake8 src/stp/ --max-line-length=79
pylint src/stp/
```

---

### ‚ùå Exclusions

These folders are not checked by linters:

- `.venv/`
- `Data/`
- `config/`
- `tests/gis_*`

---

This file focuses only on style. Testing, CI, and agents are optional layers you can add later.
````

## config/defaults.yaml

```yaml
epsg:
  default: 4326
  nysp: 2263

limits:
  socrata: 50000
  arcgis_default_max_records: 1000

validation:
  layer_name_max_length: 60
  min_dbh: 0.01
```

## config/sources.json

```json
[
    {
      "id": "trees",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/hn5i-inap.json",
      "schema": "political"
    },
    {
      "id": "work_orders",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/bdjm-n7q4.json",
      "schema": "political"
    },
    {
      "id": "planting_spaces",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/82zj-84is.json",
      "schema": "political"
    },
    {
      "id": "street_sign",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/qt6m-xctn.json",
      "schema": "political"
    },
    {
      "id": "hydrants",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/5bgh-vtsn.json",
      "schema": "political"
    },
    {
      "id": "green_infrastructure",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/df32-vzax.json",
      "schema": "political"
    },
    {
      "id": "subway_lines",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services.arcgis.com/ue9rwulIoeLEI9bj/arcgis/rest/services/NYC_SubwayLines/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "borough",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "community_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Community_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "council_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_City_Council_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "congressional_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/ArcGIS/rest/services/NYC_Congressional_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "senate_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_State_Senate_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "assembly_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_State_Assembly_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "community_tabulations",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Community_District_Tabulation_Areas_2020/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "neighborhood_tabulations",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Neighborhood_Tabulation_Areas_2020/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "census_tracts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Tracts_for_2020_US_Census/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "census_blocks",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Blocks_for_2020_US_Census/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "zoning_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyzd/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "commercial_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyco/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "special_purpose_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nysp/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "pluto",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/MAPPLUTO/FeatureServer/0",
      "schema": "land_use"
    },
    {
      "id": "street_center",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/DCM_Street_Center_Line/FeatureServer/0",
      "schema": "infrastructure"
    },
    {
      "id": "curb",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Curb_2022/FeatureServer/4",
      "schema": "infrastructure"
    },
    {
      "id": "curb_cut",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/ArcGIS/rest/services/Curb_Cut_2022/FeatureServer/5",
      "schema": "infrastructure"
    },
    {
      "id": "sidewalk",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Sidewalk_2022/FeatureServer/22",
      "schema": "infrastructure"
    }
  ]
```

## config/user.example.yaml

```yaml
epsg:
  default: 4326
  nysp: 2263

limits:
  socrata: 50000
  arcgis_default_max_records: 1000

validation:
  layer_name_max_length: 60
  min_dbh: 0.01
```

## config/user.yaml

```yaml
epsg:
  default: 4326
  nysp: 2263

limits:
  socrata: 50000
  arcgis_default_max_records: 1000

validation:
  layer_name_max_length: 60
  min_dbh: 0.01

paths:
  config:   "Config/config.yaml"
  constants: "Config/constants.yaml"
  output:
    shapefiles: "Data/shapefiles"
    tables:     "Data/tables"

filenames:
  data_inventory:  "data_inventory.csv"
  default_gpkg:    "project_data.gpkg"
```

## config/worfklow.yaml

```yaml
# Unified NYC Tree Planting Workflow Config (combined from your 01.md ops, 02.json sources, 03.md filters)
# Edit for toggles, other cities, or reordering. Pipeline reads this to run dynamically.

sources:  # From 02.json (URLs/types/schema) + 03.md (filters/simple types)
  trees:
    type: "json"  # From 03.md
    source_type: "socrata"  # From 02.json
    format: "json"
    url: "https://data.cityofnewyork.us/resource/hn5i-inap.json"
    schema: "political"
    filter: "SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')"  # From 03.md
    to_layer: "trees_raw"  # Initial save in GPKG
    enabled: true

  work_orders:
    type: "json"
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/bdjm-n7q4.json"
    schema: "political"
    filter: "STATUS <> 'Cancelled'"
    to_layer: "work_orders_raw"
    enabled: true

  planting_spaces:
    type: "json"  # Assumed from 03.md pattern (not listed, but in scaffold)
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/82zj-84is.json"
    schema: "political"
    filter: null  # None specified
    to_layer: "planting_spaces_raw"
    enabled: true

  street_sign:
    type: "json"
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/nfid-uabd.json"  # Updated from search (old qt6m-xctn invalid)
    schema: "political"
    filter: null  # Or "$where=sign_desc like '%NO STANDING%' OR sign_desc like '%NO PARKING%'" if pre-filtering; done in nostanding.py otherwise
    to_layer: "street_sign_raw"
    enabled: true

  hydrants:
    type: "json"
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/5bgh-vtsn.json"
    schema: "political"
    filter: null
    to_layer: "hydrants_raw"
    enabled: true

  green_infrastructure:
    type: "json"
    source_type: "socrata"
    format: "json"
    url: "https://data.cityofnewyork.us/resource/df32-vzax.json"  # Confirmed current
    schema: "political"
    filter: null
    to_layer: "green_infrastructure_raw"
    enabled: true

  subway_lines:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services.arcgis.com/ue9rwulIoeLEI9bj/arcgis/rest/services/NYC_SubwayLines/FeatureServer/0"
    schema: "political"
    filter: null
    to_layer: "subway_lines_raw"
    enabled: true

  # ... (Adding the rest from 02.json/03.md similarly; I shortened for space, but include all in your file)
  borough:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary/FeatureServer/0"
    schema: "political"
    filter: null
    to_layer: "borough_raw"
    enabled: true

  # community_districts, council_districts, etc. - follow pattern above

  census_blocks:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Blocks_for_2020_US_Census/FeatureServer/0"
    schema: "political"
    filter: null
    to_layer: "census_blocks_raw"
    enabled: true

  zoning_districts:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyzd/FeatureServer/0"
    schema: "zoning"
    filter: null  # Filter in ops
    to_layer: "zoning_districts_raw"
    enabled: true

  # commercial_districts, special_purpose_districts, pluto, street_center, curb, curb_cut, sidewalk - similar

  sidewalk:
    type: "shapefile"
    source_type: "arcgis"
    format: "shapefile"
    url: "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Sidewalk_2022/FeatureServer/22"
    schema: "infrastructure"
    filter: null
    to_layer: "sidewalk_raw"
    enabled: true

  grass_shrub:  # Added from search (2017 land cover raster for Grass_Shrub ops)
    type: "raster"  # Special handling (download GeoTIFF, convert to poly)
    source_type: "socrata"
    format: "geotiff"  # Export from Socrata
    url: "https://data.cityofnewyork.us/api/geospatial/he6d-2qns?method=export&format=GeoTIFF"  # Or Shapefile if preferred
    schema: "land_use"
    filter: null
    to_layer: "grass_shrub_raw"  # Save as raster or convert early
    enabled: true

prep_ops:  # From 01.md - ops per layer (description, filters if not in sources, steps)
  nyzd:  # Maps to zoning_districts source
    description: "Zoning districts where planting is prohibited"
    filter: "ZONE_DIST IN ('M1','M2','M3','IG','IH')"  # Applied in select step
    enabled: true
    steps:
      - op: copy
        input: "zoning_districts_raw"
        output: "nyzd_copy"
      - op: select
        query: "ZONE_DIST IN ('M1','M2','M3','IG','IH')"
        output: "nyzd_ready"

  dep_gi_assets:  # Maps to green_infrastructure
    description: "Green infrastructure assets to avoid"
    filter: null
    enabled: true
    steps:
      - op: copy
        input: "green_infrastructure_raw"
        output: "dep_gi_assets_copy"
      - op: buffer
        distance: 20  # Feet
        output: "dep_gi_assets_ready"

  sidewalk:
    description: "Raw sidewalk polygons, split for different logic"
    filter: null
    enabled: true
    steps:
      - op: copy
        input: "sidewalk_raw"
        output: "sidewalk_copy"
      - op: polygon_to_polyline
        input: "sidewalk_copy"
        output: "sidewalk_1"
      - op: split_immutable_mutable  # Custom? (Assume function for splitting)
        input: "sidewalk_1"
        outputs: ["sidewalk_immutable_ready", "sidewalk_mutable_ready"]

  curb_cuts:  # Maps to curb_cut (for intersections)
    description: "Sidewalk curb-cut intersections"
    filter: "SUB_FEATURE_CODE = 222700"
    enabled: true
    steps:
      - op: copy
        input: "curb_cut_raw"
        output: "curb_cuts_copy"
      - op: select
        query: "SUB_FEATURE_CODE = 222700"
        output: "curb_cuts_1"
      - op: buffer
        distance: 30
        output: "curb_cuts_ready"

  # curb_cuts_driveways: Separate group for driveway filter (same source as curb_cuts, but different filter/ops)
  curb_cuts_driveways:
    description: "Driveway curb cuts to exclude planting"
    filter: "SUB_FEATURE_CODE = 222600"
    enabled: true
    steps:
      - op: copy
        input: "curb_cut_raw"  # Reuse source
        output: "curb_cuts_driveways_copy"
      - op: select
        query: "SUB_FEATURE_CODE = 222600"
        output: "curb_cuts_driveways_1"
      - op: buffer
        distance: 15
        output: "curb_cuts_driveways_ready"

  subway_lines:
    description: "Subway buffers to exclude planting near tracks"
    filter: null
    enabled: true
    steps:
      - op: copy
        input: "subway_lines_raw"
        output: "subway_lines_copy"
      - op: buffer
        distance: 80
        output: "subway_lines_ready"

  workorders:
    description: "Active DOT work orders"
    filter: "STATUS <> 'Cancelled'"  # Already in sources, but repeated for clarity
    enabled: true
    steps:
      - op: copy  # Or export_table + select
        input: "work_orders_raw"
        output: "workorders_copy"
      - op: xy_to_point
        input: "workorders_copy"
        output: "workorders_1"
      - op: buffer
        distance: 25
        output: "workorders_ready"

  treeandsite:  # Maps to trees (or planting_spaces? Assumed trees based on filter)
    description: "Existing tree locations to avoid"
    filter: "SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')"
    enabled: true
    steps:
      - op: copy  # Export_table + select
        input: "trees_raw"
        output: "treeandsite_copy"
      - op: xy_to_point
        input: "treeandsite_copy"
        output: "treeandsite_1"
      - op: buffer
        distance: 25
        output: "treeandsite_ready"

  grass_shrub:
    description: "2017 land-use raster converted to shrub/grass polygons"
    filter: null
    enabled: true
    steps:
      - op: copy  # Copy raster
        input: "grass_shrub_raw"
        output: "grass_shrub_copy"
      - op: raster_to_polygon
        simplify: false
        output: "grass_shrub_1"
      - op: delete_field
        fields: ["gridcode", "Id"]
        output: "grass_shrub_ready"

  political_boundary:  # Composite (no direct source; unions others)
    description: "Union of all political boundaries for spatial join"
    filter: null
    enabled: true
    steps:
      - op: copy  # Or direct union
        input: null  # No single input
        output: "political_boundary_copy"
      - op: union
        inputs: ["borough_raw", "community_districts_raw", "council_districts_raw", "congressional_districts_raw", "senate_districts_raw", "assembly_districts_raw", "community_tabulations_raw", "neighborhood_tabulations_raw", "census_tracts_raw"]  # From scaffold
        output: "political_boundary_1"
      - op: delete_field
        fields: ["FID_*"]  # All FID_ fields
        output: "political_boundary_ready"

  street_centerline:  # Maps to street_center
    description: "Simplify street geometry and buffer vertices"
    filter: "L_LOW_HN IS NOT NULL"
    enabled: true
    steps:
      - op: copy
        input: "street_center_raw"
        output: "street_centerline_copy"
      - op: simplify_line
        method: "POINT_REMOVE"
        tolerance: 1  # Feet
        output: "street_centerline_1"
      - op: vertices_to_points
        input: "street_centerline_1"
        output: "street_centerline_2"
      - op: buffer
        distance: 40
        output: "street_centerline_ready"

  dep_hydrants:  # Maps to hydrants
    description: "Hydrant proximity adjustments"
    filter: null
    enabled: true
    steps:
      - op: copy
        input: "hydrants_raw"
        output: "dep_hydrants_copy"
      - op: generate_near_table
        distance: 10
        output: "dep_hydrants_1"
      - op: move_street_signs  # Custom script/tool
        input: "dep_hydrants_1"
        output: "dep_hydrants_2"
      - op: buffer
        distance: 3
        output: "dep_hydrants_ready"

final_clip:  # From 01.md end - separate section for pipeline end
  description: "Compute allowable planting points within clipped sidewalk"
  filter: null
  enabled: true
  steps:
    - op: copy
      input: "sidewalk_mutable_ready"
      output: "sidewalk_mutable_backup"
    - op: union
      inputs: ["nyzd_ready", "dep_gi_assets_ready", "curb_cuts_ready", "subway_lines_ready", "workorders_ready", "treeandsite_ready", "grass_shrub_ready"]
      output: "no_plant_zones_union"
    - op: clip
      input: "sidewalk_mutable_ready"
      clip_with: "no_plant_zones_union"
      output: "sidewalk_availability_ready"
    - op: create_points  # Custom: Fishnet or generate along lines
      input: "sidewalk_availability_ready"
      method: "fishnet"  # Or your rank_dominant
      output: "plant_pts_1"
    - op: spatial_join
      target: "plant_pts_1"
      join_features: "sidewalk_availability_ready"  # Plus political_boundary_ready?
      output: "plant_pts_ready"

big_scripts:  # Optional: Tie in nostanding/rank/curb (run after prep)
  nostanding:
    enabled: true
    input_csv: "street_sign_raw"  # Or path
    sw_layer: "sidewalk_mutable_ready"
    out_layer: "no_standing_ready"

  # rank_dominant, curb - as before
```

## data/gpkg/_gpkg.md

```markdown
# Data

## Data sources will be generated in this folder. Will be included in the gitignore because it doesn't need to be on github.
```

## data/table/_table.md

```markdown
# Data

## Helping data tables will be generated in this folder. Will be included in the gitignore because it doesn't need to be on github.
```

## docs/todo.md

```markdown
# TO DO 
## 7/6/2025
- [ ] I removed some items in user.example.yaml, so I need to make sure those removed variables aren't used elsewhere. 
- [ ] Scafffold a main.py within stp. Needs to follow the logic of the primary scope in README
- [ ] I need to find a way to utilize .env corerctly. Perhaps the best thing is to run a script on open, to check if .env is there, else create folder, create yaml with default inputs. 
- [ ] List out the steps of the project within the stp readme. 
- [ ] Parametize the pipeline for users. If they need to change when union happens in the model, figure it out.
- [ ] Create gui, for user to input parameters, select operations, input local features if they have it.
```

## License

```text
All Rights Reserved.

Copyright (c) 2025 Shawn Ganz.

This repository and its contents may not be used, copied, or distributed in any form without explicit written permission from the owner.

Unauthorized use, reproduction, or distribution is strictly prohibited.
```

## pyproject.toml

```text
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "stp"
version = "0.1.0"
description = "Street Tree Planting Analysis tools"
authors = [ { name="Shawn Ganz", email="ganz.shawn@gmail.com" } ]
readme = "README.md"
license = { file="LICENSE" }
requires-python = ">=3.10"
dependencies = [
    "geopandas>=0.14.0",
    "pandas>=2.0",
    "shapely>=2.0",
    "sqlalchemy>=2.0",
    "requests>=2.0",
    "pyproj",
    "psycopg2-binary",
]

[project.scripts]  # CLI entry points!
stp-download = "stp.download:main"  # Add a main() function to download.py if needed
stp-clean = "stp.data_cleaning:main"  # Same for others
stp-inventory = "stp.fields_inventory:main"
```

## README.md

````markdown
# Street Tree Planting Analysis (STP)

Scripts and tools for downloading, processing, and analyzing New York City street tree and sidewalk data.

---

## Table of Contents

* [Installation](#installation)
* [Environment Setup](#environment-setup)
* [Configuration](#configuration)
* [Folder Structure](#folder-structure)
* [Contributing](#contributing)
* [License](#license)

---

## Installation

1. **Clone the repository:**

   ```bash
   git clone https://github.com/shawnganznyc/Street_Tree_Planting_Analysis.git
   cd Street_Tree_Planting_Analysis
   ```

2. **Install dependencies**
pip install -r requirements.txt

## Configuration

1. **Edit configuration files:**

   * `config/defaults.yaml` ‚Äî project-wide constants (safe to commit)
   * `config/user.yaml` ‚Äî user-specific secrets/overrides (do **NOT** commit)

2. **On first use:**

   * Copy `config/user.example.yaml` to `config/user.yaml` and fill in any required secrets (e.g. Socrata app token).
   * Ensure `config/user.yaml` is listed in `.gitignore`.

---

## Folder Structure

```
config/      - Configuration files (defaults, user secrets)
src/stp/     - Main Python package (cleaning, fetchers, storage, etc.)
Data/        - Downloaded/generated data (not tracked in version control)
tests/       - Unit and integration tests
```

---

## Scope

1. Read user parameters
2. Download files
3. Create GIS assets if specified/possible
4. Clean
   * Read user settings per item to figure out what fields to keep. 
5. Manipulate
   * Shapes undergo geoprocessing functions
      * User specified options
         * ex: Buffer, 20 ft. 
   * Pipeline functions
      * union
      * clip
      * merge
      * spatial join
      * custom operations
6. Combine
7. Output
   * Number of potential trees in the area. 

# Primary Scope

1. Read established parameters
2. Download files from arcgis/nyc opendata
3. Convert json into geojson point/polygons
4. Clean files of unecessary fields
5. Apply buffers, filters, custom scripts. 
6. Create a sidewalk polyline using polygon buffer
   - Copy non mutable sidewalk polyline
7. Merge all polygons where plantings cannot occur. 
8. Clip do not plant locations with sidewalk polyline. 
9. Use non mutable sidewalk polyline to use for traffic signs / parking rules
   - Build no parking zones
      - Classify no parking into bins: 
         - No Parking
         - No Standing
            - No Standing Taxi
            - No Standing Truck Loading
            - No Standing (something else, forgot)
   - Build sidewalk vehicle parking times into each sidewalk 
   - Build mta no bus locations/rules
10. Generate potential planting locations
11. Join potential planting location with sidewalk information
12. Done.

# Current Goal
- Functioning python pipeline
   - Pipeline already developed on arcpy.
- User parameter control
   - Allow user to manipulate variables/parameters. 
      - Atm the original project use parameters and variables are somewhat set. 
- Automatic pipeline based on geography
   - All inputs need to be faithful to the pipeline, otherwise errors will occur
   - Template settings for all major cities? 

## Contributing

Pull requests and issues are welcome. Please open an issue to discuss major changes before submitting a PR.

---

## License

This project is open-source. See `LICENSE` for details.

---

**Tip:**
All user secrets and API keys go in `config/user.yaml` (never committed).
Project settings and defaults live in `config/defaults.yaml`.

---

Let me know if you want any sections expanded or customized for your onboarding workflow.
````

## requirements.txt

```text
fiona==1.10.1
geopandas==1.1.1
pandas==2.3.0
python-dotenv==1.1.1
PyYAML==6.0.2
PyYAML==6.0.2
Requests==2.32.4
Shapely==2.1.1
SQLAlchemy==2.0.41
```

## scrap/01.md

```markdown
- NYZD  
  - Description: Zoning districts where planting is prohibited  
  - Filters: `"ZONE_DIST" IN ('M1','M2','M3','IG','IH')`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `nyzd_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `nyzd_ready`

- DEP_GI_Assets  
  - Description: Green infrastructure assets to avoid  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `dep_gi_assets_copy`  
    - Operation b: Buffer  
      - Params: distance = 20 Feet  
      - Output: `dep_gi_assets_ready`

- Sidewalk  
  - Description: Raw sidewalk polygons, split for different logic  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `sidewalk_copy`  
    - Operation b: Polygon‚ÜíPolyline  
      - Params: input = `sidewalk_copy`  
      - Output: `sidewalk_1`  
    - Operation c: Split immutable/mutable  
      - Params: input = `sidewalk_1`  
      - Outputs: `sidewalk_immutable_ready`, `sidewalk_mutable_ready`

- Curb_Cuts  
  - Description: Sidewalk curb-cut intersections  
  - Filters: `SUB_FEATURE_CODE = 222700`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `curb_cuts_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `curb_cuts_1`  
    - Operation c: Buffer  
      - Params: input = `curb_cuts_1`, distance = 30 Feet  
      - Output: `curb_cuts_ready`

- Subway_Lines  
  - Description: Subway buffers to exclude planting near tracks  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `subway_lines_copy`  
    - Operation b: Buffer  
      - Params: distance = 80 Feet  
      - Output: `subway_lines_ready`

- Workorders  
  - Description: Active DOT work orders  
  - Filters: `STATUS <> 'Cancelled'`  
  - Operations  
    - Operation a: CopyFeatures (ExportTable/Select)  
      - Output: `workorders_copy`  
    - Operation b: XYTableToPoint  
      - Params: input = `workorders_copy`  
      - Output: `workorders_1`  
    - Operation c: Buffer  
      - Params: input = `workorders_1`, distance = 25 Feet  
      - Output: `workorders_ready`

- TreeandSite  
  - Description: Existing tree locations to avoid  
  - Filters: `SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')`  
  - Operations  
    - Operation a: CopyFeatures (ExportTable/Select)  
      - Output: `treeandsite_copy`  
    - Operation b: XYTableToPoint  
      - Params: input = `treeandsite_copy`  
      - Output: `treeandsite_1`  
    - Operation c: Buffer  
      - Params: input = `treeandsite_1`, distance = 25 Feet  
      - Output: `treeandsite_ready`

- Grass_Shrub  
  - Description: 2017 land-use raster converted to shrub/grass polygons  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures (Copy Raster)  
      - Output: `grass_shrub_copy`  
    - Operation b: RasterToPolygon_conversion  
      - Params: simplify = NO_SIMPLIFY  
      - Output: `grass_shrub_1`  
    - Operation c: DeleteField  
      - Params: fields = `["gridcode","Id"]`  
      - Output: `grass_shrub_ready`

- Political_Boundary  
  - Description: Union of all political boundaries for spatial join  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `political_boundary_copy`  
    - Operation b: Union_analysis  
      - Params: inputs = borough, community boards, council, senate, assembly, NTA, tracts  
      - Output: `political_boundary_1`  
    - Operation c: DeleteField  
      - Params: fields = all `FID_*`  
      - Output: `political_boundary_ready`

- Street_Centerline  
  - Description: Simplify street geometry and buffer vertices  
  - Filters: `L_LOW_HN IS NOT NULL`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `street_centerline_copy`  
    - Operation b: SimplifyLine_cartography  
      - Params: method = POINT_REMOVE, tolerance = 1 Feet  
      - Output: `street_centerline_1`  
    - Operation c: FeatureVerticesToPoints  
      - Params: input = `street_centerline_1`  
      - Output: `street_centerline_2`  
    - Operation d: Buffer  
      - Params: input = `street_centerline_2`, distance = 40 Feet  
      - Output: `street_centerline_ready`

- DEP_Hydrants  
  - Description: Hydrant proximity adjustments  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `dep_hydrants_copy`  
    - Operation b: GenerateNearTable  
      - Params: distance = 10 Feet  
      - Output: `dep_hydrants_1`  
    - Operation c: MoveStreetSigns (custom)  
      - Params: as defined in script  
      - Output: `dep_hydrants_2`  
    - Operation d: Buffer  
      - Params: input = `dep_hydrants_2`, distance = 3 Feet  
      - Output: `dep_hydrants_ready`

- Curb_Cuts_Driveways  
  - Description: Driveway curb cuts to exclude planting  
  - Filters: `SUB_FEATURE_CODE = 222600`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `curb_cuts_driveways_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `curb_cuts_driveways_1`  
    - Operation c: Buffer  
      - Params: input = `curb_cuts_driveways_1`, distance = 15 Feet  
      - Output: `curb_cuts_driveways_ready`

## Final first step

- Final Clip  
  - Description: Compute allowable planting points within clipped sidewalk  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures (non-mutable backup)  
      - Params: input = `sidewalk_mutable_ready`  
      - Output: `sidewalk_mutable_backup`  
    - Operation b: Union no-plant zones  
      - Params: inputs = nyzd_ready, dep_gi_assets_ready, curb_cuts_ready, subway_lines_ready, workorders_ready, treeandsite_ready, grass_shrub_ready  
      - Output: `no_plant_zones_union`  
    - Operation c: Clip_analysis  
      - Params: input = `sidewalk_mutable_ready`, clip_features = `no_plant_zones_union`  
      - Output: `sidewalk_availability_ready`  
    - Operation d: Create planting points  
      - Params: method = CreateFishnet or custom  
      - Output: `plant_pts_1`  
    - Operation e: SpatialJoin  
      - Params: target = `plant_pts_1`, join_features = `sidewalk_availability_ready`  
      - Output: `plant_pts_ready`
```

## scrap/02.json

```json
[
    {
      "id": "trees",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/hn5i-inap.json",
      "schema": "political"
    },
    {
      "id": "work_orders",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/bdjm-n7q4.json",
      "schema": "political"
    },
    {
      "id": "planting_spaces",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/82zj-84is.json",
      "schema": "political"
    },
    {
      "id": "street_sign",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/qt6m-xctn.json",
      "schema": "political"
    },
    {
      "id": "hydrants",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/5bgh-vtsn.json",
      "schema": "political"
    },
    {
      "id": "green_infrastructure",
      "source_type": "socrata",
      "format": "json",
      "url": "https://data.cityofnewyork.us/resource/df32-vzax.json",
      "schema": "political"
    },
    {
      "id": "subway_lines",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services.arcgis.com/ue9rwulIoeLEI9bj/arcgis/rest/services/NYC_SubwayLines/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "borough",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "community_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Community_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "council_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_City_Council_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "congressional_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/ArcGIS/rest/services/NYC_Congressional_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "senate_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_State_Senate_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "assembly_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_State_Assembly_Districts/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "community_tabulations",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Community_District_Tabulation_Areas_2020/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "neighborhood_tabulations",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Neighborhood_Tabulation_Areas_2020/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "census_tracts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Tracts_for_2020_US_Census/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "census_blocks",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Blocks_for_2020_US_Census/FeatureServer/0",
      "schema": "political"
    },
    {
      "id": "zoning_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyzd/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "commercial_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nyco/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "special_purpose_districts",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nysp/FeatureServer/0",
      "schema": "zoning"
    },
    {
      "id": "pluto",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/MAPPLUTO/FeatureServer/0",
      "schema": "land_use"
    },
    {
      "id": "street_center",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/DCM_Street_Center_Line/FeatureServer/0",
      "schema": "infrastructure"
    },
    {
      "id": "curb",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Curb_2022/FeatureServer/4",
      "schema": "infrastructure"
    },
    {
      "id": "curb_cut",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/ArcGIS/rest/services/Curb_Cut_2022/FeatureServer/5",
      "schema": "infrastructure"
    },
    {
      "id": "sidewalk",
      "source_type": "arcgis",
      "format": "shapefile",
      "url": "https://services6.arcgis.com/yG5s3afENB5iO9fj/arcgis/rest/services/Sidewalk_2022/FeatureServer/22",
      "schema": "infrastructure"
    }
  ]
```

## scrap/03.md

```markdown
2.1 Define source paths & queries:
   - All this information can be found within root/config/sources.json
    - trees
        - type: json
        - filter: `SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')` 

    - work_orders
        - type: json
        - filter: `STATUS <> 'Cancelled'`

    - street_sign
        - type: json
        - filter: *Need help*

    - hydrants
        - type: json
        - filter: None

    - green_infrastructure
        - type: json

    - subway_lines
        - shapefile

    - borough
        - shapefile

    - community_districts
        - shapefile

    - council_districts
        - shapefile

    - congressional_districts
        - shapefile

    - senate_districts
        - shapefile

    - assembly_districts
        - shapefile

    - community_tabulations
        - shapefile

    - neighborhood_tabulations
        - shapefile

    - census_tracts
        - shapefile

    - census_blocks
        - shapefile

    - zoning_districts
        - shapefile

    - commercial_districts
        - shapefile

    - special_purpose_districts
        - shapefile

    - pluto
        - shapefile

    - street_center
        - shapefile

    - curb
        - shapefile

    - curb_cut
        - shapefile

    - sidewalk
        - shapefile
```

## scrap/tokens.py

```python
import pathlib, tiktoken

enc  = tiktoken.get_encoding("cl100k_base")
root = pathlib.Path(r"C:\Projects\stp\src")

total = 0
for p in root.rglob("*.py"):
    try:
        text = p.read_text(encoding="utf‚Äë8")
    except UnicodeDecodeError:
        # last‚Äëditch: read as Latin‚Äë1 and ignore bad bytes
        text = p.read_text(encoding="latin‚Äë1", errors="ignore")
    total += len(enc.encode(text))

print(f"{total:,} tokens in {root}")
```

## scrap/zip.py

```python
"""
Zip up this project (no enclosing folder), excluding .git, .venv, env,
project.zip, and any .DS_Store files.
"""
import os
import zipfile

# Files or folders to skip entirely
EXCLUDES = {'.git', '.venv', 'env', 'project.zip'}
# Project-root name for the output archive
OUTPUT = 'project.zip'


def should_exclude(path: str) -> bool:
    """ Exclusions"""
    parts = path.split(os.sep)
    # skip any path containing an excluded folder
    if any(p in EXCLUDES for p in parts):
        return True
    # skip macOS DS_Store files
    if path.endswith('.DS_Store'):
        return True
    return False


def main() -> None:
    """ runs zip"""
    # remove old archive if it exists
    try:
        os.remove(OUTPUT)
    except OSError:
        pass

    with zipfile.ZipFile(OUTPUT, 'w', zipfile.ZIP_DEFLATED) as zf:
        for root, dirs, files in os.walk('.'):
            # prune dirs in-place so we never descend into .git/.venv/etc.
            dirs[:] = [d for d in dirs if d not in EXCLUDES]
            for fname in files:
                full = os.path.join(root, fname)
                if should_exclude(full):
                    continue
                # store relative path so no leading "./"
                arc = os.path.relpath(full, '.')
                zf.write(full, arc)


if __name__ == '__main__':
    main()
```

## secrets/secrets.yaml

```yaml
Socrate api: "7dUkig8gydigidDN6G638J8Lr"
```

## src/arcpy/arcpy converted/curb.py

```python
"""Generate curb polygons around lines using GeoPandas/Shapely."""
import math
from pathlib import Path

import geopandas as gpd
import yaml
from shapely.geometry import Polygon

def get_dominant_segment_angle(line):
    """Return angle (radians) of the longest segment in a LineString."""
    max_len = 0.0
    best_angle = 0.0
    coords = list(line.coords)
    for (x1, y1), (x2, y2) in zip(coords[:-1], coords[1:]):
        dx = x2 - x1
        dy = y2 - y1
        seg_len = math.hypot(dx, dy)
        if seg_len > max_len:
            max_len = seg_len
            best_angle = math.atan2(dy, dx)
    return best_angle

def generate_polygons(lines_gdf, extension_distance, buffer_width):
    """Return GeoDataFrame of rectangles around each line."""
    polys = []
    for line in lines_gdf.geometry:
        if line is None or len(line.coords) < 2:
            continue

        angle = get_dominant_segment_angle(line)
        sx, sy = line.coords[0]
        ex, ey = line.coords[-1]

        # Extend line ends
        sx -= extension_distance * math.cos(angle)
        sy -= extension_distance * math.sin(angle)
        ex += extension_distance * math.cos(angle)
        ey += extension_distance * math.sin(angle)

        # Buffer offset
        dx = (buffer_width / 2.0) * math.sin(angle)
        dy = (buffer_width / 2.0) * math.cos(angle)

        points = [
            (sx - dx, sy + dy),
            (ex - dx, ey + dy),
            (ex + dx, ey - dy),
            (sx + dx, sy - dy),
        ]
        polys.append(Polygon(points))

    return gpd.GeoDataFrame({"geometry": polys}, crs=lines_gdf.crs)

def main():
    """Entry point for CLI usage: build curb buffer polygons."""
    base_dir = Path.cwd()
    cfg_path = base_dir / "config" / "config.yaml"
    with open(cfg_path) as f:
        config = yaml.safe_load(f)

    curb_cfg = config.get("curb", {})
    ext_dist = float(curb_cfg.get("extension_distance", 0))
    buff_width = float(curb_cfg.get("buffer_width", 0))

    out_dir = Path(config.get("output_shapefiles", "Data/shapefiles"))
    gpkg = out_dir / "project_data.gpkg"

    lines = gpd.read_file(gpkg, layer="curb")
    polys = generate_polygons(lines, ext_dist, buff_width)

    polys.to_file(gpkg, layer="curb_buffer", driver="GPKG")
    print(f"‚úÖ wrote {gpkg} layer 'curb_buffer'")

    return gpkg


if __name__ == "__main__":
    main()
```

## src/arcpy/arcpy converted/nostandingy.py

```python
"""GeoPandas version of nostanding.py: Flag no-standing areas on sidewalks."""

import logging

import geopandas as gpd
import pandas as pd
import yaml
from shapely.affinity import translate
from shapely.geometry import Point
from shapely.ops import snap

logging.basicConfig(level=logging.INFO)  # Simple logging


def classify(raw):
    """Short code for sign text (same as original)."""
    txt = str(raw).upper()
    if "NO STANDING" in txt:
        return "NSTAND"
    if "NO PARKING" in txt:
        return "NPARK"
    if "HMP" in txt:
        return "HMP"
    if "TAXI" in txt or "HOTEL" in txt or "LOADING" in txt or "PASSENGER" in txt:
        return "CURBSIDE"
    return "OTHER"


def load_filter(csv_path, desc_f, side_f):
    """Clean and filter sign DataFrame (same as original, but returns GDF)."""
    df = pd.read_csv(csv_path)
    df[side_f] = df[side_f].astype(str).str.strip().str.upper().str[0]
    df = df[df[side_f].isin(['N', 'S', 'E', 'W'])]
    keep = "NO STANDING|NO PARKING|HMP|TAXI|HOTEL|LOADING|PASSENGER"
    df = df[df[desc_f].str.upper().str.contains(keep, na=False)]
    df = df[pd.to_numeric(df["sign_x_coord"], errors='coerce').notna()]
    df = df[pd.to_numeric(df["sign_y_coord"], errors='coerce').notna()]
    df["x_r"] = df["sign_x_coord"].round(1)
    df["y_r"] = df["sign_y_coord"].round(1)
    df = df.drop_duplicates(subset=["x_r", "y_r"]).reset_index(drop=True)

    def _pa(s):
        s = str(s).upper()
        if "<->" in s:
            return "<->"
        elif "-->" in s or "->" in s:
            return "->"
        elif "<--" in s or "<-" in s:
            return "<-"
        return None
    df["parsed_arrow"] = df[desc_f].apply(_pa)
    df = df[df["parsed_arrow"].notna()]
    df["sign_type"] = df[desc_f].map(classify)
    df["geometry"] = df.apply(lambda row: Point(row["sign_x_coord"], row["sign_y_coord"]), axis=1)
    gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:2263')
    logging.info(f"Loaded and filtered {len(gdf)} signs.")
    return gdf


def shift_points(gdf, side_f, shift_ft):
    """Shift points off curb based on side."""
    def shift_pt(pt, side):
        if side == 'N':
            return translate(pt, 0, shift_ft)
        elif side == 'S':
            return translate(pt, 0, -shift_ft)
        elif side == 'E':
            return translate(pt, shift_ft, 0)
        elif side == 'W':
            return translate(pt, -shift_ft, 0)
        return pt
    gdf['geometry'] = gdf.apply(lambda row: shift_pt(row.geometry, row[side_f]), axis=1)
    logging.info("Shifted signs.")
    return gdf


def snap_to_sidewalks(signs_gdf, sw_gdf, tolerance=60):
    """Snap signs to nearest sidewalk within tolerance feet."""
    signs_gdf = gpd.sjoin_nearest(signs_gdf, sw_gdf, distance_col='near_dist', max_distance=tolerance)
    signs_gdf = signs_gdf[signs_gdf['near_dist'].notna()]  # Drop orphans
    
    def snap_geom(row):
        if pd.isna(row['near_dist']):
            return row.geometry
        line = sw_gdf.loc[row['index_right']].geometry
        return snap(row.geometry, line, tolerance)
    signs_gdf['geometry'] = signs_gdf.apply(snap_geom, axis=1)
    logging.info(f"Snapped {len(signs_gdf)} signs to sidewalks.")
    return signs_gdf


def segment_compass(seg_geom, sign_pt):
    """Compass direction relative to sign (north/south/east/west)."""
    dx = seg_geom.centroid.x - sign_pt.centroid.x
    dy = seg_geom.centroid.y - sign_pt.centroid.y
    if abs(dx) >= abs(dy):
        return "east" if dx > 0 else "west"
    return "north" if dy > 0 else "south"


def erase_gaps_and_flag(sw_gdf, signs_gdf, side_f):
    """Erase 3ft gaps at signs, explode, flag no_stand."""
    gaps = signs_gdf.buffer(1.5)  # 3ft diameter
    sw_split = sw_gdf.difference(gaps.unary_union)
    sw_split = sw_split.explode(ignore_index=True)  # Multipart to singlepart
    sw_split = sw_split[sw_split.length > 0.05]  # Drop slivers

    # Spatial join segments to signs (within 2ft)
    joined = gpd.sjoin(sw_split, signs_gdf, how='left', predicate='within', distance=2)
    
    # Add compass and flag
    arrow_to_compass = {
        "E": {"<-": "north", "->": "south"},
        "W": {"<-": "south", "->": "north"},
        "N": {"<-": "west",  "->": "east"},
        "S": {"<-": "east",  "->": "west"},
    }

    def get_compass(row):
        if row['parsed_arrow'] == "<->":
            return "both"
        curb = row[side_f].strip().upper() if pd.notna(row[side_f]) else ""
        arr = row['parsed_arrow'].strip() if pd.notna(row['parsed_arrow']) else ""
        return arrow_to_compass.get(curb, {}).get(arr)
    joined['compass'] = joined.apply(get_compass, axis=1)
    
    def get_flag(row):
        if row['compass'] == "both":
            return 1
        seg_side = segment_compass(row.geometry, signs_gdf.loc[row['index_right']].geometry)
        return 1 if seg_side == row['compass'] else 0
    joined['no_stand'] = joined.apply(get_flag, axis=1)
    
    # Aggregate max no_stand per segment (groupby)
    aggregated = joined.groupby(joined.index)['no_stand'].max().reset_index()
    sw_split['no_stand'] = aggregated['no_stand']
    
    logging.info("Erased gaps and flagged segments.")
    return sw_split


def main(csv_path, sw_path, out_path, desc_f='desc', side_f='side', shift_ft=5.0):
    """Run the full no-standing process."""
    try:
        with open('config/config.yaml') as f:  # Use your config
            config = yaml.safe_load(f)
        # Override with config if needed
        
        signs_gdf = load_filter(csv_path, desc_f, side_f)
        sw_gdf = gpd.read_file(sw_path)  # e.g., GeoPackage layer
        signs_gdf = shift_points(signs_gdf, side_f, shift_ft)
        signs_gdf = snap_to_sidewalks(signs_gdf, sw_gdf)
        final_gdf = erase_gaps_and_flag(sw_gdf, signs_gdf, side_f)
        final_gdf.to_file(out_path, driver='GPKG')
        logging.info(f"Done! Output: {out_path}")
    except Exception as e:
        logging.error(f"Oops: {e}")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Process no-standing signs.")
    parser.add_argument('--csv', default='data/signs.csv', help='Sign CSV path')
    parser.add_argument('--sw', default='data/project_data.gpkg|layer=sidewalks', help='Sidewalk GPKG')
    parser.add_argument('--out', default='data/no_stand.gpkg', help='Output GPKG')
    args = parser.parse_args()
    main(args.csv, args.sw, args.out)
```

## src/arcpy/arcpy converted/rank.py

```python
"""GeoPandas version of rank_dominant_working.py: Prune planting points."""

import logging
import geopandas as gpd
from shapely import line_interpolate_point, length

logging.basicConfig(level=logging.INFO)


def generate_points_along_lines(lines_gdf, spacing):
    """Create points every 'spacing' feet along lines."""
    points = []
    for idx, row in lines_gdf.iterrows():
        line = row.geometry
        dist = 0
        while dist < length(line):
            pt = line_interpolate_point(line, dist)
            points.append({'geometry': pt, 'parent_fid': idx, 'parent_len': length(line)})
            dist += spacing
    pts_gdf = gpd.GeoDataFrame(points, crs=lines_gdf.crs)
    logging.info(f"Generated {len(pts_gdf)} points.")
    return pts_gdf


def resolve_conflicts(pts_gdf, lines_gdf, spacing, buffer_dist, max_iter=3):
    """Iteratively prune conflicting points."""
    for iter_n in range(1, max_iter + 1):
        # Find near pairs (like GenerateNearTable)
        joined = gpd.sjoin_nearest(pts_gdf, pts_gdf, distance_col='dist', max_distance=buffer_dist)
        joined = joined[joined.index != joined['index_right']]  # No self-joins
        
        if joined.empty:
            break
        
        # Rank and pick winners/losers (longer parent wins)
        winners, losers = set(), set()
        for _, row in joined.iterrows():
            i, j = row.name, row['index_right']
            len_i, len_j = pts_gdf.loc[i]['parent_len'], pts_gdf.loc[j]['parent_len']
            if len_i > len_j or (len_i == len_j and i < j):
                winners.add(i)
                losers.add(j)
            else:
                winners.add(j)
                losers.add(i)
        
        if not losers:
            break
        
        # Buffer losers and erase from lines
        loser_buffers = pts_gdf.loc[list(losers)].buffer(buffer_dist - 0.01)
        lines_gdf = lines_gdf.difference(loser_buffers.unary_union)
        lines_gdf = lines_gdf[lines_gdf.length >= 3]  # Filter short lines
        
        # Regenerate points
        pts_gdf = generate_points_along_lines(lines_gdf, spacing)
    
    logging.info(f"Resolved after {iter_n} iterations.")
    return pts_gdf, lines_gdf


def main(line_path, out_pts_path, out_lines_path, spacing=25.0, buffer_dist=30.0):
    "Main"
    try:
        lines_gdf = gpd.read_file(line_path)
        pts_gdf = generate_points_along_lines(lines_gdf, spacing)
        final_pts, final_lines = resolve_conflicts(pts_gdf, lines_gdf, spacing, buffer_dist)
        final_pts.to_file(out_pts_path)
        final_lines.to_file(out_lines_path)
        logging.info("Done pruning!")
    except Exception as e:
        logging.error(f"Oops: {e}")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Prune planting points.")
    parser.add_argument('--lines', default='data/sidewalks.gpkg', help='Input lines GPKG')
    parser.add_argument('--out_pts', default='data/plant_points.gpkg', help='Output points')
    parser.add_argument('--out_lines', default='data/pruned_lines.gpkg', help='Output lines')
    parser.add_argument('--spacing', type=float, default=25.0)
    parser.add_argument('--buffer', type=float, default=30.0)
    args = parser.parse_args()
    main(args.lines, args.out_pts, args.out_lines, args.spacing, args.buffer)
```

## src/arcpy/arcpy depracated/arcpy_nostanding.py

```python
import os
import pandas as pd
import arcpy

arcpy.env.overwriteOutput = True
scratch = arcpy.env.scratchGDB

def classify(raw):
    """Return a short code describing the sign text."""
    txt = str(raw).upper()
    if "NO STANDING" in txt:
        return "NSTAND"
    if "NO PARKING" in txt:
        return "NPARK"
    if "HMP" in txt:
        return "HMP"
    if "TAXI" in txt or "HOTEL" in txt or "LOADING" in txt or "PASSENGER" in txt:
        return "CURBSIDE"
    return "OTHER"

def load_filter(csv_path, desc_f, side_f):
    """Return cleaned DataFrame of sign records filtered by text and side."""
    df = pd.read_csv(csv_path)
    
    # normalize N/S/E/W
    df[side_f] = (df[side_f]
                  .astype(str).str.strip()
                  .str.upper().str[0]
                  .where(lambda s: s.isin(list("NSEW"))))
    df = df[df[side_f].notna()]

    # keep only our keywords
    keep = "NO STANDING|NO PARKING|HMP|TAXI|HOTEL|LOADING|PASSENGER"
    df = df[df[desc_f].str.upper().str.contains(keep, na=False)]

    # valid coords
    df = df[pd.to_numeric(df["sign_x_coord"], "coerce").notna()]
    df = df[pd.to_numeric(df["sign_y_coord"], "coerce").notna()]

    # dedupe on 1-ft grid
    df["x_r"] = df["sign_x_coord"].round(1)
    df["y_r"] = df["sign_y_coord"].round(1)
    df = df.drop_duplicates(subset=["x_r", "y_r"]).reset_index(drop=True)

    # parse arrow glyphs
    def _pa(s):
        s = str(s).upper()
        if "<->" in s:
            return "<->"
        elif "-->" in s or "->" in s:
            return "->"
        elif "<--" in s or "<-" in s:
            return "<-"
    df["parsed_arrow"] = df[desc_f].apply(_pa)
    df = df[df["parsed_arrow"].notna()]

    return df

# helper that classifies the segment side
def segment_compass(seg_geom, sign_pt):
    """Return 'north', 'south', 'east', or 'west' relative to sign_pt."""
    dx = seg_geom.centroid.X - sign_pt.centroid.X
    dy = seg_geom.centroid.Y - sign_pt.centroid.Y
    if abs(dx) >= abs(dy):  # horizontal distance dominates or ties
        return "east" if dx > 0 else "west"
    return "north" if dy > 0 else "south"

# ‚îÄ‚îÄ 0. Params ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 0: csv_path, 1: sw_fc, 2: cen_fc, 3: out_fc, 4: desc_f, 5: side_f, 6: shift_ft
csv_path, sw_fc, cen_fc, out_fc, desc_f, side_f, shift_ft = [
    arcpy.GetParameterAsText(i) for i in range(7)
]
shift_ft = float(shift_ft)

# ‚îÄ‚îÄ 0.1 Copy & integrate centerlines (don‚Äôt mutate source)‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
cen_work = os.path.join(scratch, "cen_work")
arcpy.management.CopyFeatures(cen_fc, cen_work)
arcpy.management.Integrate(cen_work, "0.01 Feet")

# ‚îÄ‚îÄ 1.  CSV ‚Üí point feature class  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
df = load_filter(csv_path, desc_f, side_f)
df["sign_type"] = df[desc_f].map(classify)
df["jid"] = df.index
tmp_csv = os.path.join(scratch, "clean_signs.csv")
df.to_csv(tmp_csv, index=False,
          columns=["jid", "sign_x_coord", "sign_y_coord", side_f, "parsed_arrow","sign_type"])
arcpy.AddMessage(f"Cleaned signs CSV ‚Üí {tmp_csv}")

sr = arcpy.SpatialReference(2263)          # NY State Plane Feet
signs = os.path.join(scratch, "signs_pts")
arcpy.management.XYTableToPoint(tmp_csv, signs,
                                "sign_x_coord", "sign_y_coord",
                                coordinate_system=sr)
arcpy.AddMessage(f"Signs ‚Üí point FC: {signs}")

# ‚îÄ‚îÄ 2.  Curb offset  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
signs_shifted = os.path.join(scratch, "signs_shifted")
arcpy.management.CopyFeatures(signs, signs_shifted)

with arcpy.da.UpdateCursor(signs_shifted, ["SHAPE@", side_f]) as cur:
    for shp, sd in cur:
        dx, dy = {"N": (0,  shift_ft),
                  "S": (0, -shift_ft),
                  "E": ( shift_ft, 0),
                  "W": (-shift_ft, 0)}.get((sd or "").upper(), (0, 0))
        pt = shp.centroid
        new_pt = arcpy.PointGeometry(arcpy.Point(pt.X + dx, pt.Y + dy), sr)
        cur.updateRow([new_pt, sd])

arcpy.AddMessage("‚ÜîÔ∏è  Shifted signs off original location.")

# ‚îÄ‚îÄ 3.  Build working sidewalk copy  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
arcpy.env.overwriteOutput = True
sw_work = os.path.join(scratch, "sw_work")
if arcpy.Exists(sw_work):
    arcpy.Delete_management(sw_work)
arcpy.management.CopyFeatures(sw_fc, sw_work)
arcpy.AddMessage(f"Copied sidewalks ‚Üí {sw_work}")

# ‚îÄ‚îÄ 4.  Snap curb‚Äëshifted signs to their sidewalk  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
signs_snapped_sw = os.path.join(scratch, "signs_snapped_sw")
arcpy.management.CopyFeatures(signs_shifted, signs_snapped_sw)

near_sidewalk = os.path.join(scratch, "near_sidewalk")
arcpy.analysis.GenerateNearTable(signs_snapped_sw, sw_work,
                                 near_sidewalk, search_radius="60 Feet",
                                 closest="CLOSEST")

arcpy.management.JoinField(signs_snapped_sw,      # target
                           "OBJECTID",            # sign OID
                           near_sidewalk,         # near table
                           "IN_FID",              # join key
                           ["NEAR_FID"])          # adds curb sidewalk OID
arcpy.AddMessage("üìå  Joined NEAR_FID onto curb‚Äëshifted signs.")

# lookup:  sidewalk OID  ‚Üí  geometry
sw_geom = {oid: g for oid, g in arcpy.da.SearchCursor(sw_work, ["OID@", "SHAPE@"])}

with arcpy.da.UpdateCursor(signs_snapped_sw, ["SHAPE@", "NEAR_FID"]) as cur:
    for shp, nid in cur:
        seg = sw_geom.get(nid)
        if seg:
            meas = seg.measureOnLine(shp.centroid)
            cur.updateRow([seg.positionAlongLine(meas), nid])

arcpy.AddMessage(f"Snapped signs ‚Üí sidewalks: {signs_snapped_sw}")

# ‚îÄ‚îÄ 4.2  Filter out signs that never snapped to a sidewalk ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# A.  signs whose NEAR_FID is NULL  ‚Üí  sign_errors
sign_errors = os.path.join(scratch, "sign_errors")
arcpy.analysis.Select(signs_snapped_sw, sign_errors, "NEAR_FID IS NULL")

# B.  signs we want to ignore (e.g. sign_type = 'NPARK')  ‚Üí  sign_skip
sign_skip = os.path.join(scratch, "sign_skip")
arcpy.analysis.Select(signs_snapped_sw, sign_skip, "sign_type = 'NPARK'")

# C.  delete both kinds from the working sign layer
with arcpy.da.UpdateCursor(signs_snapped_sw,
        ["NEAR_FID", "sign_type"]) as cur:

    for nid, stype in cur:
        if nid is None or stype == "NPARK":
            cur.deleteRow()

cnt_orphan = int(arcpy.management.GetCount(sign_errors)[0])
cnt_skip   = int(arcpy.management.GetCount(sign_skip)[0])

arcpy.AddMessage(f"üõà  {cnt_orphan} orphans ‚Üí sign_errors; "
                 f"{cnt_skip} NPARK signs ‚Üí sign_skip.")

# ------------------------------------------------------------
# 4.  Keep only sidewalks that have at least one matched sign
# ------------------------------------------------------------
# 4‚ÄëA  sid list from NEAR_FID
ids = {oid for oid, in arcpy.da.SearchCursor(signs_snapped_sw, ["NEAR_FID"])
        if oid is not None}

if not ids:
    arcpy.AddWarning("No sidewalks matched any sign within 60‚ÄØft ‚Äî nothing to process.")
    raise SystemExit()

id_sql = f"OBJECTID IN ({','.join(map(str, ids))})"

sw_work_has_sign = os.path.join(scratch, "sw_work_has_sign")
arcpy.analysis.Select(sw_work, sw_work_has_sign, id_sql)

arcpy.AddMessage(f"Sidewalks with signs ‚Üí {sw_work_has_sign} ({len(ids)})")

# join the side code from sidewalk layer onto the sign layer
arcpy.management.JoinField(signs_snapped_sw,       # target
                           "NEAR_FID",             # key on sign
                           sw_work_has_sign,       # source sidewalks
                           "OBJECTID",
                           [side_f])               # e.g. SIDE_CODE

# add a compass field and populate it
arcpy.management.AddField(signs_snapped_sw, "COMPASS", "TEXT", field_length=5)

arrow_to_compass = {
    "E": {"<-": "north", "->": "south"},
    "W": {"<-": "south", "->": "north"},
    "N": {"<-": "west",  "->": "east"},
    "S": {"<-": "east",  "->": "west"},
}

with arcpy.da.UpdateCursor(signs_snapped_sw,
        [side_f, "parsed_arrow", "COMPASS"]) as cur:

    for curb, arr, comp in cur:
        curb = (curb or "").strip().upper()
        arr  = (arr  or "").strip()

        if arr == "<->":
            comp = "both"
        else:
            comp = arrow_to_compass.get(curb, {}).get(arr)

        cur.updateRow([curb, arr, comp])

# ------------------------------------------------------------
# 5.  Erase a ¬±1.5‚ÄØft window around each sign (buffer‚Äëerase)
# ------------------------------------------------------------

# 5‚ÄëA  buffer the signs once
sign_buf = os.path.join(scratch, "sign_buf")
arcpy.analysis.Buffer(signs_snapped_sw, sign_buf, "1.5 Feet",
                      dissolve_option="NONE")

# Erase a ¬±1.5‚ÄØft window around each sign
sw_split_raw = os.path.join(scratch, "sw_split_raw")
arcpy.analysis.PairwiseErase(sw_work_has_sign, sign_buf, sw_split_raw)
arcpy.AddMessage(f"Erased 3‚ÄØft gaps at signs ‚Üí {sw_split_raw}")

# explode multipart to singlepart
sw_split = os.path.join(scratch, "sw_split")              # final working copy
arcpy.management.MultipartToSinglepart(sw_split_raw, sw_split)

# drop zero‚Äëlength slivers
sliver_sql = "SHAPE_Length < 0.05"          # adjust threshold if needed
arcpy.management.MakeFeatureLayer(sw_split, "split_lyr", sliver_sql)
if int(arcpy.management.GetCount("split_lyr")[0]) > 0:
    arcpy.management.DeleteFeatures("split_lyr")
arcpy.management.Delete("split_lyr")

# ------------------------------------------------------------
# 6.  Flag before / after / both via COMPASS logic
# ------------------------------------------------------------

# --- sign geometry lookup (needed inside the cursor) ----------
sign_geom = {oid: geom for oid, geom
             in arcpy.da.SearchCursor(signs_snapped_sw,
                                      ["OBJECTID", "SHAPE@"])}

# 1) ONE‚ÄëTO‚ÄëMANY spatial join: every segment ‚Üî every sign ‚â§ 2‚ÄØft
sw_join = os.path.join(scratch, "sw_split_joined")
arcpy.analysis.SpatialJoin(
    target_features   = sw_split,          # exploded single‚Äëpart segments
    join_features     = signs_snapped_sw,  # snapped signs (with COMPASS)
    out_feature_class = sw_join,
    join_operation    = "JOIN_ONE_TO_MANY",
    match_option      = "WITHIN_A_DISTANCE",
    search_radius     = "2 Feet"
)

# rename join keys for clarity
arcpy.management.AlterField(sw_join, "TARGET_FID", new_field_name="SEG_ID")
arcpy.management.AlterField(sw_join, "JOIN_FID",   new_field_name="SIGN_OID")
arcpy.management.JoinField(sw_join,          # target = sw_join rows
                           "SIGN_OID",       # key in sw_join
                           signs_snapped_sw, # source signs
                           "OBJECTID",       # key in signs
                           ["sign_type"])    # field(s) to copy

# add the flag field *before* the cursor
arcpy.management.AddField(sw_join, "no_stand", "SHORT")

# 2) row‚Äëby‚Äërow flag
with arcpy.da.UpdateCursor(
        sw_join,
        ["SEG_ID", "SIGN_OID", "COMPASS", "no_stand", "SHAPE@"]) as cur:

    for seg_id, soid, comp, flag, seg in cur:
        if comp == "both":
            flag = 1
        else:
            sign_pt  = sign_geom.get(soid)
            seg_side = segment_compass(seg, sign_pt)
            flag     = int(seg_side == comp) if seg_side else 0
        cur.updateRow([seg_id, soid, comp, flag, seg])

# 3) collapse duplicates: MAX(no_stand) per segment
stat_tbl = os.path.join(scratch, "seg_flag_stat")
arcpy.analysis.Statistics(
        sw_join, stat_tbl,
        [["no_stand", "MAX"]],
        case_field="SEG_ID")

arcpy.management.AlterField(stat_tbl, "MAX_no_stand",
                            new_field_name="no_stand")

# choose "MIN_SIGN_TYPE" = the alphabetically first type for that segment
type_tbl = os.path.join(scratch, "seg_type_stat")
arcpy.analysis.Statistics(
    sw_join, type_tbl,
    [["sign_type", "MIN"]],          # MIN, MAX, or COUNT
    case_field="SEG_ID")

arcpy.management.AlterField(type_tbl, "MIN_sign_type",
                            new_field_name="sign_type")

# join both flag + type back onto sw_split
arcpy.management.JoinField(sw_split, "OBJECTID",
                           stat_tbl,  "SEG_ID", ["no_stand"])
arcpy.management.JoinField(sw_split, "OBJECTID",
                           type_tbl,  "SEG_ID", ["sign_type"])

# 4) join the final flag back to the single‚Äëpart sidewalk layer
#    remove any placeholder no_stand first
if "no_stand" in [f.name for f in arcpy.ListFields(sw_split)]:
    arcpy.management.DeleteField(sw_split, ["no_stand"])

arcpy.management.JoinField(sw_split,          # target = segments
                           "OBJECTID",        # key in sw_split
                           stat_tbl,          # source table
                           "SEG_ID",          # key in stats table
                           ["no_stand"])

arcpy.AddMessage("‚úÖ  Segments flagged via COMPASS logic")

# ------------------------------------------------------------
# 8.  Clean up & export
# ------------------------------------------------------------
arcpy.management.CopyFeatures(sw_split, out_fc)     # overwriteOutput should be True
arcpy.AddMessage(f"üéâ  Final no‚Äëstanding layer ‚Üí {out_fc}")
```

## src/arcpy/arcpy depracated/arcpy_rank_dominant_working.py

```python
import arcpy
from pathlib import Path
import pandas as pd
from collections import defaultdict

arcpy.ImportToolbox(r"D:\ArcGIS\Projects\Street_Tree_Planting_Analysis\Street_Tree_Planting_Analysis.atbx", "stp")

def generate_initial_points(line_fc, spacing, work_gdb):
    """Copy input lines, then create and rank an initial set of points."""
    lines_working = str(work_gdb / "lines_working")
    if arcpy.Exists(lines_working):
        arcpy.management.Delete(lines_working)
    
    arcpy.management.CopyFeatures(str(line_fc), lines_working)

    pts = str(work_gdb / "pts_0")
    arcpy.stp.generatepoints(lines_working, str(spacing), pts)
    arcpy.stp.addrankfields(pts, lines_working)

    return lines_working, pts

def identify_non_conflicting_points(pts, spacing, work_gdb):
    """Remove points that intersect within 'spacing' feet of each other."""
    pts_work = str(work_gdb / "pts_0_working")
    arcpy.management.CopyFeatures(pts, pts_work)

    conflicts_fc = str(work_gdb / "potential_conflicts")
    arcpy.analysis.SpatialJoin(
        target_features=pts_work,
        join_features=pts_work,
        out_feature_class=conflicts_fc,
        join_operation="JOIN_ONE_TO_MANY",
        match_option="INTERSECT",
        search_radius=f"{spacing} Feet"
    )

    # Filter out self-joins so only overlapping points remain

    true_conflicts = str(work_gdb / "true_conflicts")
    arcpy.management.MakeFeatureLayer(conflicts_fc, "conflicts_lyr")
    arcpy.management.SelectLayerByAttribute("conflicts_lyr", "NEW_SELECTION", '"PARENT_OID" <> "PARENT_OID_1"')
    arcpy.management.CopyFeatures("conflicts_lyr", true_conflicts)

    conflict_ids = {row[0] for row in arcpy.da.SearchCursor(true_conflicts, ["TARGET_FID"])}

    # Remove points whose OID appears in the conflict table
    good_boys_fc = str(work_gdb / "pts_0_cleaned")
    with arcpy.da.UpdateCursor(pts_work, ["OBJECTID"]) as cursor:
        for row in cursor:
            if row[0] in conflict_ids:
                cursor.deleteRow()

    arcpy.management.CopyFeatures(pts_work, good_boys_fc)
    return good_boys_fc

def resolve_conflicts_iteratively(pts, lines_working, spacing, buffer_dist, work_gdb, max_iterations=3):
    """Iteratively buffer and erase lines to remove conflicting planting points."""
    iter_n = 0
    line_suppress_map = defaultdict(list)

    while True:
        iter_n += 1
        if iter_n > max_iterations:
            arcpy.AddWarning("Max iterations reached.")
            break
        
        line_suppress_map = defaultdict(list)

        near_tbl = str(work_gdb / f"near_{iter_n}")
        arcpy.analysis.GenerateNearTable(
            pts, pts, near_tbl, f"{buffer_dist} Feet", "NO_LOCATION",
            closest="ALL", method="PLANAR"
        )  # produces pairwise distances between points

        if int(arcpy.management.GetCount(near_tbl)[0]) == 0:
            break

        df = pd.DataFrame(arcpy.da.TableToNumPyArray(near_tbl, ["IN_FID", "NEAR_FID"]))
        df = df[df.IN_FID < df.NEAR_FID]

        rank = {}
        with arcpy.da.SearchCursor(pts, ["OID@", "PARENT_LEN", "PARENT_FID"]) as cur:
            for oid, ln_len, ln_fid in cur:
                rank[oid] = (ln_len, ln_fid)

        winners, losers = set(), set()
        for _, row in df.iterrows():
            i, j = int(row.IN_FID), int(row.NEAR_FID)
            if i not in rank or j not in rank or rank[i][1] == rank[j][1]:
                continue
            r_i, r_j = rank[i][0], rank[j][0]
            if r_i > r_j or (r_i == r_j and i < j):
                winners.add(i)
                losers.add(j)
                line_suppress_map[rank[j][1]].append(i)
            else:
                winners.add(j)
                losers.add(i)
                line_suppress_map[rank[i][1]].append(j)

        if not winners:
            break

        # Points to buffer are the losers grouped by parent line
        suppression_pts = list({oid for pts in line_suppress_map.values() for oid in pts})
        existing_oids = {row[0] for row in arcpy.da.SearchCursor(pts, ["OID@"])}
        suppression_pts = [oid for oid in suppression_pts if oid in existing_oids]

        if not suppression_pts:
            continue

        suppression_pt_layer = arcpy.management.MakeFeatureLayer(pts, "suppression_pts")[0]
        oid_field = arcpy.Describe(suppression_pt_layer).OIDFieldName
        arcpy.management.SelectLayerByAttribute(
            suppression_pt_layer, "NEW_SELECTION",
            f"{oid_field} IN ({','.join(map(str, suppression_pts))})"
        )

        # Defensive check before buffering
        count = int(arcpy.management.GetCount(suppression_pt_layer)[0])
        arcpy.AddMessage(f"üß™ Suppression point layer count: {count}")
        if count == 0:
            arcpy.AddMessage("‚ö†Ô∏è No suppression points selected ‚Äî skipping regeneration.")
            continue  # skip this iteration

        # Now it's safe to buffer
        buffer_fc = work_gdb / f"iter_{iter_n}_winner_buffers"
        arcpy.analysis.PairwiseBuffer(suppression_pt_layer, str(buffer_fc), f"{buffer_dist - 0.01} Feet", dissolve_option="ALL")

        losing_lines = list(line_suppress_map.keys())
        losing_line_layer = arcpy.management.MakeFeatureLayer(lines_working, "losing_losing_line_layer")[0]
        oid_field = arcpy.Describe(losing_line_layer).OIDFieldName
        arcpy.management.SelectLayerByAttribute(
            losing_line_layer, "NEW_SELECTION", f"{oid_field} IN ({','.join(map(str, losing_lines))})"
        )

        arcpy.AddMessage(f"üß™ Trying to erase these lines: {losing_lines[:5]}... (total: {len(losing_lines)})")

        count = int(arcpy.management.GetCount(str(losing_line_layer))[0])
        arcpy.AddMessage(f"üìä Selected lines for erase: {count}")
        if count == 0:
            arcpy.AddWarning("‚ö†Ô∏è No losing lines selected ‚Äî skipping erase this round.")
            continue

        # Optional geometry repair
        arcpy.management.RepairGeometry(str(buffer_fc), "DELETE_NULL")

        # Run erase
        trimmed_losing_lines = work_gdb / f"iter_{iter_n}_trimmed_losing_lines"
        arcpy.analysis.PairwiseErase(losing_line_layer, str(buffer_fc), str(trimmed_losing_lines))

        # Select survivor lines (NOT in losing_lines)
        survivor_lines = work_gdb / f"iter_{iter_n}_survivor_lines"
        arcpy.management.MakeFeatureLayer(lines_working, "lines_working_lyr")
        arcpy.management.SelectLayerByAttribute(
            "lines_working_lyr", "NEW_SELECTION",
            f"{oid_field} NOT IN ({','.join(map(str, losing_lines))})"
        )
        arcpy.management.CopyFeatures("lines_working_lyr", str(survivor_lines))

        # Merge survivors + trimmed losers
        merged_lines = work_gdb / f"iter_{iter_n}_merged_lines"
        arcpy.management.Merge([str(survivor_lines), str(trimmed_losing_lines)], str(merged_lines))

        # Filter out final lines under X feet before generating points
        min_line_length = 3
        lines_filtered = str(work_gdb / f"iter_{iter_n}_filtered_lines")

        # Create in-memory layer from merged result
        filtered_layer = arcpy.management.MakeFeatureLayer(str(merged_lines), f"merged_lines_lyr_{iter_n}")[0]

        # Select only lines that meet minimum length
        arcpy.management.SelectLayerByAttribute(
            filtered_layer, "NEW_SELECTION",
            f"Shape_Length >= {min_line_length}"
        )

        # Save filtered version to new feature class
        arcpy.management.CopyFeatures(filtered_layer, lines_filtered)

        # Set for next round
        lines_working = lines_filtered

        # Step: Regenerate points
        pts = str(work_gdb / f"pts_{iter_n}_regenerated")
        arcpy.stp.generatepoints(lines_working, str(spacing), pts)
        arcpy.stp.addrankfields(pts, lines_working)

    return None, None, iter_n, lines_working

def rank_dominant_prune(line_fc: str, out_pts_fc: str, final_lines_fc: str, spacing: float, buffer_dist: float) -> None:
    """High level prune workflow used by the ArcGIS tool."""
    work_gdb = Path(arcpy.env.scratchGDB)
    arcpy.env.workspace = str(work_gdb)
    arcpy.env.overwriteOutput = True

    # Step 1: Generate working copy of input lines and initial points
    lines_working, pts = generate_initial_points(line_fc, spacing, work_gdb)

    # Step 2: Run all pruning ‚Äî this mutates lines_working
    _, _, _, final_lines = resolve_conflicts_iteratively(
        pts, lines_working, spacing, buffer_dist, work_gdb
    )

    # Step 3: Generate final planting points from clean geometry
    arcpy.stp.generatepoints(str(final_lines), str(spacing), out_pts_fc)
    arcpy.AddMessage(f"üå≥ Final planting points generated from trimmed geometry: {out_pts_fc}")

    # Step 4: Generate final plantable sidewalk from clean geometry
    arcpy.management.CopyFeatures(final_lines, final_lines_fc)
    arcpy.AddMessage(f"üóÇ Final pruned sidewalk geometry saved to: {final_lines_fc}")

# ArcGIS Tool Entry
if __name__ == "__main__":
    line_fc = arcpy.GetParameterAsText(0)
    out_pts_fc = arcpy.GetParameterAsText(1)
    final_lines_fc = arcpy.GetParameterAsText(2)
    spacing = float(arcpy.GetParameterAsText(3))
    buffer_dist = float(arcpy.GetParameterAsText(4))

    rank_dominant_prune(line_fc, out_pts_fc, final_lines_fc, spacing, buffer_dist)
```

## src/arcpy/arcpy depracated/arcpy_Second_Step_Alternative.py

```python
# -*- coding: utf-8 -*-
"""
Generated by ArcGIS ModelBuilder on : 2025-06-10 11:33:01
"""
import arcpy

def SecondStepAlternative():  # Second_Step_Alternative
    """ArcPy model step generating alternative curb buffers."""

    # To allow overwriting outputs change overwriteOutput option to True.
    arcpy.env.overwriteOutput = False

    # Model Environment settings
    with arcpy.EnvManager(scratchWorkspace="D:\\ArcGIS\\Projects\\Street_Tree_Planting_Analysis\\Scrap.gdb"):
        SIDEWALK_3_ = "D:\\ArcGIS\\Data\\FileGDB-data-Planimetric_2022_AGOL_Link.gdb\\SIDEWALK"
        PLUTO = "D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped"
        Field_Map = "Borough \"Borough\" true false false 2 Text 0 0,First,#,D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped,Borough,0,1;Block \"Block\" true false false 4 Long 0 0,First,#,D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped,Block,-1,-1;Lot \"Lot\" true false false 2 Short 0 0,First,#,D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped,Lot,-1,-1;CD \"CD\" true true false 2 Short 0 0,First,#,D:\\ArcGIS\\Data\\Pluto\\MapPLUTO24v1_1.gdb\\MapPLUTO_24v1_1_clipped,CD,-1,-1"

        # Process: Copy Pluto (Export Features) (conversion)
        PLUTO_Copy = fr"{arcpy.env.scratchGDB}\Pluto_Copy"
        arcpy.conversion.ExportFeatures(in_features=PLUTO, out_features=PLUTO_Copy, field_mapping=Field_Map)

        # Process: Calculate Unique Blocks (Calculate Field) (management)
        PLUTO_Unique_Blocks = arcpy.management.CalculateField(in_table=PLUTO_Copy, field="Unique_Blocks", expression="Concatenate($feature.Borough,$feature.Block)", expression_type="ARCADE")[0]

        # Process: Pairwise Dissolve (Pairwise Dissolve) (analysis)
        PLUTO_Dissolve = fr"{arcpy.env.scratchGDB}\PLUTO_Dissolve"
        arcpy.analysis.PairwiseDissolve(in_features=PLUTO_Unique_Blocks, out_feature_class=PLUTO_Dissolve, dissolve_field=["Unique_Blocks"], multi_part="SINGLE_PART")

        # Process: Sidewalk Erase (Pairwise Erase) (analysis)
        Output_Feature_Class = fr"{arcpy.env.scratchGDB}\SIDEWALK_Erase"
        arcpy.analysis.PairwiseErase(in_features=SIDEWALK_3_, erase_features=PLUTO_Dissolve, out_feature_class=Output_Feature_Class)

        # Process: Collapse Hydro Polygon (Collapse Hydro Polygon) (cartography)
        Sidewalk_Collapse = fr"{arcpy.env.scratchGDB}\Sidewalk_Collapse"
        Output_Polygon_Feature_Class = ""
        Sidewalk_Collapse_InPoly_DecodeID, Sidewalk_Collapse_InLine_DecodeID = arcpy.cartography.CollapseHydroPolygon(in_features=[Output_Feature_Class], out_line_feature_class=Sidewalk_Collapse, out_poly_feature_class=Output_Polygon_Feature_Class)

        # Process: Multipart To Singlepart (Multipart To Singlepart) (management)
        PLUTO_Singlepart = fr"{arcpy.env.scratchGDB}\PLUTO_Singlepart"
        arcpy.management.MultipartToSinglepart(in_features=PLUTO_Dissolve, out_feature_class=PLUTO_Singlepart)

        # Process: Add Unique Block Parts (Add Field) (management)
        PLUTO_Unique = arcpy.management.AddField(in_table=PLUTO_Singlepart, field_name="Unique_Block_Parts", field_type="TEXT")[0]

        # Process: Calculate Unique PLUTO Blocks (Calculate Field) (management)
        Pluto_Blocks_2_ = arcpy.management.CalculateField(in_table=PLUTO_Unique, field="Unique_Block_Parts", expression="str(!Unique_Blocks!)+str(!OBJECTID!)")[0]

        # Process: Pluto Blocks Copy (Copy Features) (management)
        Pluto_Working = "D:\\ArcGIS\\Projects\\Street_Tree_Planting_Analysis\\Scrap.gdb\\Pluto_Blocks"
        arcpy.management.CopyFeatures(in_features=Pluto_Blocks_2_, out_feature_class=Pluto_Working)

        # Process: Pluto + Buffer (Pairwise Buffer) (analysis)
        Pluto_Buffer' = fr"{arcpy.env.scratchGDB}\Pluto_Buffer"
        arcpy.analysis.PairwiseBuffer(in_features=Pluto_Blocks_2_, out_feature_class=Pluto_Buffer', buffer_distance_or_field="25 Feet")

        # Process: Pairwise Erase (Pairwise Erase) (analysis)
        Pluto_Prep = fr"{arcpy.env.scratchGDB}\Pluto_Buffer_Topology"
        arcpy.analysis.PairwiseErase(in_features=Pluto_Buffer', erase_features=PLUTO_Dissolve, out_feature_class=Pluto_Prep)

        # Process: Pairwise Intersect (Pairwise Intersect) (analysis)
        Sidewalk_Pluto = "D:\\ArcGIS\\Projects\\Street_Tree_Planting_Analysis\\Street_Tree_Planting_Analysis.gdb\\Sidewalk_Pluto"
        arcpy.analysis.PairwiseIntersect(in_features=[Sidewalk_Collapse, Pluto_Prep], out_feature_class=Sidewalk_Pluto)

        # Process: Delete Field (2) (Delete Field) (management)
        Sidewalk_Pluto_3_ = arcpy.management.DeleteField(in_table=Sidewalk_Pluto, drop_field=["FID_Sidewalk_Collapse", "InPoly_ID", "InPoly_FID", "InLine_ID", "InLine_FID", "COLLAPSED", "FID_Pluto_Buffer_Topology_Erase_Clean", "ORIG_FID"])[0]

if __name__ == '__main__':
    # Global Environment settings
    with arcpy.EnvManager(autoCommit=1000, baUseDetailedAggregation=False, cellAlignment="DEFAULT", 
                          cellSize="MAXOF", cellSizeProjectionMethod="CONVERT_UNITS", coincidentPoints="MEAN", 
                          compression="LZ77", maintainAttachments=True, maintainSpatialIndex=False, 
                          matchMultidimensionalVariable=True, nodata="NONE", outputMFlag="Same As Input", 
                          outputZFlag="Same As Input", preserveGlobalIds=False, pyramid="PYRAMIDS -1 NEAREST DEFAULT 75 NO_SKIP NO_SIPS", 
                          qualifiedFieldNames=True, randomGenerator="0 ACM599", rasterStatistics="STATISTICS 1 1", 
                          resamplingMethod="NEAREST", terrainMemoryUsage=False, tileSize="128 128", 
                          tinSaveVersion="CURRENT", transferDomains=False, transferGDBAttributeProperties=False, 
                          unionDimension=False, workspace="D:\\ArcGIS\\Projects\\Street_Tree_Planting_Analysis\\Scrap.gdb"):
        SecondStepAlternative()
```

## src/stp/cli/stp_pipeline.py

```python
# -*- coding: utf-8 -*-
"""
Entry point for the Street Tree Planting (stp) pipeline.
Follows the primary scope defined in the README:

1. Read established parameters
2. Download files from ArcGIS/NYC OpenData
3. Convert JSON into GeoJSON features
4. Clean files of unnecessary fields
5. Apply buffers, filters, custom scripts
6. Create mutable and immutable sidewalk polylines
7. Merge all polygons where plantings cannot occur
8. Clip do-not-plant locations against sidewalk polyline
9. Process traffic signs, parking rules, MTA no-bus zones
10. Generate potential planting locations
11. Join planting locations with sidewalk info
12. Finish and export results
"""

import argparse
import logging
import sys


def parse_args():  # noqa: D103
    """
    Parse command-line arguments for pipeline parameters.

    Returns:
        argparse.Namespace: Parsed arguments
    """
    parser = argparse.ArgumentParser(
        description="Run the STP pipeline with user parameters"
    )
    parser.add_argument(
        "--config", required=True, help="Path to config YAML file"
    )
    return parser.parse_args()


def load_parameters(config_path):  # noqa: D103
    """
    Load pipeline parameters from a YAML config file.

    Args:
        config_path (str): Path to config file

    Returns:
        dict: Parameters dictionary
    """
    import yaml

    with open(config_path) as fh:
        params = yaml.safe_load(fh)
    return params


def download_sources(params):  # noqa: D103
    """
    Download source datasets from ArcGIS/NYC OpenData.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement download logic
    pass


def convert_to_geojson(params):  # noqa: D103
    """
    Convert downloaded JSON to GeoJSON point/polygon files.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement conversion
    pass


def clean_datasets(params):  # noqa: D103
    """
    Remove unnecessary fields from datasets.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement cleaning
    pass


def apply_spatial_ops(params):  # noqa: D103
    """
    Apply buffers, filters, and custom scripts.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement spatial operations
    pass


def build_sidewalk_polylines(params):  # noqa: D103
    """
    Create mutable and immutable sidewalk polylines.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement polyline creation
    pass


def merge_no_plant_zones(params):  # noqa: D103
    """
    Merge polygons where plantings cannot occur.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement merge logic
    pass


def clip_sidewalk(params):  # noqa: D103
    """
    Clip do-not-plant zones with sidewalk polyline.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement clipping
    pass


def process_parking_and_signs(params):  # noqa: D103
    """
    Build parking zones and classify rules, and process MTA zones.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement parking and sign logic
    pass


def generate_planting_locations(params):  # noqa: D103
    """
    Generate potential planting locations within allowed areas.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement location generation
    pass


def join_and_export(params):  # noqa: D103
    """
    Join planting points with sidewalk attributes and export.

    Args:
        params (dict): Pipeline parameters
    """
    # TODO: implement join and export
    pass


def main():  # noqa: D103
    """
    Main function orchestrating the pipeline steps.
    """
    args = parse_args()
    params = load_parameters(args.config)

    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
        stream=sys.stdout,
    )

    logging.info("Starting STP pipeline")
    download_sources(params)
    convert_to_geojson(params)
    clean_datasets(params)
    apply_spatial_ops(params)
    build_sidewalk_polylines(params)
    merge_no_plant_zones(params)
    clip_sidewalk(params)
    process_parking_and_signs(params)
    generate_planting_locations(params)
    join_and_export(params)
    logging.info("STP pipeline completed successfully")


if __name__ == "__main__":  # noqa: G004
    main()
```

## src/stp/core/config.py

```python
"""
Config loader for STP.

Loads YAML defaults, optional user overrides, and environment variables
with deep-merge logic and caching via lru_cache.
Provides get_setting() and get_constant() for dot-path access.
Also loads and validates workflow.yaml for pipeline orchestration.
"""

from __future__ import annotations

import os
import logging
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict

import yaml
from dotenv import load_dotenv

logging.basicConfig(level=logging.INFO)

# Load environment variables from .env into os.environ (if .env exists)
load_dotenv()


def load_user_config() -> Dict[str, Any]:
    """Load critical overrides from environment (.env or system env)."""
    # Pull API keys and database creds directly from environment variables
    return {
        "api_key": os.getenv("API_KEY"),
        "db_user": os.getenv("DB_USER"),
        "db_pass": os.getenv("DB_PASS"),
    }


def _deep_get(mapping: Dict[str, Any], keys: list[str]) -> Any:
    """Walk a nested dict by a list of keys; return None if any key is missing."""
    current: Any = mapping
    for key in keys:
        if not isinstance(current, dict):
            # If current level isn't a dict, path is invalid
            return None
        current = current.get(key)
    return current


def _merge(base: Dict[str, Any], update: Dict[str, Any]) -> Dict[str, Any]:
    """Recursively deep-merge two dicts: nested dicts merge, others overwrite."""
    # Create a shallow copy so we don't mutate the original base dict
    result = dict(base)
    for key, val in update.items():
        # If both base and update have dicts at this key, merge them recursively
        if key in result and isinstance(result[key], dict) and isinstance(val, dict):
            result[key] = _merge(result[key], val)
        else:
            # Otherwise, the update value replaces or adds to the result
            result[key] = val
    return result


@lru_cache(maxsize=1)
def _load_defaults() -> Dict[str, Any]:
    """Load defaults.yaml once and cache it for fast subsequent access."""
    # Locate the repo root relative to this file
    root = Path(__file__).resolve().parents[2]
    path = root / "config" / "defaults.yaml"
    with open(path, encoding="utf-8") as f:
        # Safe-load YAML; return empty dict if file is empty
        return yaml.safe_load(f) or {}


@lru_cache(maxsize=1)
def _load_overrides() -> Dict[str, Any]:
    """Load user.yaml overrides once and cache; return empty dict if absent."""
    root = Path(__file__).resolve().parents[2]
    path = root / "config" / "user.yaml"
    if path.exists():
        with open(path, encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    # No overrides file means no override entries
    return {}


@lru_cache(maxsize=1)
def load_config() -> Dict[str, Any]:
    """Merge defaults and user overrides into one config dict (cached)."""
    defaults = _load_defaults()
    overrides = _load_overrides()
    # Combine with override taking precedence
    return _merge(defaults, overrides)


def get_setting(
    key: str,
    default: Any | None = None,
    required: bool = False,
    env_override: bool = True,
) -> Any:
    """
    Retrieve a config value using dot-path lookup with this precedence:
      1. Environment variable (if env_override=True)
      2. user.yaml overrides
      3. defaults.yaml
      4. provided default arg

    Raises if 'required' is True and resulting value is missing or placeholder.
    """
    # 1) Check environment variables first (e.g., 'DB_HOST' for 'db.host')
    if env_override:
        env_key = key.upper().replace('.', '_')
        if (val := os.getenv(env_key)) is not None:
            return val

    # 2) Load merged config and attempt lookup in overrides then defaults
    keys = key.split('.')
    value = _deep_get(_load_overrides(), keys)
    if value is None:
        value = _deep_get(_load_defaults(), keys)
    if value is None:
        # 3) Fall back to function default if still missing
        value = default

    # 4) If marked required but missing or placeholder, error out
    if required and (value is None or value == "REPLACE_ME"):
        raise RuntimeError(f"Missing required setting: {key}")
    return value


def get_constant(key: str, default: Any | None = None) -> Any:
    """Fetch a constant from defaults.yaml only, ignoring user overrides."""
    keys = key.split('.')
    value = _deep_get(_load_defaults(), keys)
    # If not found, use provided default
    return default if value is None else value


@lru_cache(maxsize=1)
def load_workflow(path: str = 'config/workflow.yaml') -> Dict[str, Any]:
    """
    Parse workflow.yaml, merge with config globals (e.g., from defaults.yaml),
    and perform basic validation. This drives the pipeline orchestrator.
    Cached for efficiency like load_config().
    """
    root = Path(__file__).resolve().parents[2]
    full_path = root / path
    with open(full_path, 'r', encoding='utf-8') as f:
        workflow = yaml.safe_load(f)

    # Merge globals from the main config (e.g., EPSG defaults, limits)
    config = load_config()
    workflow['globals'] = config  # Merge the full config for broader access

    # Basic validation to catch errors early (expand as needed)
    required_sections = ['sources', 'prep_ops']  # Add 'final_clip', 'big_scripts' if always needed
    for section in required_sections:
        if section not in workflow:
            raise ValueError(f"workflow.yaml missing '{section}' section")
    for dataset_id, dataset in workflow['sources'].items():
        if 'url' not in dataset or 'format' not in dataset:
            logging.warning("Source %s missing 'url' or 'format'", dataset_id)

    logging.info("Parsed and merged workflow.yaml with config globals")
    return workflow


__all__ = [
    "load_user_config",
    "load_config",
    "get_setting",
    "get_constant",
    "load_workflow"
    ]
```

## src/stp/core/http.py

```python
"""Simple HTTP client helpers."""

from typing import Optional

import requests

_session = requests.Session()


def fetch_bytes(url: str, session: Optional[requests.Session] = None) -> bytes:
    """Return response content for GET request."""
    sess = session or _session
    resp = sess.get(url)
    resp.raise_for_status()
    return resp.content
```

## src/stp/core/settings.py

```python
"""Global constants for the STP package, pulled from config."""

from config import get_constant

DEFAULT_EPSG = get_constant('epsg.default', 4326)
NYSP_EPSG = get_constant('epsg.nysp', 2263)

# TODO: All done! No more hardcodes‚Äîconstants now come from defaults.yaml.
# Remove this file later if you want to access directly via get_constant everywhere.
```

## src/stp/fetch/arcgis.py

```python
"""Fetchers for ArcGIS REST services."""

from __future__ import annotations

from io import BytesIO
from pathlib import Path
from typing import List, Tuple

import geopandas as gpd

from .. import http_client
from ..storage.file_storage import sanitize_layer_name
from ..settings import DEFAULT_EPSG

__all__ = ["fetch_arcgis_vector", "fetch_arcgis_table"]


def _build_query_url(service_url: str, as_geojson: bool = True) -> str:
    """Return an ArcGIS REST query URL for *service_url*."""
    base = service_url.rstrip("/")
    if not base.lower().endswith("query"):
        base = f"{base}/query"
    params = "where=1%%3D1&outFields=*&returnGeometry=true"
    if as_geojson:
        params += "&outSR=4326&f=geojson"
    else:
        params += "&f=json"
    return f"{base}?{params}"


def fetch_arcgis_vector(
    service_url: str,
) -> List[Tuple[str, gpd.GeoDataFrame, int, int]]:
    """Fetch vector data from an ArcGIS FeatureServer layer."""
    url = _build_query_url(service_url, as_geojson=True)
    data = http_client.fetch_bytes(url)
    gdf = gpd.read_file(BytesIO(data))
    epsg = gdf.crs.to_epsg() or DEFAULT_EPSG
    layer_name = sanitize_layer_name(Path(service_url).stem)
    return [(layer_name, gdf, epsg, 4326)]


def fetch_arcgis_table(
    service_url: str,
) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Fetch a non-spatial table from an ArcGIS service."""
    url = _build_query_url(service_url, as_geojson=True)
    data = http_client.fetch_bytes(url)
    gdf = gpd.read_file(BytesIO(data))
    gdf.set_crs(epsg=DEFAULT_EPSG, inplace=True)
    layer_name = sanitize_layer_name(Path(service_url).stem)
    return [(layer_name, gdf, DEFAULT_EPSG)]
```

## src/stp/fetch/csv.py

```python
"""CSV direct download helper."""

from io import BytesIO
from pathlib import Path
from typing import List, Tuple
import logging

import geopandas as gpd
import pandas as pd
from pandas.errors import ParserError
from shapely.geometry import Point

from ..core import http
from ..storage.file_storage import sanitize_layer_name
from ..core import DEFAULT_EPSG #TODO use config.py to get default values

logger = logging.getLogger(__name__)


def fetch_csv_direct(url: str) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Download and parse a CSV URL."""
    data = http.fetch_bytes(url)
    try:
        df = pd.read_csv(BytesIO(data))
    except ParserError as err:
        logger.warning("CSV parse failed for %s: %s", url, err)
        return []
    if "latitude" not in df.columns or "longitude" not in df.columns:
        return []
    geometry = [Point(xy) for xy in zip(df.longitude, df.latitude)]
    gdf = gpd.GeoDataFrame(
        df.drop(columns=["latitude", "longitude"]),
        geometry=geometry,
        crs=f"EPSG:{DEFAULT_EPSG}",
    )
    layer_name = sanitize_layer_name(Path(url).stem)
    return [(layer_name, gdf, DEFAULT_EPSG)]
```

## src/stp/fetch/download.py

```python
"""Dispatcher for direct dataset downloads."""

from typing import List, Tuple

import geopandas as gpd

from .geojson import fetch_geojson_direct
from .csv import fetch_csv_direct


def fetch_direct(url: str) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Fetch data from *url* using the appropriate fetcher."""
    lower = url.lower()
    if lower.endswith((".geojson", ".json")):
        return fetch_geojson_direct(url)
    if lower.endswith(".csv"):
        return fetch_csv_direct(url)
    raise ValueError(f"Unsupported format: {url}")
```

## src/stp/fetch/gdb.py

```python
"""Fetchers for zipped shapefiles or geodatabases."""

from __future__ import annotations

from pathlib import Path
from typing import List, Tuple
from tempfile import TemporaryDirectory
import zipfile

import fiona
import geopandas as gpd

from .. import http_client
from ..storage.file_storage import sanitize_layer_name
from ..settings import DEFAULT_EPSG

__all__ = ["fetch_gdb_or_zip"]


def fetch_gdb_or_zip(url: str) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Download a zipped archive and extract layers."""
    data = http_client.fetch_bytes(url)
    results: List[Tuple[str, gpd.GeoDataFrame, int]] = []
    with TemporaryDirectory() as tmpdir:
        zip_path = Path(tmpdir) / "data.zip"
        with open(zip_path, "wb") as fh:
            fh.write(data)
        with zipfile.ZipFile(zip_path, "r") as zf:
            zf.extractall(tmpdir)
        for shp in Path(tmpdir).rglob("*.shp"):
            gdf = gpd.read_file(shp)
            epsg = gdf.crs.to_epsg() or DEFAULT_EPSG
            results.append((sanitize_layer_name(shp.stem), gdf, epsg))
        for gdb in Path(tmpdir).rglob("*.gdb"):
            for layer in fiona.listlayers(str(gdb)):
                gdf = gpd.read_file(gdb, layer=layer)
                epsg = gdf.crs.to_epsg() or DEFAULT_EPSG
                results.append((sanitize_layer_name(layer), gdf, epsg))
    return results
```

## src/stp/fetch/geojson.py

```python
"""GeoJSON direct download helper."""

from io import BytesIO
from pathlib import Path
from typing import List, Tuple
import logging

import geopandas as gpd
from fiona.errors import DriverError, FionaValueError

from .. import http_client
from ..storage.file_storage import sanitize_layer_name
from ..settings import DEFAULT_EPSG

logger = logging.getLogger(__name__)


def fetch_geojson_direct(url: str) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Download and parse a GeoJSON URL."""
    data = http_client.fetch_bytes(url)
    try:
        gdf = gpd.read_file(BytesIO(data))
    except (FionaValueError, DriverError) as err:
        logger.warning("GeoJSON read failed for %s: %s", url, err)
        return []
    gdf.set_crs(epsg=DEFAULT_EPSG, inplace=True)
    layer_name = sanitize_layer_name(Path(url).stem)
    return [(layer_name, gdf, DEFAULT_EPSG)]
```

## src/stp/fetch/gpkg.py

```python
"""Fetch layers from a GeoPackage."""

from __future__ import annotations

from pathlib import Path
from typing import List, Tuple
from tempfile import TemporaryDirectory

import fiona
import geopandas as gpd

from .. import http_client
from ..storage.file_storage import sanitize_layer_name
from ..settings import DEFAULT_EPSG

__all__ = ["fetch_gpkg_layers"]


def fetch_gpkg_layers(
    path_or_url: str,
) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Load all layers from a GeoPackage file or URL."""
    tmpdir: TemporaryDirectory | None = None
    gpkg_path = Path(path_or_url)
    if path_or_url.startswith("http"):
        tmpdir = TemporaryDirectory()
        gpkg_path = Path(tmpdir.name) / "data.gpkg"
        gpkg_path.write_bytes(http_client.fetch_bytes(path_or_url))
    try:
        results: List[Tuple[str, gpd.GeoDataFrame, int]] = []
        for layer in fiona.listlayers(str(gpkg_path)):
            gdf = gpd.read_file(gpkg_path, layer=layer)
            epsg = gdf.crs.to_epsg() or DEFAULT_EPSG
            results.append((sanitize_layer_name(layer), gdf, epsg))
        return results
    finally:
        if tmpdir is not None:
            tmpdir.cleanup()
```

## src/stp/fetch/lookup.py

```python
"""
Fetcher utilization
"""
# src/stp/scripts/download_utils.py

from stp.fetch.socrata import dispatch_socrata_table
from stp.fetch.arcgis import fetch_arcgis_table, fetch_arcgis_vector
from stp.fetch.csv import fetch_csv_direct
from stp.fetch.geojson import fetch_geojson_direct
from stp.fetch.gdb import fetch_gdb_or_zip
from stp.fetch.gpkg import fetch_gpkg_layers

FETCHERS = {
    ("socrata", "csv"):   dispatch_socrata_table,
    ("socrata", "json"):  dispatch_socrata_table,
    ("socrata", "geojson"): dispatch_socrata_table,
    ("socrata", "shapefile"): dispatch_socrata_table,
    ("arcgis", "csv"):    fetch_arcgis_table,
    ("arcgis", "json"):   fetch_arcgis_table,
    ("arcgis", "geojson"): fetch_arcgis_vector,
    ("arcgis", "shapefile"): fetch_arcgis_vector,
    (None, "csv"):        fetch_csv_direct,
    (None, "geojson"):    fetch_geojson_direct,
    (None, "shapefile"):  fetch_gdb_or_zip,
    (None, "gpkg"):       fetch_gpkg_layers,
}
```

## src/stp/fetch/socrata.py

```python
"""Placeholder Socrata fetcher."""

from __future__ import annotations

from typing import List, Tuple, Optional

import geopandas as gpd

__all__ = ["dispatch_socrata_table"]


def dispatch_socrata_table(url: str, app_token: Optional[str] = None
                           ) -> List[Tuple[str, gpd.GeoDataFrame, int]]:
    """Temporary stub for Socrata dataset fetching."""
    raise NotImplementedError("Socrata fetcher not implemented")
```

## src/stp/fetch/__init__.py

```python
"""Spatial data fetcher helpers."""

from .csv import fetch_csv_direct
from .geojson import fetch_geojson_direct
from .arcgis import fetch_arcgis_vector, fetch_arcgis_table
from .gdb import fetch_gdb_or_zip
from .gpkg import fetch_gpkg_layers
from .socrata import dispatch_socrata_table

__all__ = [
    "fetch_csv_direct",
    "fetch_geojson_direct",
    "fetch_arcgis_vector",
    "fetch_arcgis_table",
    "fetch_gdb_or_zip",
    "fetch_gpkg_layers",
    "dispatch_socrata_table",
]
```

## src/stp/process/clean/address.py

```python
"""Address-related cleaning routines."""

from typing import Optional

import geopandas as gpd
import pandas as pd


def clean_street_signs(
    gdf: gpd.GeoDataFrame,
    *,
    require_record_type: str = "Current",
    date_fields: Optional[list[str]] = None,
    int_fields: Optional[list[str]] = None,
    drop_suffixes: Optional[list[str]] = None,
    keep_fields: Optional[list[str]] = None,
) -> gpd.GeoDataFrame:
    """Clean street sign records and drop non-current entries."""
    df = gdf.copy()
    if date_fields is None:
        date_fields = [
            "order_completed_on_date",
            "sign_design_voided_on_date",
        ]
    if int_fields is None:
        int_fields = ["distance_from_intersection"]
    if drop_suffixes is None:
        drop_suffixes = [
            "on_street_suffix",
            "from_street_suffix",
            "to_street_suffix",
        ]
    if keep_fields is None:
        keep_fields = [
            "order_number",
            "record_type",
            "order_type",
            "borough",
            "on_street",
            "from_street",
            "side_of_street",
            "order_completed_on_date",
            "sign_code",
            "sign_description",
            "sign_size",
            "sign_location",
            "distance_from_intersection",
            "arrow_direction",
            "sheeting_type",
            "support",
            "to_street",
            "facing_direction",
            "sign_notes",
            "sign_design_voided_on_date",
        ]
    df = df[df["record_type"].str.strip().str.title() == require_record_type]
    for fld in date_fields:
        if fld in df:
            df[fld] = pd.to_datetime(df[fld], errors="coerce")
    for fld in int_fields:
        if fld in df:
            df[fld] = pd.to_numeric(df[fld], errors="coerce")
    df = df.drop(columns=[c for c in drop_suffixes if c in df.columns])
    final_cols = [c for c in keep_fields if c in df.columns] + ["geometry"]
    df = df[final_cols].copy()
    df["record_type"] = df["record_type"].str.strip().str.title()
    df["side_of_street"] = df["side_of_street"].str.strip().str.upper()
    df["arrow_direction"] = df["arrow_direction"].str.strip()
    df["sign_description"] = df["sign_description"].str.strip()
    return gpd.GeoDataFrame(df, geometry="geometry", crs=gdf.crs)
```

## src/stp/process/clean/trees.py

```python
"""Tree-related cleaning routines."""

from typing import Optional

import geopandas as gpd

MIN_DBH = 0.01


def clean_trees_basic(
    trees: gpd.GeoDataFrame,
    *,
    structure_field: str = "tpstructure",
    require_structure: str = "Full",
    id_field: str = "objectid",
    out_id: str = "TreeID",
) -> gpd.GeoDataFrame:
    """Return trees with full structure."""
    df = trees.loc[
        trees[structure_field] == require_structure, [id_field, "geometry"]
    ].copy()
    return df.rename(columns={id_field: out_id})


def clean_trees_advanced(
    trees: gpd.GeoDataFrame,
    planting_spaces: gpd.GeoDataFrame,
    *,
    condition_field: str = "tpcondition",
    drop_conditions: Optional[list[str]] = None,
    structure_field: str = "tpstructure",
    require_structure: str = "Full",
    dbh_field: str = "dbh",
    min_dbh: float = MIN_DBH,
    ps_key: str = "plantingspaceglobalid",
    ps_globalid: str = "globalid",
    ps_status_field: str = "psstatus",
    keep_ps_status: str = "Populated",
    ps_jur_field: str = "jurisdiction",
    exclude_jur: str = "Private",
    id_field: str = "objectid",
    out_id: str = "TreeID",
) -> gpd.GeoDataFrame:
    """Return cleaned trees joined to planting spaces."""
    if drop_conditions is None:
        drop_conditions = ["Unknown", "Dead"]
    mask = (
        ~trees[condition_field].isin(drop_conditions)
        & (trees[structure_field] == require_structure)
        & trees[dbh_field].gt(min_dbh)
    )
    df = trees.loc[mask].copy()
    df = df.merge(
        planting_spaces[[ps_globalid, ps_status_field, ps_jur_field]],
        left_on=ps_key,
        right_on=ps_globalid,
        how="inner",
    )
    ps_mask = (
        (df[ps_status_field] == keep_ps_status)
        & (df[ps_jur_field] != exclude_jur)
    )
    df = df.loc[ps_mask]
    df = df[[id_field, "geometry"]]
    return gpd.GeoDataFrame(df, geometry="geometry", crs=trees.crs).rename(
        columns={id_field: out_id}
    )


def canceled_work_orders(
    wo: gpd.GeoDataFrame,
    *,
    wo_type_field: str = "wotype",
    wo_cat_field: str = "wocategory",
    wo_status_field: str = "wostatus",
    allowed_types: Optional[list[str]] = None,
    allow_category: str = "Tree Planting",
    cancel_status: str = "Cancel",
    id_field: str = "objectid",
    out_id: str = "WOID",
) -> gpd.GeoDataFrame:
    """Filter work orders to cancelled planting jobs."""
    if allowed_types is None:
        allowed_types = [
            "Tree Plant-Park Tree",
            "Tree Plant-Street Tree",
            "Tree Plant-Street Tree Block",
        ]
    mask = (
        wo[wo_type_field].isin(allowed_types)
        & (wo[wo_cat_field] == allow_category)
        & (wo[wo_status_field] == cancel_status)
    )
    df = wo.loc[mask, [id_field, "geometry"]].copy()
    return df.rename(columns={id_field: out_id})


def clean_planting_spaces(
    ps: gpd.GeoDataFrame,
    *,
    status_field: str = "psstatus",
    keep_status: str = "Populated",
    jur_field: str = "jurisdiction",
    exclude_jur: str = "Private",
    id_field: str = "globalid",
    out_id: str = "PSID",
) -> gpd.GeoDataFrame:
    """Return populated planting spaces excluding private sites."""
    mask = ps[status_field] == keep_status
    if exclude_jur:
        mask &= ps[jur_field] != exclude_jur
    df = ps.loc[mask, [id_field, "geometry"]].copy()
    return df.rename(columns={id_field: out_id})
```

## src/stp/process/clean/__init__.py

```python

```

## src/stp/process/custom_ops.py

```python
"""Custom operations for STP pipeline (special/complex functions)."""

import logging

import geopandas as gpd
from pygeoops import centerline
# pip install pygeoops (medial axis for polygon centerlines)

logging.basicConfig(level=logging.INFO)


def collapse_to_centerline(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
    """Collapse polygons to centerlines (like CollapseHydroPolygon).

    Extracts medial axis (midpoints between edges).
    """
    centerlines = []
    for geom in gdf.geometry:
        if geom.is_valid and not geom.is_empty:
            cl = centerline(geom)
            if cl:
                centerlines.append(cl)
    collapsed = gpd.GeoDataFrame(geometry=centerlines, crs=gdf.crs)
    logging.info("Collapsed to centerlines")
    return collapsed
```

## src/stp/process/data_cleaning.py

```python
"""High-level cleaning dispatch functions."""

from .clean.trees import (
    clean_trees_basic,
    clean_trees_advanced,
    canceled_work_orders,
    clean_planting_spaces,
)
from .clean.address import clean_street_signs

__all__ = [
    "clean_trees_basic",
    "clean_trees_advanced",
    "canceled_work_orders",
    "clean_planting_spaces",
    "clean_street_signs",
]
```

## src/stp/process/field_ops.py

```python
"""Field operations for STP pipeline (attribute tweaks)."""

import logging

import geopandas as gpd

logging.basicConfig(level=logging.INFO)

def calculate_unique_blocks(
    gdf: gpd.GeoDataFrame,
    borough_field: str = 'Borough',
    block_field: str = 'Block',
    output_field: str = 'Unique_Blocks'
) -> gpd.GeoDataFrame:
    """Calculate unique blocks (concat Borough + Block, like CalculateField)."""
    gdf[output_field] = gdf[borough_field].astype(str) + gdf[block_field].astype(str)
    logging.info("Calculated %s", output_field)
    return gdf


def add_unique_parts(
    gdf: gpd.GeoDataFrame,
    unique_field: str = 'Unique_Blocks',
    output_field: str = 'Unique_Block_Parts'
) -> gpd.GeoDataFrame:
    """Add unique parts field(Unique_Blocks + index,
      like AddField + CalculateField)."""
    gdf[output_field] = gdf[unique_field].astype(str) + gdf.index.astype(str)
    logging.info("Added %s", output_field)
    return gdf


def delete_fields(
        gdf: gpd.GeoDataFrame, fields_to_drop: list
        ) -> gpd.GeoDataFrame:
    """Delete fields (like DeleteField). Drops specified columns."""
    gdf = gdf.drop(columns=fields_to_drop, errors='ignore')
    logging.info("Deleted fields")
    return gdf
```

## src/stp/process/geometry_ops.py

```python
"""Geometry operations for STP pipeline (shape-changing functions)."""

import logging

import geopandas as gpd

logging.basicConfig(level=logging.INFO)


def dissolve_gdf(gdf: gpd.GeoDataFrame, by_field: str, single_part: bool = True
                 ) -> gpd.GeoDataFrame:
    """Dissolve by field (like PairwiseDissolve).

    Groups features with same value in by_field, merging geometries.
    """
    dissolved = gdf.dissolve(by=by_field)
    if single_part:
        # Multipart to singlepart
        dissolved = dissolved.explode(ignore_index=True)
    logging.info("Dissolved by %s", by_field)
    return dissolved


def erase_gdf(input_gdf: gpd.GeoDataFrame, erase_gdf: gpd.GeoDataFrame
              ) -> gpd.GeoDataFrame:
    """Erase input by erase geometry (like PairwiseErase).

    Removes parts of input that overlap erase.
    """
    # unary_union combines erase shapes into one
    erased_geom = input_gdf.difference(erase_gdf.unary_union)
    erased = gpd.GeoDataFrame(
        geometry=erased_geom, crs=input_gdf.crs
        ).explode(ignore_index=True)
    logging.info("Erased geometries")
    return erased


def buffer_gdf(gdf: gpd.GeoDataFrame, distance: float
               ) -> gpd.GeoDataFrame:
    """Buffer geometries (like PairwiseBuffer).

    Adds a 'halo' around shapes.
    """
    buffered = gdf.buffer(distance)
    buffered_gdf = gpd.GeoDataFrame(geometry=buffered, crs=gdf.crs)
    logging.info("Buffered by %s", distance)
    return buffered_gdf


def intersect_gdf(input_gdf: gpd.GeoDataFrame, intersect_gdf: gpd.GeoDataFrame
                  ) -> gpd.GeoDataFrame:
    """Intersect (like PairwiseIntersect).

    Keeps overlapping parts, combines attributes.
    """
    intersected = gpd.overlay(input_gdf, intersect_gdf, how='intersection')
    logging.info("Intersected geometries")
    return intersected


def repair_geometry(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
    """Repair invalid geometries (like RepairGeometry).

    Fixes issues like self-intersects or bad rings.
    """
    gdf['geometry'] = gdf.make_valid()
    logging.info("Repaired geometries")
    return gdf
```

## src/stp/process/table.py

```python
"""Backward-compatible passthroughs for inventory and metadata helpers."""

from __future__ import annotations

from ..record.db import record as record_layer_metadata_db
from ..record.csv import record as record_layer_metadata_csv
from ..fetch.gpkg import (
    from_gpkg as build_fields_inventory_gpkg,
)
from ..fetch.postgis import (
    from_postgis as build_fields_inventory_postgis,
)
from ..fetch.export import to_csv as write_inventory

__all__ = [
    "record_layer_metadata_db",
    "record_layer_metadata_csv",
    "build_fields_inventory_gpkg",
    "build_fields_inventory_postgis",
    "write_inventory",
]
```

## src/stp/record/csv.py

```python
"""CSV metadata recording functions."""

from __future__ import annotations

import csv
import logging
from datetime import datetime
from pathlib import Path

__all__ = ["record"]


def record(
        csv_path: Path,
        layer_id: str,
        url: str,
        source_epsg: int,
        service_wkid: int | None = None) -> None:
    """Append a row to ``layers_inventory.csv``.

    Parameters
    ----------
    csv_path:
        Destination CSV path.
    layer_id:
        Identifier for the layer being recorded.
    url:
        Source URL of the dataset.
    source_epsg:
        EPSG code of the dataset.
    service_wkid:
        Optional WKID from an ArcGIS service.
    """
    logger = logging.getLogger(__name__)
    write_header = not csv_path.exists()
    try:
        csv_path.parent.mkdir(parents=True, exist_ok=True)
        with csv_path.open("a", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            if write_header:
                writer.writerow([
                    "layer_id",
                    "source_url",
                    "source_epsg",
                    "service_wkid",
                    "downloaded_at",
                ])
            writer.writerow([
                layer_id,
                url,
                source_epsg,
                service_wkid if service_wkid is not None else "",
                datetime.utcnow().isoformat(),
            ])
    except Exception as exc:  # pragma: no cover - log and continue
        logger.error("Failed to record metadata CSV %s: %s", csv_path, exc)
```

## src/stp/record/db.py

```python
"""Database metadata recording functions."""

from __future__ import annotations

import logging
from sqlalchemy.engine import Engine
from sqlalchemy import text

__all__ = ["record"]


def record(
        engine: Engine,
        layer_id: str,
        url: str,
        source_epsg: int,
        service_wkid: int | None = None) -> None:
    """Insert a row into the ``layers_inventory`` table.

    Parameters
    ----------
    engine:
        SQLAlchemy database engine.
    layer_id:
        Identifier for the layer being recorded.
    url:
        Source URL of the dataset.
    source_epsg:
        EPSG code of the dataset.
    service_wkid:
        Optional WKID from an ArcGIS service.
    """
    if engine is None:
        return

    logger = logging.getLogger(__name__)
    stmt = text(
        """
        INSERT INTO layers_inventory (
            layer_id, source_url, source_epsg, service_wkid, downloaded_at
        ) VALUES (
            :layer_id, :url, :epsg, :service_wkid, NOW()
        )
        ON CONFLICT (layer_id) DO NOTHING
        """
    )
    try:
        engine.execute(
            stmt,
            {
                "layer_id": layer_id,
                "url": url,
                "epsg": source_epsg,
                "service_wkid": service_wkid,
            },
        )
    except Exception as exc:  # pragma: no cover - log and continue
        logger.error("Failed to record metadata for %s: %s", layer_id, exc)
```

## src/stp/record/export.py

```python
"""Export inventory DataFrames."""

from __future__ import annotations

from pathlib import Path
import pandas as pd

__all__ = ["to_csv"]


def to_csv(df: pd.DataFrame, out_csv: Path, show_path: bool = True) -> None:
    """Write *df* to *out_csv* and optionally print the path."""
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(out_csv, index=False)
    if show_path:
        print(f"Schema inventory written to {out_csv}")
```

## src/stp/record/gpkg.py

```python
"""Extract field inventory from a GeoPackage."""

from __future__ import annotations

from pathlib import Path
from typing import List, Dict

import fiona
import pandas as pd

__all__ = ["from_gpkg"]


def from_gpkg(gpkg_path: Path) -> pd.DataFrame:
    """Return field inventory for all layers in *gpkg_path*.

    The returned ``DataFrame`` has columns ``layer_name``, ``field_name`` and
    ``field_type``.
    """
    rows: List[Dict[str, str]] = []
    for layer in fiona.listlayers(str(gpkg_path)):
        with fiona.open(str(gpkg_path), layer=layer) as src:
            for field, ftype in src.schema["properties"].items():
                rows.append(
                    {
                        "layer_name": layer,
                        "field_name": field,
                        "field_type": ftype,
                    }
                )
    return pd.DataFrame(rows)
```

## src/stp/record/postgis.py

```python
"""Extract field inventory from a PostGIS database."""

from __future__ import annotations

from sqlalchemy import text
from sqlalchemy.engine import Engine
import pandas as pd

__all__ = ["from_postgis"]


def from_postgis(engine: Engine, schema: str = "public") -> pd.DataFrame:
    """Return field inventory for all tables in a PostGIS schema."""
    sql = text(
        """
        SELECT table_name AS layer_name,
               column_name AS field_name,
               data_type AS field_type
        FROM information_schema.columns
        WHERE table_schema = :schema
        ORDER BY table_name, ordinal_position
        """
    )
    return pd.read_sql(sql, engine, params={"schema": schema})
```

## src/stp/record/__init__.py

```python
"""Schema inventory helpers."""
```

## src/stp/scaffold.md

````markdown
# Scaffold.md: Tree Planting Analysis Pipeline Overview

This document outlines the high-level workflow for the STP (Spatial Tabular Pipeline) GIS project, originally built in ArcPy/ModelBuilder and now converted to pure Python (using GeoPandas for ops like buffers/unions/clips). The focus is NYC tree planting analysis, but it's designed for flexibility (e.g., toggle steps for other cities via config edits). We use a unified YAML config to drive the pipeline dynamically, with Docker for automatic PostGIS setup (spatial DB storage for efficient queries/filters).

## 1. Define Configuration
Configuration is centralized for ease‚Äîedit files to customize sources, ops, filters, and params without changing code. This replaces a "locked" pipeline with variable/user-defined flows (e.g., nest ops in YAML for reordering/toggling).

- **root/config/workflow.yaml**: Main unified config (combines sources, prep ops, filters, and final steps).
  - Data sources from open data portals (e.g., Socrata/ArcGIS REST).
  - Includes URL, type/format, schema, filter parameters (e.g., Socrata $where queries).
  - Nested ops/steps for processing (e.g., copy ‚Üí select ‚Üí buffer), with descriptions, enabled toggles, inputs/outputs.
  - Users can fill out or override (e.g., add local paths for offline use).
  - For reusing a feature class multiple times (e.g., curb_cut in different filters): Reference the same output_layer in steps‚Äîno duplication needed. If mutable (changes needed without overwriting), nest a 'copy' op first. If non-mutable download, use local_path fallback in sources section.
  - Example snippet (from our merged version):
    ```yaml
    data:  # Top-level for all datasets (renamed from 'data_id' for clarity)
        trees:
            sources:  # Fetch details
                type: "json"
                source_type: "socrata"
                format: "json"
                url: "https://data.cityofnewyork.us/resource/hn5i-inap.json"
                schema: "political"
                filter: "SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')"
                to_layer: "trees_raw"
                enabled: true
            prep_ops:  # Nested processing
                description: "Existing tree locations to avoid"
                enabled: true
                steps:
                    - op: copy
                    input: "trees_raw"  # References the source's to_layer
                    output: "trees_copy"
                    - op: xy_to_point
                    output: "trees_1"
                    - op: buffer
                    distance: 25
                    output: "trees_ready"

        nyzd:  # Another dataset bundle
            sources:
            # ... (URL/type/filter)
            prep_ops:
            # ... (steps like copy/select)
    ```

- **root/config/defaults.yaml**: Stores default parameters for tools/ops (e.g., EPSG codes, limits). Optional but useful for globals; can be loaded into pipeline.py and overridden in workflow.yaml if needed.
  - Example:
    ```yaml
    epsg:
      default: 4326
      nysp: 2263
    limits:
      socrata: 50000
      arcgis_default_max_records: 1000
    validation:
      layer_name_max_length: 60
      min_dbh: 0.01
    ```

- **root/config/user.yaml**: User-specific overrides (e.g., API keys, batch params, DB creds). Loads defaults.yaml if none specified. With variable pipelines, this is key for personalization (e.g., change Socrata batch size).
  - Example for DB/Docker:
    ```yaml
    db:
      user: 'admin'
      pass: 'admin'
      host: 'localhost'
      port: 5432
      db_name: 'tree_pipeline'
    api:
      socrata_key: 'your_key_here'  # If needed for throttled APIs
      batch_size: 50000  # Override defaults never above 50000, if above revert to 50,000
    ```

## 2. Download & Prep Sources
Primary goal: Fetch and prep NYC-relevant data for tree planting (e.g., trees, hydrants, sidewalks) from open sources. Variable for other cities (edit workflow.yaml URLs/filters). Downloads via fetchers/ (Socrata/ArcGIS REST handlers in STP tools), with API keys/batch params from user.yaml (e.g., batch to avoid limits on big datasets like census blocks).

- Sources located in workflow.yaml 'sources' section (merged from old sources.json).
- Sources can be from anywhere, but default to Socrata and ArcGIS REST.
  - Requires API key/parameters (e.g., batch size/combining for large fetches) from user.yaml.
- Storage: Use Docker to automatically install/setup a DB with PostgreSQL/PostGIS for spatial efficiency (queries/filters on layers like zoning districts).
  - Docker auto-creates the environment (isolated, portable‚Äîrun `docker compose up -d` from docker-compose.yml).
  - Load creds from user.yaml (e.g., user: 'admin', pass: 'admin', host: 'localhost', port: 5432).
  - Connect: `engine = create_engine('postgresql://admin:admin@localhost:5432/tree_pipeline')`.
  - Load data: After download, `gdf.to_postgis('trees_raw', engine, if_exists='replace')` (GeoPandas handles conversion to spatial tables).
  - Why PostGIS? Faster for ops (e.g., SQL filters during select) than files; fallback to GPKG if needed.
- Prep: Run nested steps from workflow.yaml 'prep_ops' (e.g., copy ‚Üí xy_to_point ‚Üí buffer). Outputs saved as new tables/layers.
- Final clip/big scripts: Handled in workflow.yaml sections (nested steps for union/clip/points generation, then nostanding/rank/curb).

3. Define Queries
    - 


3. Download & Convert JSON ‚Üí GeoJSON / in-geodatabase exports
    - Data sources: 
        - Socrata
        - ArcGIS REST 
    - Data is initially stored in ./data 
    * files are stored in a gpkg in ./data/gpkg

4. Data Cleaning
    - 

5. Preliminary Operations
   3.1 ExportFeatures(Online_NYZD ‚Üí NYZD)
   3.2 PairwiseBuffer(NYZD ‚Üí NYZD_Buffer, 20 ft)
   3.3 ExportFeatures(BK_Vaults ‚Üí BK_Vaults_2)
   3.4 PairwiseBuffer(BK_Vaults_2 ‚Üí BK_Vaults_Buffer, 20 ft)
   3.5 ExportFeatures(STP_Apps_Vaults ‚Üí DOT_Vault_ExportFeatures)
   3.6 PairwiseBuffer(DOT_Vault_ExportFeatures ‚Üí DOT_Vault_Buffer, 20 ft)
   3.7 ExportFeatures(ConEd_Transformer_Vault ‚Üí ConEd_Transformer_Vault)
   3.8 PairwiseBuffer(ConEd_Transformer_Vault ‚Üí ConEd_Transformer_Buffer, 20 ft)
   3.9 ExportFeatures(DEP_GI_Assets ‚Üí DEP_GI_Assets)
   3.10 PairwiseBuffer(DEP_GI_Assets ‚Üí DEP_GI_Assets_Buffer, 20 ft)

4. Copy & select raw features
   4.1 CopyFeatures(SIDEWALK ‚Üí SIDEWALK_2)
   4.2 CopyFeatures(HVI_CensusTracts_v2013 ‚Üí HVI_CensusTracts_v2013_Copy)
   4.3 CopyFeatures(Curb_Cuts_Intersections_2 ‚Üí CURB_CUT_CopyFeatures)
   4.4 Select(CURB_CUT_CopyFeatures ‚Üí Curb_Cuts_Intersections, where SUB_FEATURE_CODE=222700)
   4.5 PairwiseBuffer(Curb_Cuts_Intersections ‚Üí Curb_Cuts_Intersection_Buffer, 30 ft)
   4.6 CopyFeatures(Subway_Lines ‚Üí Subway_Lines_2)
   4.7 PairwiseBuffer(Subway_Lines_2 ‚Üí Subway_Lines_Buffer, 80 ft)

5. Convert CSV ‚Üí points
   5.1 ExportTable(Workorders.csv ‚Üí Workorders)
   5.2 XYTableToPoint(Workorders ‚Üí Workorders_XYTableToPoint)
   5.3 PairwiseBuffer(Workorders_XYTableToPoint ‚Üí WorkOrders_Buffer, 25 ft)
   5.4 ExportTable(TreeandSite.csv ‚Üí TreenSite)
   5.5 XYTableToPoint(TreenSite ‚Üí TreenSite_XYTableToPoint)
   5.6 PairwiseBuffer(TreenSite_XYTableToPoint ‚Üí TreeAndSite_Buffer, 25 ft)

6. Clean & prep shrub layer
   6.1 CopyFeatures(Grass_Shrub_ExportFeatures ‚Üí Grass_Shrub)
   6.2 DeleteField(Grass_Shrub, ["Id", "gridcode"])
   6.3 AddField("Pit_Type", TEXT, length=10)
   6.4 CalculateField(Pit_Type = "EP/LP")

7. Clean & prep HVI tracts
   7.1 CopyFeatures(FHNR_Datasets_HVI_CensusTracts ‚Üí HVI_CensusTracts_v2013_CopyFeatures)
   7.2 DeleteField(HVI_CensusTracts_v2013_CopyFeatures, [...lots of fields...])
   7.3 AlterField("QUINTILES" ‚Üí "HVI_CT_2013")

8. Union political & HVI boundaries
   8.1 Union([
         Political_Boundary,
         HVI_CensusTracts_v2013_CopyFeatures_2,
         NYCDOHMH_HVI_CensusTracts_2018_Clip,
         NYCDOHMH_HVI_CensusTracts_2023,
         NYCDOHMH_HVI_NeighborhoodTabulationAreas_2018,
         NYCDOHMH_HVI_NeighborhoodTabulationAreas_2023,
         NYCDCP_Borough_Boundaries_Water_Included,
         NYCDCP_Borough_Boundaries
       ] ‚Üí Political_Boundary_Final)
   8.2 DeleteField(Political_Boundary_Final, [all FID_* fields])

9. Further buffers & selections
   9.1 PairwiseBuffer(Curb_Cuts_Intersections ‚Üí Curb_Cuts_Intersection_20ft, 20 ft)
   9.2 Select(Street_Centerline WHERE L_LOW_HN <> '' ‚Üí Street_Centerline_Select)
   9.3 SimplifyLine(Street_Centerline_Select ‚Üí Street_Centerli_SimplifyLine, POINT_REMOVE, 1 ft)
   9.4 FeatureVerticesToPoints(Street_Centerli_SimplifyLine ‚Üí Street_Vertices_Points)
   9.5 PairwiseBuffer(Street_Vertices_Points ‚Üí Street_Vertice_Buffer, 40 ft)

10. Hydrant proximity & cleanup
   10.1 Near(DEP_Hydrants ‚Üí Sidewalk_Pluto, radius=10 ft, location)
   10.2 MoveStreetSigns(Hydrants_Near ‚Üí Hydrants_Corrected)
   10.3 PairwiseBuffer(Hydrants_Corrected ‚Üí DEP_Hydrants_PairwiseBuffer, 3 ft)

11. Driveway curb-cut processing
    11.1 Select(CURB_CUT_CopyFeatures WHERE SUB_FEATURE_CODE=222600 ‚Üí Curb_Cuts_Driveways)
    11.2 CurbCuts(input=Curb_Cuts_Driveways, extension=7, buffer=15 ‚Üí Curb_Cuts_Driveways_Buffer)

12. Vault union & cleanup
    12.1 Union([BK_Vaults_Buffer, DOT_Vault_Buffer] ‚Üí Vaults)
    12.2 DeleteField(Vaults, [a long list of original fields])
    12.3 AddField("Vaults", LONG)
    12.4 CalculateField(Vaults = 1)
    12.5 (Placeholder) Union(‚Ä¶ ‚Üí Output_Feature_Class)

13. Final buffer
    13.1 PairwiseBuffer(Curb_Cuts_Intersections ‚Üí Curb_Cuts_Intersection_10ft, 10 ft)
````

## src/stp/scrap.md

```markdown
# To Use / How To Use

## No Planting Areas / Locations to clip sidewalk

- NYZD  
  - Description: Zoning districts where planting is prohibited  
  - Filters: `"ZONE_DIST" IN ('M1','M2','M3','IG','IH')`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `nyzd_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `nyzd_ready`

- DEP_GI_Assets  
  - Description: Green infrastructure assets to avoid  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `dep_gi_assets_copy`  
    - Operation b: Buffer  
      - Params: distance = 20 Feet  
      - Output: `dep_gi_assets_ready`

- Sidewalk  
  - Description: Raw sidewalk polygons, split for different logic  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `sidewalk_copy`  
    - Operation b: Polygon‚ÜíPolyline  
      - Params: input = `sidewalk_copy`  
      - Output: `sidewalk_1`  
    - Operation c: Split immutable/mutable  
      - Params: input = `sidewalk_1`  
      - Outputs: `sidewalk_immutable_ready`, `sidewalk_mutable_ready`

- Curb_Cuts  
  - Description: Sidewalk curb-cut intersections  
  - Filters: `SUB_FEATURE_CODE = 222700`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `curb_cuts_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `curb_cuts_1`  
    - Operation c: Buffer  
      - Params: input = `curb_cuts_1`, distance = 30 Feet  
      - Output: `curb_cuts_ready`

- Subway_Lines  
  - Description: Subway buffers to exclude planting near tracks  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `subway_lines_copy`  
    - Operation b: Buffer  
      - Params: distance = 80 Feet  
      - Output: `subway_lines_ready`

- Workorders  
  - Description: Active DOT work orders  
  - Filters: `STATUS <> 'Cancelled'`  
  - Operations  
    - Operation a: CopyFeatures (ExportTable/Select)  
      - Output: `workorders_copy`  
    - Operation b: XYTableToPoint  
      - Params: input = `workorders_copy`  
      - Output: `workorders_1`  
    - Operation c: Buffer  
      - Params: input = `workorders_1`, distance = 25 Feet  
      - Output: `workorders_ready`

- TreeandSite  
  - Description: Existing tree locations to avoid  
  - Filters: `SITE_TYPE = 'Tree Site' AND CONDITION NOT IN ('Dead','Removed','Stump')`  
  - Operations  
    - Operation a: CopyFeatures (ExportTable/Select)  
      - Output: `treeandsite_copy`  
    - Operation b: XYTableToPoint  
      - Params: input = `treeandsite_copy`  
      - Output: `treeandsite_1`  
    - Operation c: Buffer  
      - Params: input = `treeandsite_1`, distance = 25 Feet  
      - Output: `treeandsite_ready`

- Grass_Shrub  
  - Description: 2017 land-use raster converted to shrub/grass polygons  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures (Copy Raster)  
      - Output: `grass_shrub_copy`  
    - Operation b: RasterToPolygon_conversion  
      - Params: simplify = NO_SIMPLIFY  
      - Output: `grass_shrub_1`  
    - Operation c: DeleteField  
      - Params: fields = `["gridcode","Id"]`  
      - Output: `grass_shrub_ready`

- Political_Boundary  
  - Description: Union of all political boundaries for spatial join  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `political_boundary_copy`  
    - Operation b: Union_analysis  
      - Params: inputs = borough, community boards, council, senate, assembly, NTA, tracts  
      - Output: `political_boundary_1`  
    - Operation c: DeleteField  
      - Params: fields = all `FID_*`  
      - Output: `political_boundary_ready`

- Street_Centerline  
  - Description: Simplify street geometry and buffer vertices  
  - Filters: `L_LOW_HN IS NOT NULL`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `street_centerline_copy`  
    - Operation b: SimplifyLine_cartography  
      - Params: method = POINT_REMOVE, tolerance = 1 Feet  
      - Output: `street_centerline_1`  
    - Operation c: FeatureVerticesToPoints  
      - Params: input = `street_centerline_1`  
      - Output: `street_centerline_2`  
    - Operation d: Buffer  
      - Params: input = `street_centerline_2`, distance = 40 Feet  
      - Output: `street_centerline_ready`

- DEP_Hydrants  
  - Description: Hydrant proximity adjustments  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `dep_hydrants_copy`  
    - Operation b: GenerateNearTable  
      - Params: distance = 10 Feet  
      - Output: `dep_hydrants_1`  
    - Operation c: MoveStreetSigns (custom)  
      - Params: as defined in script  
      - Output: `dep_hydrants_2`  
    - Operation d: Buffer  
      - Params: input = `dep_hydrants_2`, distance = 3 Feet  
      - Output: `dep_hydrants_ready`

- Curb_Cuts_Driveways  
  - Description: Driveway curb cuts to exclude planting  
  - Filters: `SUB_FEATURE_CODE = 222600`  
  - Operations  
    - Operation a: CopyFeatures  
      - Output: `curb_cuts_driveways_copy`  
    - Operation b: Select  
      - Params: filter above  
      - Output: `curb_cuts_driveways_1`  
    - Operation c: Buffer  
      - Params: input = `curb_cuts_driveways_1`, distance = 15 Feet  
      - Output: `curb_cuts_driveways_ready`

## Final first step

- Final Clip  
  - Description: Compute allowable planting points within clipped sidewalk  
  - Filters: None  
  - Operations  
    - Operation a: CopyFeatures (non-mutable backup)  
      - Params: input = `sidewalk_mutable_ready`  
      - Output: `sidewalk_mutable_backup`  
    - Operation b: Union no-plant zones  
      - Params: inputs = nyzd_ready, dep_gi_assets_ready, curb_cuts_ready, subway_lines_ready, workorders_ready, treeandsite_ready, grass_shrub_ready  
      - Output: `no_plant_zones_union`  
    - Operation c: Clip_analysis  
      - Params: input = `sidewalk_mutable_ready`, clip_features = `no_plant_zones_union`  
      - Output: `sidewalk_availability_ready`  
    - Operation d: Create planting points  
      - Params: method = CreateFishnet or custom  
      - Output: `plant_pts_1`  
    - Operation e: SpatialJoin  
      - Params: target = `plant_pts_1`, join_features = `sidewalk_availability_ready`  
      - Output: `plant_pts_ready`
```

## src/stp/storage/file_storage.py

```python
"""File-based GeoDataFrame helpers."""

from pathlib import Path

import geopandas as gpd
import pandas as pd

__all__ = [
    "get_geopackage_path",
    "sanitize_layer_name",
    "export_spatial_layer",
    "reproject_all_layers",
]

LAYER_NAME_MAX_LENGTH = 60


def get_geopackage_path(
    output_dir: Path, filename: str = "project_data.gpkg"
) -> Path:
    """Return a fresh GeoPackage path under *output_dir*."""
    gpkg = Path(output_dir) / filename
    if gpkg.exists():
        try:
            gpkg.unlink()
        except PermissionError as err:
            print(f"\u26a0\ufe0f Could not delete '{gpkg}': {err}")
    return gpkg


def sanitize_layer_name(name: str) -> str:
    """Return *name* cleaned for file or database layers."""
    safe = "".join(ch if (ch.isalnum() or ch == "_") else "_" for ch in name)
    if safe and safe[0].isdigit():
        safe = "_" + safe
    return safe[:LAYER_NAME_MAX_LENGTH]


def export_spatial_layer(gdf: gpd.GeoDataFrame, layer_name: str,
                         gpkg_path: Path) -> None:
    """Write ``gdf`` to ``gpkg_path`` under ``layer_name``."""
    gdf.to_file(gpkg_path, layer=layer_name, driver="GPKG")


def reproject_all_layers(
    gpkg_path: Path, metadata_csv: Path, target_epsg: int
) -> None:
    """Reproject each layer in the GeoPackage in place."""
    meta = pd.read_csv(metadata_csv)
    for _, row in meta.iterrows():
        layer_name = row["layer_id"]
        source_epsg = int(row["source_epsg"])
        raw_wkid = row.get("service_wkid")
        try:
            service_wkid = int(raw_wkid) if raw_wkid not in (None, "") else ""
        except ValueError:
            service_wkid = ""
        gdf = gpd.read_file(gpkg_path, layer=layer_name)
        if gdf.crs is None:
            gdf = gdf.set_crs(epsg=source_epsg, allow_override=True)
        else:
            gdf = gdf.to_crs(epsg=source_epsg)
        gdf = gdf.to_crs(epsg=target_epsg)
        gdf.to_file(gpkg_path, layer=layer_name, driver="GPKG")
        if service_wkid:
            print(
                f"Reprojected '{layer_name}': {service_wkid} ‚Üí {target_epsg}"
            )
        else:
            print(
                f"Reprojected '{layer_name}': {source_epsg} ‚Üí {target_epsg}"
            )
```

## src/stp/storage/__init__.py

```python

```

## src/stp/stp.md

`````markdown
# STP ‚Äì Spatial / Tabular Pipeline

`src/stp` is a lightweight toolkit for fetching raw spatial / tabular data,
cleaning it, and persisting it to GeoPackage or PostGIS.  
Everything is pure-Python, built on GeoPandas, Shapely and SQLAlchemy.

---

## Folder structure

```
![alt text](image.png)
stp/
‚îÇ
‚îú‚îÄ‚îÄ cleaning/        ‚Üê cleaning steps for specific datasets
‚îÇ   ‚îú‚îÄ‚îÄ address.py
‚îÇ   ‚îú‚îÄ‚îÄ trees.py
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îú‚îÄ‚îÄ fetchers/        ‚Üê source-specific download helpers
‚îÇ   ‚îú‚îÄ‚îÄ arcgis.py         # ArcGIS REST ‚Üí GeoJSON/Parquet
‚îÇ   ‚îú‚îÄ‚îÄ csv.py            # Plain CSV files (HTTP/local)
‚îÇ   ‚îú‚îÄ‚îÄ gdb.py            # FileGDB via ogr2ogr
‚îÇ   ‚îú‚îÄ‚îÄ geojson.py        # Arbitrary GeoJSON endpoints
‚îÇ   ‚îú‚îÄ‚îÄ gpkg.py           # Remote GeoPackage layers
‚îÇ   ‚îú‚îÄ‚îÄ socrata.py        # Socrata Open Data API
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îú‚îÄ‚îÄ inventory/       ‚Üê schema inspection & export utilities
‚îÇ   ‚îú‚îÄ‚îÄ export.py         # dump layer ‚Üí CSV/Markdown schema
‚îÇ   ‚îú‚îÄ‚îÄ gpkg.py           # field inventory for GeoPackage
‚îÇ   ‚îú‚îÄ‚îÄ postgis.py        # field inventory for PostGIS
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îú‚îÄ‚îÄ metadata/        ‚Üê read / write layer-level metadata
‚îÇ   ‚îú‚îÄ‚îÄ csv.py            # side-car CSV metadata files
‚îÇ   ‚îú‚îÄ‚îÄ db.py             # PostGIS / SQLite layer comments
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îú‚îÄ‚îÄ scripts/         ‚Üê one-off CLI entry points
‚îÇ   ‚îî‚îÄ‚îÄ download_utils.py # `python -m stp.scripts.download_utils`
‚îÇ
‚îú‚îÄ‚îÄ storage/         ‚Üê persistence back-ends
‚îÇ   ‚îú‚îÄ‚îÄ db\_storage.py     # PostGIS via SQLAlchemy
‚îÇ   ‚îú‚îÄ‚îÄ file\_storage.py   # GeoPackage / Shapefile
‚îÇ   ‚îî‚îÄ‚îÄ **init**.py
‚îÇ
‚îî‚îÄ‚îÄ (root modules)   ‚Üê orchestration & shared helpers
‚îú‚îÄ‚îÄ config\_loader.py
‚îú‚îÄ‚îÄ data\_cleaning.py
‚îú‚îÄ‚îÄ download.py
‚îú‚îÄ‚îÄ fields\_inventory.py
‚îú‚îÄ‚îÄ http\_client.py
‚îú‚îÄ‚îÄ settings.py
‚îú‚îÄ‚îÄ table.py
‚îî‚îÄ‚îÄ **init**.py

```

---

## Top-level module cheat-sheet

| Module | Purpose |
|--------|---------|
| **config_loader.py** | Read YAML/ENV configuration & expose `get_setting`, `get_constant`. |
| **settings.py** | Hard-coded fall-backs (NYSP EPSG 2263, default filenames, etc.). |
| **download.py** | ‚ÄúOrchestrator‚Äù ‚Äì loops through every fetcher listed in config and drops raw files into `Data/raw/`. |
| **http_client.py** | Thin `requests.Session` wrapper with retry & back-off. |
| **data_cleaning.py** | Pipeline runner ‚Äì re-projects, trims fields, fixes datatypes using functions from `cleaning/`. |
| **fields_inventory.py** | Generates a schema report (dtype, null %, sample) for any GeoDataFrame or DB layer. |
| **table.py** | Helpers to convert between CSV ‚Üî GeoJSON ‚Üî GeoPackage with consistent field ordering. |

---

## How the pieces fit

```

fetchers/        ‚Üí  Data/raw/\*.geojson / .csv
‚îÇ
‚ñº
cleaning/         GeoDataFrame in EPSG:2263
‚îÇ
‚ñº
storage/          PostGIS (db\_storage)  or  GeoPackage (file\_storage)
‚îÇ
‚ñº
inventory/        Optional schema dump / metadata write-back

````

---

### Quickstart

```bash
# 1) install deps
pip install -r requirements.txt   # add psycopg2-binary for PostGIS

# 2) pull all configured layers
python -m stp.download

# 3) clean & normalise
python -m stp.data_cleaning

# 4) inspect schema (optional)
python -m stp.fields_inventory Data/clean/addresses.gpkg
````

That‚Äôs it!
Drop this `README_stp.md` into `src/stp/` to give new contributors an instant
map of the toolkit.
`````

## src/stp/zip.py

```python
import os
import zipfile

def main():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    root_folder_name = os.path.basename(script_dir)
    zip_path = os.path.join(script_dir, 'archived_py_files.zip')
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(script_dir):
            for file in files:
                if file.endswith('.py'):
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(root, script_dir)
                    path_parts = [root_folder_name] if rel_path == '.' else [root_folder_name] + rel_path.split(os.sep)
                    prefix = '_'.join(path_parts) + '_'
                    new_name = prefix + file
                    with open(full_path, 'rb') as f:
                        content = f.read()
                    zipf.writestr(new_name, content)

if __name__ == '__main__':
    main()
```

## src/stp/__init__.py

```python

```

## src/temp/download_data.py

```python
"""
download_data.py

Main entry point for fetching spatial and tabular datasets based on a
configuration file and source registry. Supports Socrata, ArcGIS, and
direct URLs (CSV, GeoJSON, Shapefile, GPKG). Records metadata and
optionally reprojects in a GeoPackage or loads into PostGIS.
"""

import json
import logging
from pathlib import Path

# use get_setting (aliased to 'get') so both settings.yaml overrides and
# defaults.yaml fallbacks work the same way
from stp.config_loader import get_setting as get, get_constant

from stp.fetch import fetch_arcgis_vector

from stp.storage.file_storage import (
    get_geopackage_path,
    get_postgis_engine,
    reproject_all_layers,
    sanitize_layer_name,
    export_spatial_layer,
)
from stp.process.table import (
    record_layer_metadata_csv,
    record_layer_metadata_db,
)
from stp.fetch.lookup import FETCHERS

logger = logging.getLogger(__name__)


def setup_destinations():
    """Read config settings and prepare output destinations."""
    socrata_token = get("socrata.app_token")
    db_cfg = get("db", {})

    if db_cfg.get("enabled", False):
        db_engine = get_postgis_engine(db_cfg)
    else:
        db_engine = None

    output_epsg = get("data.output_epsg", get_constant("nysp_epsg"))
    out_shp_dir = Path(get("data.output_shapefile"))
    out_tbl_dir = Path(get("data.output_tables"))
    out_shp_dir.mkdir(parents=True, exist_ok=True)
    out_tbl_dir.mkdir(parents=True, exist_ok=True)

    if not db_engine:
        metadata_csv = out_tbl_dir / get_constant(
            "data_inventory_filename"
        )
        gpkg = get_geopackage_path(out_shp_dir)
        if metadata_csv.exists():
            try:
                metadata_csv.unlink()
            except OSError as e:
                logger.warning(
                    "Could not delete existing CSV '%s': %s", metadata_csv, e
                )
    else:
        metadata_csv = None
        gpkg = None

    return socrata_token, db_engine, gpkg, metadata_csv, output_epsg


def load_layer_list():
    """Load the list of layers to fetch from the JSON registry."""
    data_sources = Path("config") / "sources.json"
    with open(data_sources, encoding="utf-8") as f:
        return json.load(f)


def process_layer(
    layer, idx, total, socrata_token, db_engine, gpkg, metadata_csv
):
    """Fetch, record metadata, and store one layer."""
    layer_id = layer["id"]
    url = layer["url"]
    stype = layer.get("source_type")
    fmt = layer.get("format", "").lower()
    helper_fn = FETCHERS.get((stype, fmt))

    logger.info(
        "[%d/%d] %s (source_type=%s, format=%s)",
        idx,
        total,
        layer_id,
        stype,
        fmt,
    )

    if stype == "arcgis":
        raw = fetch_arcgis_vector(url)
        results = [
            (layer_id, gdf, src_epsg, wkid)
            for (_, gdf, src_epsg, wkid) in raw
        ]
    elif stype == "socrata":
        raw = helper_fn(url, app_token=socrata_token)
        results = [
            (layer_id, gdf, src_epsg, None)
            for (_, gdf, src_epsg) in raw
        ]
    else:
        raw = helper_fn(url)
        results = [
            (layer_id, gdf, src_epsg, None)
            for (_, gdf, src_epsg) in raw
        ]

    for raw_name, gdf, source_epsg, service_wkid in results:
        clean_name = sanitize_layer_name(raw_name)

        if db_engine:
            record_layer_metadata_db(
                db_engine,
                clean_name,
                url,
                source_epsg,
                service_wkid,
            )
            gdf.to_postgis(
                clean_name,
                db_engine,
                if_exists="replace",
                index=False,
            )
        else:
            record_layer_metadata_csv(
                metadata_csv,
                clean_name,
                url,
                source_epsg,
                service_wkid,
            )
            export_spatial_layer(gdf, clean_name, gpkg)


def finalize(gpkg, metadata_csv, output_epsg):
    """Reproject all layers in the GeoPackage to the target EPSG."""
    if gpkg and metadata_csv:
        reproject_all_layers(gpkg, metadata_csv, target_epsg=output_epsg)


def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
    )
    socrata_token, db_engine, gpkg, metadata_csv, output_epsg = (
        setup_destinations()
    )
    layers = load_layer_list()
    total = len(layers)
    for idx, layer in enumerate(layers, start=1):
        process_layer(
            layer,
            idx,
            total,
            socrata_token,
            db_engine,
            gpkg,
            metadata_csv,
        )
    finalize(gpkg, metadata_csv, output_epsg)


if __name__ == "__main__":
    main()
```

## src/temp/political_boundary.py

```python
"""
tempp
"""
from pathlib import Path
import geopandas as gpd
from sqlalchemy import create_engine
from shapely.ops import unary_union
from stp.config_loader import get_setting, get_constant
from stp.config_loader import get_setting, get_constant

# 1) Paths and config
base_dir = Path.cwd()
db_cfg = get_setting("db", {})
output_epsg = get_setting("data.output_epsg", get_constant("nysp_epsg", 2263))
output_dir = Path(get_setting("data.output_shapefile", "Data/shapefiles"))
output_dir.mkdir(parents=True, exist_ok=True)

# 2) Setup storage mode
engine = None
if db_cfg.get("enabled"):
    conn_url = (
        f"{db_cfg['driver']}://{db_cfg['user']}:{db_cfg['password']}@"
        f"{db_cfg['host']}:{db_cfg['port']}/{db_cfg['database']}"
    )
    engine = create_engine(conn_url)
else:
    gpkg_path = output_dir / get_constant(
        "default_gpkg_name", "project_data.gpkg"
    )
    if gpkg_path.exists():
        gpkg_path.unlink()

# 3) Define layers to process
layer_ids = [
    "borough", "community_districts", "city_council_districts",
    "us_congressional_districts", "state_senate_districts",
    "state_assembly_districts", "community_district_tabulation_areas",
    "neighborhood_tabulation_areas", "census_tracts", "census_blocks",
    "zoning_districts", "commercial_districts", "special_purpose_districts"
]

# 4) Load each layer into GeoDataFrames
gdfs = []
for layer in layer_ids:
    if engine:
        # Read from PostGIS
        gdf = gpd.read_postgis(
            f"SELECT * FROM {layer}", engine, geom_col="geometry"
        )
        gdf = gpd.read_postgis(
            f"SELECT * FROM {layer}", engine, geom_col="geometry"
        )
        gdf.set_crs(epsg=output_epsg, inplace=True)
    else:
        # Read from GeoPackage
        gdf = gpd.read_file(gpkg_path, layer=layer)
    gdfs.append(gdf)

# 5) Union all boundaries
all_union = unary_union([gdf.unary_union for gdf in gdfs])
result_gdf = gpd.GeoDataFrame([{"geometry": all_union}], crs=output_epsg)

# 6) Persist the result
if engine:
    result_gdf.to_postgis(
        "political_boundaries", engine, if_exists="replace", index=False
    )
else:
    result_gdf.to_file(
        gpkg_path, layer="political_boundaries", driver="GPKG"
    )
print("‚úÖ political_boundaries created")
```

## tests/conftest.py

```python
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parents[1] / "src"))
```

## tests/test_address.py

```python
import geopandas as gpd
from shapely.geometry import Point
import stp.clean.address as addr


def test_clean_street_signs():
    df = gpd.GeoDataFrame(
        {
            "record_type": ["Current"],
            "order_completed_on_date": ["2020-01-01"],
            "sign_description": ["desc"],
            "distance_from_intersection": ["1"],
            "side_of_street": ["N"],
            "arrow_direction": ["E"],
            "sign_code": ["A"],
            "sign_notes": ["note"],
        },
        geometry=[Point(0, 0)],
    )
    cleaned = addr.clean_street_signs(df)
    assert not cleaned.empty
```

## tests/test_config_loader.py

```python
"""
test for config_loader
"""
from stp.config_loader import _deep_get


def test_deep_get_path_found():
    "variables are list, keys"
    config = {"a": {"b": 42}}
    assert _deep_get(config, ["a", "b"]) == 42
```

## tests/test_csv.py

```python
"""
tests csv
"""
import geopandas as gpd
from pytest import MonkeyPatch
import stp.fetch.csv as csv_f


def test_fetch_csv_direct(monkeypatch: MonkeyPatch):
    """Verify fetch_csv_direct returns [] when CSV lacks lat/lon cols."""
    csv_data = b"latitude,longitude\n1,2\n"

    def fake_bytes(url):
        return csv_data

    monkeypatch.setattr(csv_f.http_client, "fetch_bytes", fake_bytes)
    res = csv_f.fetch_csv_direct("http://x/data.csv")
    assert res[0][0] == "data"
    assert isinstance(res[0][1], gpd.GeoDataFrame)
    assert res[0][2] == csv_f.DEFAULT_EPSG
```

## tests/test_db_storage.py

```python
import stp.storage.db_storage as dbs


def test_get_postgis_engine(monkeypatch):
    captured = {}

    def fake_create(url):
        captured["url"] = url
        return "engine"

    monkeypatch.setattr(dbs, "create_engine", fake_create)
    cfg = {
        "enabled": True,
        "driver": "postgis",
        "user": "u",
        "password": "p",
        "host": "h",
        "port": "1",
        "database": "d",
    }
    engine = dbs.get_postgis_engine(cfg)
    assert engine == "engine"
    assert "postgis://u:p@h:1/d" == captured["url"]

    cfg["enabled"] = False
    assert dbs.get_postgis_engine(cfg) is None
```

## tests/test_download.py

```python
import stp.fetch.download as dl


def test_fetch_direct_dispatch(monkeypatch):
    monkeypatch.setattr(
        dl,
        "fetch_geojson_direct",
        lambda url: [("a", None, 1)],
    )
    monkeypatch.setattr(dl, "fetch_csv_direct", lambda url: [("b", None, 1)])

    assert dl.fetch_direct("file.geojson")[0][0] == "a"
    assert dl.fetch_direct("file.csv")[0][0] == "b"
```

## tests/test_file_storage.py

```python
import stp.storage.file_storage as fs


def test_sanitize_layer_name():
    assert fs.sanitize_layer_name("bad name!") == "bad_name_"


def test_get_geopackage_path(tmp_path):
    gpkg = fs.get_geopackage_path(tmp_path)
    assert gpkg.parent == tmp_path
```

## tests/test_geojson.py

```python
import geopandas as gpd
from shapely.geometry import Point
import stp.fetch.geojson as gj


def test_fetch_geojson_direct(monkeypatch):
    def fake_bytes(url):
        return b"{}"

    monkeypatch.setattr(gj.http_client, "fetch_bytes", fake_bytes)
    dummy = gpd.GeoDataFrame({"geometry": [Point(0, 0)]}, geometry="geometry")

    def fake_read_file(buf):
        return dummy

    monkeypatch.setattr(gpd, "read_file", lambda *a, **k: dummy)
    res = gj.fetch_geojson_direct("http://x/data.geojson")
    assert res[0][0] == "data"
    assert res[0][1] is dummy
    assert res[0][2] == gj.DEFAULT_EPSG
```

## tests/test_http_client.py

```python
import types
import stp.http_client as hc


class DummyResponse:
    def __init__(self, data: bytes):
        self.content = data

    def raise_for_status(self):
        pass


def test_fetch_bytes(monkeypatch):
    def fake_get(url):
        return DummyResponse(b"ok")

    monkeypatch.setattr(hc._session, "get", fake_get)
    data = hc.fetch_bytes("http://example.com")
    assert data == b"ok"
```

## tests/test_trees.py

```python
import geopandas as gpd
from shapely.geometry import Point
import stp.clean.trees as trees


def test_clean_trees_basic():
    df = gpd.GeoDataFrame(
        {"tpstructure": ["Full"], "objectid": [1]},
        geometry=[Point(0, 0)],
    )
    res = trees.clean_trees_basic(df)
    assert "TreeID" in res


def test_canceled_work_orders():
    df = gpd.GeoDataFrame(
        {
            "wotype": ["Tree Plant-Street Tree"],
            "wocategory": ["Tree Planting"],
            "wostatus": ["Cancel"],
            "objectid": [1],
        },
        geometry=[Point(0, 0)],
    )
    out = trees.canceled_work_orders(df)
    assert not out.empty
```

## Statistics

- Total Files: 73
- Total Characters: 188644
- Total Tokens: 0
``````

## tests/conftest.py

```python
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parents[1] / "src"))
```

## tests/test_address.py

```python
import geopandas as gpd
from shapely.geometry import Point
import stp.clean.address as addr


def test_clean_street_signs():
    df = gpd.GeoDataFrame(
        {
            "record_type": ["Current"],
            "order_completed_on_date": ["2020-01-01"],
            "sign_description": ["desc"],
            "distance_from_intersection": ["1"],
            "side_of_street": ["N"],
            "arrow_direction": ["E"],
            "sign_code": ["A"],
            "sign_notes": ["note"],
        },
        geometry=[Point(0, 0)],
    )
    cleaned = addr.clean_street_signs(df)
    assert not cleaned.empty
```

## tests/test_config_loader.py

```python
"""
test for config_loader
"""
from stp.config_loader import _deep_get


def test_deep_get_path_found():
    "variables are list, keys"
    config = {"a": {"b": 42}}
    assert _deep_get(config, ["a", "b"]) == 42
```

## tests/test_csv.py

```python
"""
tests csv
"""
import geopandas as gpd
from pytest import MonkeyPatch
import stp.fetch.csv as csv_f


def test_fetch_csv_direct(monkeypatch: MonkeyPatch):
    """Verify fetch_csv_direct returns [] when CSV lacks lat/lon cols."""
    csv_data = b"latitude,longitude\n1,2\n"

    def fake_bytes(url):
        return csv_data

    monkeypatch.setattr(csv_f.http_client, "fetch_bytes", fake_bytes)
    res = csv_f.fetch_csv_direct("http://x/data.csv")
    assert res[0][0] == "data"
    assert isinstance(res[0][1], gpd.GeoDataFrame)
    assert res[0][2] == csv_f.DEFAULT_EPSG
```

## tests/test_db_storage.py

```python
import stp.storage.db_storage as dbs


def test_get_postgis_engine(monkeypatch):
    captured = {}

    def fake_create(url):
        captured["url"] = url
        return "engine"

    monkeypatch.setattr(dbs, "create_engine", fake_create)
    cfg = {
        "enabled": True,
        "driver": "postgis",
        "user": "u",
        "password": "p",
        "host": "h",
        "port": "1",
        "database": "d",
    }
    engine = dbs.get_postgis_engine(cfg)
    assert engine == "engine"
    assert "postgis://u:p@h:1/d" == captured["url"]

    cfg["enabled"] = False
    assert dbs.get_postgis_engine(cfg) is None
```

## tests/test_download.py

```python
import stp.fetch.download as dl


def test_fetch_direct_dispatch(monkeypatch):
    monkeypatch.setattr(
        dl,
        "fetch_geojson_direct",
        lambda url: [("a", None, 1)],
    )
    monkeypatch.setattr(dl, "fetch_csv_direct", lambda url: [("b", None, 1)])

    assert dl.fetch_direct("file.geojson")[0][0] == "a"
    assert dl.fetch_direct("file.csv")[0][0] == "b"
```

## tests/test_file_storage.py

```python
import stp.storage.file_storage as fs


def test_sanitize_layer_name():
    assert fs.sanitize_layer_name("bad name!") == "bad_name_"


def test_get_geopackage_path(tmp_path):
    gpkg = fs.get_geopackage_path(tmp_path)
    assert gpkg.parent == tmp_path
```

## tests/test_geojson.py

```python
import geopandas as gpd
from shapely.geometry import Point
import stp.fetch.geojson as gj


def test_fetch_geojson_direct(monkeypatch):
    def fake_bytes(url):
        return b"{}"

    monkeypatch.setattr(gj.http_client, "fetch_bytes", fake_bytes)
    dummy = gpd.GeoDataFrame({"geometry": [Point(0, 0)]}, geometry="geometry")

    def fake_read_file(buf):
        return dummy

    monkeypatch.setattr(gpd, "read_file", lambda *a, **k: dummy)
    res = gj.fetch_geojson_direct("http://x/data.geojson")
    assert res[0][0] == "data"
    assert res[0][1] is dummy
    assert res[0][2] == gj.DEFAULT_EPSG
```

## tests/test_http_client.py

```python
import types
import stp.http_client as hc


class DummyResponse:
    def __init__(self, data: bytes):
        self.content = data

    def raise_for_status(self):
        pass


def test_fetch_bytes(monkeypatch):
    def fake_get(url):
        return DummyResponse(b"ok")

    monkeypatch.setattr(hc._session, "get", fake_get)
    data = hc.fetch_bytes("http://example.com")
    assert data == b"ok"
```

## tests/test_trees.py

```python
import geopandas as gpd
from shapely.geometry import Point
import stp.clean.trees as trees


def test_clean_trees_basic():
    df = gpd.GeoDataFrame(
        {"tpstructure": ["Full"], "objectid": [1]},
        geometry=[Point(0, 0)],
    )
    res = trees.clean_trees_basic(df)
    assert "TreeID" in res


def test_canceled_work_orders():
    df = gpd.GeoDataFrame(
        {
            "wotype": ["Tree Plant-Street Tree"],
            "wocategory": ["Tree Planting"],
            "wostatus": ["Cancel"],
            "objectid": [1],
        },
        geometry=[Point(0, 0)],
    )
    out = trees.canceled_work_orders(df)
    assert not out.empty
```

## Statistics

- Total Files: 74
- Total Characters: 346449
- Total Tokens: 0
```````

## tests/conftest.py

```python
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parents[1] / "src"))
```

## tests/test_address.py

```python
import geopandas as gpd
from shapely.geometry import Point
import stp.clean.address as addr


def test_clean_street_signs():
    df = gpd.GeoDataFrame(
        {
            "record_type": ["Current"],
            "order_completed_on_date": ["2020-01-01"],
            "sign_description": ["desc"],
            "distance_from_intersection": ["1"],
            "side_of_street": ["N"],
            "arrow_direction": ["E"],
            "sign_code": ["A"],
            "sign_notes": ["note"],
        },
        geometry=[Point(0, 0)],
    )
    cleaned = addr.clean_street_signs(df)
    assert not cleaned.empty
```

## tests/test_config_loader.py

```python
"""
test for config_loader
"""
from stp.config_loader import _deep_get


def test_deep_get_path_found():
    "variables are list, keys"
    config = {"a": {"b": 42}}
    assert _deep_get(config, ["a", "b"]) == 42
```

## tests/test_csv.py

```python
"""
tests csv
"""
import geopandas as gpd
from pytest import MonkeyPatch
import stp.fetch.csv as csv_f


def test_fetch_csv_direct(monkeypatch: MonkeyPatch):
    """Verify fetch_csv_direct returns [] when CSV lacks lat/lon cols."""
    csv_data = b"latitude,longitude\n1,2\n"

    def fake_bytes(url):
        return csv_data

    monkeypatch.setattr(csv_f.http_client, "fetch_bytes", fake_bytes)
    res = csv_f.fetch_csv_direct("http://x/data.csv")
    assert res[0][0] == "data"
    assert isinstance(res[0][1], gpd.GeoDataFrame)
    assert res[0][2] == csv_f.DEFAULT_EPSG
```

## tests/test_db_storage.py

```python
import stp.storage.db_storage as dbs


def test_get_postgis_engine(monkeypatch):
    captured = {}

    def fake_create(url):
        captured["url"] = url
        return "engine"

    monkeypatch.setattr(dbs, "create_engine", fake_create)
    cfg = {
        "enabled": True,
        "driver": "postgis",
        "user": "u",
        "password": "p",
        "host": "h",
        "port": "1",
        "database": "d",
    }
    engine = dbs.get_postgis_engine(cfg)
    assert engine == "engine"
    assert "postgis://u:p@h:1/d" == captured["url"]

    cfg["enabled"] = False
    assert dbs.get_postgis_engine(cfg) is None
```

## tests/test_download.py

```python
import stp.fetch.download as dl


def test_fetch_direct_dispatch(monkeypatch):
    monkeypatch.setattr(
        dl,
        "fetch_geojson_direct",
        lambda url: [("a", None, 1)],
    )
    monkeypatch.setattr(dl, "fetch_csv_direct", lambda url: [("b", None, 1)])

    assert dl.fetch_direct("file.geojson")[0][0] == "a"
    assert dl.fetch_direct("file.csv")[0][0] == "b"
```

## tests/test_file_storage.py

```python
import stp.storage.file_storage as fs


def test_sanitize_layer_name():
    assert fs.sanitize_layer_name("bad name!") == "bad_name_"


def test_get_geopackage_path(tmp_path):
    gpkg = fs.get_geopackage_path(tmp_path)
    assert gpkg.parent == tmp_path
```

## tests/test_geojson.py

```python
import geopandas as gpd
from shapely.geometry import Point
import stp.fetch.geojson as gj


def test_fetch_geojson_direct(monkeypatch):
    def fake_bytes(url):
        return b"{}"

    monkeypatch.setattr(gj.http_client, "fetch_bytes", fake_bytes)
    dummy = gpd.GeoDataFrame({"geometry": [Point(0, 0)]}, geometry="geometry")

    def fake_read_file(buf):
        return dummy

    monkeypatch.setattr(gpd, "read_file", lambda *a, **k: dummy)
    res = gj.fetch_geojson_direct("http://x/data.geojson")
    assert res[0][0] == "data"
    assert res[0][1] is dummy
    assert res[0][2] == gj.DEFAULT_EPSG
```

## tests/test_http_client.py

```python
import types
import stp.http_client as hc


class DummyResponse:
    def __init__(self, data: bytes):
        self.content = data

    def raise_for_status(self):
        pass


def test_fetch_bytes(monkeypatch):
    def fake_get(url):
        return DummyResponse(b"ok")

    monkeypatch.setattr(hc._session, "get", fake_get)
    data = hc.fetch_bytes("http://example.com")
    assert data == b"ok"
```

## tests/test_trees.py

```python
import geopandas as gpd
from shapely.geometry import Point
import stp.clean.trees as trees


def test_clean_trees_basic():
    df = gpd.GeoDataFrame(
        {"tpstructure": ["Full"], "objectid": [1]},
        geometry=[Point(0, 0)],
    )
    res = trees.clean_trees_basic(df)
    assert "TreeID" in res


def test_canceled_work_orders():
    df = gpd.GeoDataFrame(
        {
            "wotype": ["Tree Plant-Street Tree"],
            "wocategory": ["Tree Planting"],
            "wostatus": ["Cancel"],
            "objectid": [1],
        },
        geometry=[Point(0, 0)],
    )
    out = trees.canceled_work_orders(df)
    assert not out.empty
```

## Statistics

- Total Files: 76
- Total Characters: 506651
- Total Tokens: 0
